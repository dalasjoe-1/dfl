{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GetLotto.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Q8MptZ7cjH8T"
      ],
      "authorship_tag": "ABX9TyM7VP1Yo21rFJrzlZnaYkYr",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dalasjoe-1/dfl/blob/master/GetLotto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP2-bRVzoZ9T"
      },
      "source": [
        "# ALL Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzbFpN7ZokLu"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "!pip install tensorflow-gpu==2.0.0-rc1\n",
        "\n",
        "main_url = \"https://www.dhlottery.co.kr/gameResult.do?method=byWin\" # 마지막 회차를 얻기 위한 주소\n",
        "basic_url = \"https://www.dhlottery.co.kr/gameResult.do?method=byWin&drwNo=\" # 임의의 회차를 얻기 위한 주소\n",
        "\n",
        "# 마지막 회차 정보를 가져옴\n",
        "def GetLast(): \n",
        "    resp = requests.get(main_url)\n",
        "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "    result = str(soup.find(\"meta\", {\"id\" : \"desc\", \"name\" : \"description\"})['content'])\n",
        "    s_idx = result.find(\" \")\n",
        "    e_idx = result.find(\"회\")\n",
        "    return int(result[s_idx + 1 : e_idx])\n",
        "\n",
        "# 지정된 파일에 지정된 범위의 회차 정보를 기록함\n",
        "def Crawler(s_count, e_count, fp):\n",
        "    for i in range(s_count , e_count + 1):\n",
        "        crawler_url = basic_url + str(i)\n",
        "        resp = requests.get(crawler_url)\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        text = soup.text\n",
        "\n",
        "        s_idx = text.find(\" 당첨결과\")\n",
        "        s_idx = text.find(\"당첨번호\", s_idx) + 4\n",
        "        e_idx = text.find(\"보너스\", s_idx)\n",
        "        numbers = text[s_idx:e_idx].strip().split()\n",
        "\n",
        "        s_idx = e_idx + 3\n",
        "        e_idx = s_idx + 3\n",
        "        bonus = text[s_idx:e_idx].strip()\n",
        "\n",
        "        s_idx = text.find(\"1등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money1 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"2등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money2 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"3등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money3 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"4등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money4 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"5등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money5 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        line = str(i) + ',' + numbers[0] + ',' + numbers[1] + ',' + numbers[2] + ',' + numbers[3] + ',' + numbers[4] + ',' + numbers[5] + ',' + bonus + ',' + money1 + ',' + money2 + ',' + money3 + ',' + money4 + ',' + money5\n",
        "        print(line)\n",
        "        line += '\\n'\n",
        "        fp.write(line)\n",
        "\n",
        "last = GetLast() # 마지막 회차를 가져옴\n",
        "\n",
        "lotto_dir_name='lotto'\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "if not path.exists(lotto_base_dir):\n",
        "  print('Check your google drive directory. See you file explorer')\n",
        "\n",
        "\n",
        "print (\"Last=\",last)\n",
        "\n",
        "# 완전히 다시 쓰기\n",
        "#fp = open(path.join(lotto_base_dir, \"lotto_data.txt\"), 'w')\n",
        "\n",
        "# 기존 데이타에 추가하기\n",
        "fp = open(path.join(lotto_base_dir, \"lotto_data.txt\"), 'a')\n",
        "\n",
        "#Crawler(last-1, last, fp) # 처음부터 마지막 회차까지 저장\n",
        "#fp.close()\n",
        "\n",
        "\n",
        "\n",
        "# 최대 100번 에포크까지 수행\n",
        "\n",
        "model = tf.keras.models.load_model('./drive/MyDrive/my_model.h5')\n",
        "model.summary()\n",
        "\n",
        "for epoch in range(50):\n",
        "\n",
        "    model.reset_states() # 중요! 매 에포크마다 1회부터 다시 훈련하므로 상태 초기화 필요\n",
        "\n",
        "    batch_train_loss = []\n",
        "    batch_train_acc = []\n",
        "\n",
        "    for i in range(len(x_samples)):\n",
        "        \n",
        "        xs = x_samples[i].reshape(1, 1, 45)\n",
        "        ys = y_samples[i].reshape(1, 45)\n",
        "        \n",
        "        loss, acc = model.train_on_batch(xs, ys) #배치만큼 모델에 학습시킴\n",
        "\n",
        "        batch_train_loss.append(loss)\n",
        "        batch_train_acc.append(acc)\n",
        "\n",
        "    train_loss.append(np.mean(batch_train_loss))\n",
        "    train_acc.append(np.mean(batch_train_acc))\n",
        "\n",
        "    print('epoch {0:4d} train acc {1:0.3f} loss {2:0.3f}'.format(epoch, np.mean(batch_train_acc), np.mean(batch_train_loss)))  \n",
        "\n",
        "\n",
        "# Trainning 끝나면 무조건 모델에 저장\n",
        "model.save('my_model.h5')\n",
        "!cp /content/my_model.h5 /content/drive/My\\ Drive/\n",
        "\n",
        "\n",
        "from ftplib import FTP\n",
        "\n",
        "ftp = FTP('112.175.184.78')\n",
        "ftp.login('dalasjoe', 'Dalasjoe75!')\n",
        "\n",
        "# ftp.cwd('html') # \"test\"디렉터리로 이동\n",
        "# ftp.retrlines('LIST') # 디렉터리의 내용을 목록화\n",
        "# #ftp.retrbinary('RETR README', open('README', 'wb').write) # README 파일 저장\n",
        "# ftp.quit()\n",
        "\n",
        "\n",
        "ftp.cwd('html')  # 업로드할 FTP 폴더로 이동\n",
        "myfile = open(path.join(lotto_base_dir, \"predict2.txt\"),'rb')  # 로컬 파일 열기\n",
        "ftp.storbinary('STOR ' + 'predict2.txt', myfile )  # 파일을 FTP로 업로드\n",
        "myfile.close()  # 파일 닫기\n",
        "\n",
        "print (\"File Saved\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSm4U5BjfO1R"
      },
      "source": [
        "# 저장된 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MABE44rDfTf8"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "otto_dir_name='lotto'\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "\n",
        "if not path.exists(lotto_base_dir):\n",
        "  print('Check your google drive directory. See you file explorer')\n",
        "\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "with open(path.join(lotto_base_dir, \"lotto_data.txt\"), \"r\") as f:\n",
        "  print(\"11111\")\n",
        "  line = f.read()\n",
        "  print(\">>>\" + line)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZRsvoibHt5d"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8MptZ7cjH8T"
      },
      "source": [
        "# **데이터 수집**  - Crawling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6kMeQ2_aCfm"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "main_url = \"https://www.dhlottery.co.kr/gameResult.do?method=byWin\" # 마지막 회차를 얻기 위한 주소\n",
        "basic_url = \"https://www.dhlottery.co.kr/gameResult.do?method=byWin&drwNo=\" # 임의의 회차를 얻기 위한 주소\n",
        "\n",
        "# 마지막 회차 정보를 가져옴\n",
        "def GetLast(): \n",
        "    resp = requests.get(main_url)\n",
        "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "    result = str(soup.find(\"meta\", {\"id\" : \"desc\", \"name\" : \"description\"})['content'])\n",
        "    s_idx = result.find(\" \")\n",
        "    e_idx = result.find(\"회\")\n",
        "    return int(result[s_idx + 1 : e_idx])\n",
        "\n",
        "# 지정된 파일에 지정된 범위의 회차 정보를 기록함\n",
        "def Crawler(s_count, e_count, fp):\n",
        "    for i in range(s_count , e_count + 1):\n",
        "        crawler_url = basic_url + str(i)\n",
        "        resp = requests.get(crawler_url)\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        text = soup.text\n",
        "\n",
        "        s_idx = text.find(\" 당첨결과\")\n",
        "        s_idx = text.find(\"당첨번호\", s_idx) + 4\n",
        "        e_idx = text.find(\"보너스\", s_idx)\n",
        "        numbers = text[s_idx:e_idx].strip().split()\n",
        "\n",
        "        s_idx = e_idx + 3\n",
        "        e_idx = s_idx + 3\n",
        "        bonus = text[s_idx:e_idx].strip()\n",
        "\n",
        "        s_idx = text.find(\"1등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money1 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"2등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money2 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"3등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money3 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"4등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money4 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"5등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money5 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        line = str(i) + ',' + numbers[0] + ',' + numbers[1] + ',' + numbers[2] + ',' + numbers[3] + ',' + numbers[4] + ',' + numbers[5] + ',' + bonus + ',' + money1 + ',' + money2 + ',' + money3 + ',' + money4 + ',' + money5\n",
        "        print(line)\n",
        "        line += '\\n'\n",
        "        fp.write(line)\n",
        "\n",
        "last = GetLast() # 마지막 회차를 가져옴\n",
        "\n",
        "lotto_dir_name='lotto'\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "if not path.exists(lotto_base_dir):\n",
        "  print('Check your google drive directory. See you file explorer')\n",
        "\n",
        "#with open(path.join(lotto_base_dir, \"lotto_data.txt\"), \"r\") as f:\n",
        "#  print(\"11111\")\n",
        "#   line = f.read()\n",
        "#   print(\">>>\" + line)\n",
        "\n",
        "print (\"Last=\",last)\n",
        "\n",
        "# 완전히 다시 쓰기\n",
        "#fp = open(path.join(lotto_base_dir, \"lotto_data.txt\"), 'w')\n",
        "\n",
        "# 기존 데이타에 추가하기\n",
        "fp = open(path.join(lotto_base_dir, \"lotto_data.txt\"), 'a')\n",
        "\n",
        "Crawler(1, last, fp) # 처음부터 마지막 회차까지 저장\n",
        "fp.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b59-XaqSHwpT"
      },
      "source": [
        "## 데이터 로딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGhgk1uAH1KF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8244175-9b54-4adc-a5ed-2d08cac781fd"
      },
      "source": [
        "from os import path\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "rows = np.loadtxt(\"./drive/MyDrive/lotto_data.txt\", delimiter=\",\")\n",
        "row_count = len(rows)\n",
        "\n",
        "print(\"Data Loaded : row count: \" + str(row_count))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Data Loaded : row count: 965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjyyBnKlfNfS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ0yqd9Xjmwh"
      },
      "source": [
        "# **TensorFlow 설치 및 모델 초기화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-vuJrQn2pNt",
        "outputId": "d9aa4e6c-5f87-47b5-b5a1-ed78c9122c3a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "!pip install tensorflow-gpu==2.0.0-rc1\n",
        "\n",
        "# 당첨번호를 원핫인코딩벡터(ohbin)으로 변환\n",
        "def numbers2ohbin(numbers):\n",
        "\n",
        "    ohbin = np.zeros(45) #45개의 빈 칸을 만듬\n",
        "\n",
        "    for i in range(6): #여섯개의 당첨번호에 대해서 반복함\n",
        "        ohbin[int(numbers[i])-1] = 1 #로또번호가 1부터 시작하지만 벡터의 인덱스 시작은 0부터 시작하므로 1을 뺌\n",
        "    \n",
        "    return ohbin\n",
        "\n",
        "# 원핫인코딩벡터(ohbin)를 번호로 변환\n",
        "def ohbin2numbers(ohbin):\n",
        "\n",
        "    numbers = []\n",
        "    \n",
        "    for i in range(len(ohbin)):\n",
        "        if ohbin[i] == 1.0: # 1.0으로 설정되어 있으면 해당 번호를 반환값에 추가한다.\n",
        "            numbers.append(i+1)\n",
        "    \n",
        "    return numbers\n",
        "\n",
        "\n",
        "numbers = rows[:, 1:7]\n",
        "ohbins = list(map(numbers2ohbin, numbers))\n",
        "\n",
        "x_samples = ohbins[0:row_count-1]\n",
        "y_samples = ohbins[1:row_count]\n",
        "\n",
        "#원핫인코딩으로 표시\n",
        "print(\"ohbins\")\n",
        "print(\"X[0]: \" + str(x_samples[0]))\n",
        "print(\"Y[0]: \" + str(y_samples[0]))\n",
        "\n",
        "#번호로 표시\n",
        "print(\"numbers\")\n",
        "print(\"X[0]: \" + str(ohbin2numbers(x_samples[0])))\n",
        "print(\"Y[0]: \" + str(ohbin2numbers(y_samples[0])))\n",
        "\n",
        "# 데이터셋 구성\n",
        "train_idx = (0, 700) # 훈련셋 \n",
        "val_idx = (700, 800) # 검증셋\n",
        "test_idx = (800, len(x_samples)) #시험셋\n",
        "\n",
        "print(\"train: {0}, val: {1}, test: {2}\".format(train_idx, val_idx, test_idx))\n",
        "\n",
        "# 모델을 정의합니다. - LSTM\n",
        "#타입스텝은 1인 대신, 상태유지(stateful 옵션을 True)으로 설정했습니다.\n",
        "#45개의 벡터로 출력합니다.\n",
        "#각각의 벡터는 0.0과 1.0사이의 실수값으로 나옵니다. 각 벡터가 독립적으로 모두 1.0이 나오거나 0.0이 나올 수 있는 멀티레이블 문제입니다.\n",
        "#멀티레이블 문제라 출력층의 활성화함수가 softmax가 아닌 sigmoid로 설정하였습니다.\n",
        "model = keras.Sequential([\n",
        "    keras.layers.LSTM(128, batch_input_shape=(1, 1, 45), return_sequences=False, stateful=True),\n",
        "    keras.layers.Dense(45, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 모델을 컴파일합니다.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 매 에포크마다 훈련과 검증의 손실 및 정확도를 기록하기 위한 변수\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "\n",
        "\n",
        "# 88회부터 지금까지 1등부터 5등까지 상금의 평균낸다.\n",
        "mean_prize = [  np.mean(rows[87:, 8]),\n",
        "            np.mean(rows[87:, 9]),\n",
        "            np.mean(rows[87:, 10]),\n",
        "            np.mean(rows[87:, 11]),\n",
        "            np.mean(rows[87:, 12])]\n",
        "\n",
        "print(mean_prize)   \n",
        "\n",
        "train_total_reward = []\n",
        "train_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "val_total_reward = []\n",
        "val_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "test_total_reward = []\n",
        "test_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "model.reset_states()\n",
        "\n",
        "# print('[No. ] 1st 2nd 3rd 4th 5th 6th Rewards')\n",
        "\n",
        "# for i in range(len(x_samples)):\n",
        "#     xs = x_samples[i].reshape(1, 1, 45)\n",
        "#     ys_pred = model.predict_on_batch(xs) # 모델의 출력값을 얻음\n",
        "    \n",
        "#     sum_reward = 0\n",
        "#     sum_grade = np.zeros(6, dtype=int) # 6등까지 변수\n",
        "\n",
        "#     for n in range(10): # 10판 수행\n",
        "#         numbers = gen_numbers_from_probability(ys_pred[0])\n",
        "        \n",
        "#         #i회차 입력 후 나온 출력을 i+1회차와 비교함\n",
        "#         grade, reward = calc_reward(rows[i+1,1:7], rows[i+1,7], numbers) \n",
        "\n",
        "#         sum_reward += reward\n",
        "#         sum_grade[grade] += 1\n",
        "\n",
        "#         if i >= train_idx[0] and i < train_idx[1]:\n",
        "#             train_total_grade[grade] += 1\n",
        "#         elif i >= val_idx[0] and i < val_idx[1]:\n",
        "#             val_total_grade[grade] += 1\n",
        "#         elif i >= test_idx[0] and i < test_idx[1]:\n",
        "#             val_total_grade[grade] += 1\n",
        "    \n",
        "#     if i >= train_idx[0] and i < train_idx[1]:\n",
        "#         train_total_reward.append(sum_reward)\n",
        "#     elif i >= val_idx[0] and i < val_idx[1]:\n",
        "#         val_total_reward.append(sum_reward)\n",
        "#     elif i >= test_idx[0] and i < test_idx[1]:\n",
        "#         test_total_reward.append(sum_reward)\n",
        "                        \n",
        "#     print('[{0:4d}] {1:3d} {2:3d} {3:3d} {4:3d} {5:3d} {6:3d} {7:15,d}'.format(i+1, sum_grade[0], sum_grade[1], sum_grade[2], sum_grade[3], sum_grade[4], sum_grade[5], int(sum_reward)))\n",
        "\n",
        "# print('Total') \n",
        "# print('==========')    \n",
        "# print('Train {0:5d} {1:5d} {2:5d} {3:5d} {4:5d} {5:5d} {6:15,d}'.format(train_total_grade[0], train_total_grade[1], train_total_grade[2], train_total_grade[3], train_total_grade[4], train_total_grade[5], int(sum(train_total_reward))))\n",
        "# print('Val   {0:5d} {1:5d} {2:5d} {3:5d} {4:5d} {5:5d} {6:15,d}'.format(val_total_grade[0], val_total_grade[1], val_total_grade[2], val_total_grade[3], val_total_grade[4], val_total_grade[5], int(sum(val_total_reward))))\n",
        "# print('Test  {0:5d} {1:5d} {2:5d} {3:5d} {4:5d} {5:5d} {6:15,d}'.format(test_total_grade[0], test_total_grade[1], test_total_grade[2], test_total_grade[3], test_total_grade[4], test_total_grade[5], int(sum(test_total_reward))))\n",
        "# print('==========')   "
      ],
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-rc1 in /usr/local/lib/python3.7/dist-packages (2.0.0rc1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.34.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.1.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.15.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.1.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.8.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (3.12.4)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.2.0)\n",
            "Requirement already satisfied: tb-nightly<1.15.0a20190807,>=1.15.0a20190806 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.15.0a20190806)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.12.0)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.14.0.dev2019080601)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.36.2)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.4.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (3.3.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.19.5)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0-rc1) (3.1.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-rc1) (56.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (3.3.4)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==2.0.0-rc1) (1.5.2)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (4.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (3.7.4.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (3.4.1)\n",
            "ohbins\n",
            "X[0]: [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.\n",
            " 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            "Y[0]: [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.\n",
            " 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            "numbers\n",
            "X[0]: [10, 23, 29, 33, 37, 40]\n",
            "Y[0]: [9, 13, 21, 25, 32, 42]\n",
            "train: (0, 700), val: (700, 800), test: (800, 964)\n",
            "[2432219832.8667426, 57182024.93166287, 1451067.9487471527, 52814.325740318905, 5000.0]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jigZv6GgC35S",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 340
        },
        "outputId": "db8f6407-ff4d-490b-8f8a-c0f54900638b"
      },
      "source": [
        "model = tf.keras.models.load_model('./drive/MyDrive/my_model.h5')\n",
        "model.summary()"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-a44379a18188>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./drive/MyDrive/my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m   raise IOError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    119\u001b[0m   \u001b[0;31m# Look for metadata file or parse the SavedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m   \u001b[0mobject_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0mpath_to_metadata_pb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_METADATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n\u001b[0;32m--> 116\u001b[0;31m          constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: ./drive/MyDrive/my_model.h5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S6KaOT3lyq5"
      },
      "source": [
        "# **Trainning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyJvMPVl5QV8",
        "outputId": "1a70fd96-2cbd-4d8b-aebf-0d016f103efc",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "# 최대 100번 에포크까지 수행\n",
        "\n",
        "# function ClickConnect(){\n",
        "#     console.log(\"코랩 연결 끊김 방지\"); \n",
        "#     document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "# }\n",
        "# setInterval(ClickConnect, 60 * 1000)\n",
        "\n",
        "model = tf.keras.models.load_model('./drive/MyDrive/my_model.h5')\n",
        "model.summary()\n",
        "\n",
        "for epoch in range(50):\n",
        "\n",
        "    model.reset_states() # 중요! 매 에포크마다 1회부터 다시 훈련하므로 상태 초기화 필요\n",
        "\n",
        "    batch_train_loss = []\n",
        "    batch_train_acc = []\n",
        "\n",
        "    for i in range(len(x_samples)):\n",
        "        \n",
        "        xs = x_samples[i].reshape(1, 1, 45)\n",
        "        ys = y_samples[i].reshape(1, 45)\n",
        "        \n",
        "        loss, acc = model.train_on_batch(xs, ys) #배치만큼 모델에 학습시킴\n",
        "\n",
        "        batch_train_loss.append(loss)\n",
        "        batch_train_acc.append(acc)\n",
        "\n",
        "    train_loss.append(np.mean(batch_train_loss))\n",
        "    train_acc.append(np.mean(batch_train_acc))\n",
        "\n",
        "    print('epoch {0:4d} train acc {1:0.3f} loss {2:0.3f}'.format(epoch, np.mean(batch_train_acc), np.mean(batch_train_loss)))  \n",
        "\n",
        "\n",
        "# Trainning 끝나면 무조건 모델에 저장\n",
        "model.save('my_model.h5')\n",
        "!cp /content/my_model.h5 /content/drive/My\\ Drive/\n",
        "\n",
        "# 마지막 회차까지 학습한 모델로 다음 회차 추론\n",
        "\n",
        " # 등수와 상금을 반환함\n",
        "# 순위에 오르지 못한 경우에는 등수가 0으로 반환함\n",
        "def calc_reward(true_numbers, true_bonus, pred_numbers):\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for ps in pred_numbers:\n",
        "        if ps in true_numbers:\n",
        "            count += 1\n",
        "\n",
        "    if count == 6:\n",
        "        return 0, mean_prize[0]\n",
        "    elif count == 5 and true_bonus in pred_numbers:\n",
        "        return 1, mean_prize[1]\n",
        "    elif count == 5:\n",
        "        return 2, mean_prize[2]\n",
        "    elif count == 4:\n",
        "        return 3, mean_prize[3]\n",
        "    elif count == 3:\n",
        "        return 4, mean_prize[4]\n",
        "\n",
        "    return 5, 0\n",
        "\n",
        "def gen_numbers_from_probability(nums_prob):\n",
        "\n",
        "    ball_box = []\n",
        "\n",
        "    for n in range(45):\n",
        "        ball_count = int(nums_prob[n] * 100 + 1)\n",
        "        ball = np.full((ball_count), n+1) #1부터 시작\n",
        "        ball_box += list(ball)\n",
        "\n",
        "    selected_balls = []\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        if len(selected_balls) == 6:\n",
        "            break\n",
        "        \n",
        "        ball_index = np.random.randint(len(ball_box), size=1)[0]\n",
        "        ball = ball_box[ball_index]\n",
        "\n",
        "        if ball not in selected_balls:\n",
        "            selected_balls.append(ball)\n",
        "\n",
        "   \n",
        "    selected_balls.sort()\n",
        "\n",
        "    return selected_balls\n",
        "\n",
        "print('receive numbers')\n",
        "\n",
        "xs = x_samples[-1].reshape(1, 1, 45)\n",
        "\n",
        "ys_pred = model.predict_on_batch(xs)\n",
        "\n",
        "list_numbers = []\n",
        "\n",
        "# 50 개 뽑기\n",
        "# for n in range(100):\n",
        "#     numbers = gen_numbers_from_probability(ys_pred[0])  \n",
        "#     print('{0} : {1}'.format(n, numbers))    \n",
        "#     list_numbers.append(numbers)  \n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "with open(path.join(lotto_base_dir, \"predict.txt\"), \"w\") as f:\n",
        "  print (\"predict.txt\")\n",
        "\n",
        "  for n in range(50):\n",
        "    numbers = gen_numbers_from_probability(ys_pred[0])  \n",
        "    print('{0} : {1}'.format(n, numbers))    \n",
        "    list_numbers.append(numbers)  \n",
        "    line_str=','.join(str(e) for e in numbers)\n",
        "    line_str += '\\n'\n",
        "    print(line_str)\n",
        "    f.write(line_str)\n",
        "f.close()"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-b46e1ab03b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# setInterval(ClickConnect, 60 * 1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./drive/MyDrive/my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m   raise IOError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    119\u001b[0m   \u001b[0;31m# Look for metadata file or parse the SavedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m   \u001b[0mobject_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0mpath_to_metadata_pb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_METADATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n\u001b[0;32m--> 116\u001b[0;31m          constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: ./drive/MyDrive/my_model.h5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzXKsUUUmCyg"
      },
      "source": [
        "# **Prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcvMpyO_oXCF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzZWF0ns5Xk6"
      },
      "source": [
        "# 마지막 회차까지 학습한 모델로 다음 회차 추론\n",
        "\n",
        " # 등수와 상금을 반환함\n",
        "# 순위에 오르지 못한 경우에는 등수가 0으로 반환함\n",
        "def calc_reward(true_numbers, true_bonus, pred_numbers):\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for ps in pred_numbers:\n",
        "        if ps in true_numbers:\n",
        "            count += 1\n",
        "\n",
        "    if count == 6:\n",
        "        return 0, mean_prize[0]\n",
        "    elif count == 5 and true_bonus in pred_numbers:\n",
        "        return 1, mean_prize[1]\n",
        "    elif count == 5:\n",
        "        return 2, mean_prize[2]\n",
        "    elif count == 4:\n",
        "        return 3, mean_prize[3]\n",
        "    elif count == 3:\n",
        "        return 4, mean_prize[4]\n",
        "\n",
        "    return 5, 0\n",
        "\n",
        "def gen_numbers_from_probability(nums_prob):\n",
        "\n",
        "    ball_box = []\n",
        "\n",
        "    for n in range(45):\n",
        "        ball_count = int(nums_prob[n] * 100 + 1)\n",
        "        ball = np.full((ball_count), n+1) #1부터 시작\n",
        "        ball_box += list(ball)\n",
        "\n",
        "    selected_balls = []\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        if len(selected_balls) == 6:\n",
        "            break\n",
        "        \n",
        "        ball_index = np.random.randint(len(ball_box), size=1)[0]\n",
        "        ball = ball_box[ball_index]\n",
        "\n",
        "        if ball not in selected_balls:\n",
        "            selected_balls.append(ball)\n",
        "\n",
        "   \n",
        "    selected_balls.sort()\n",
        "\n",
        "    return selected_balls\n",
        "\n",
        "print('receive numbers')\n",
        "\n",
        "xs = x_samples[-1].reshape(1, 1, 45)\n",
        "\n",
        "ys_pred = model.predict_on_batch(xs)\n",
        "\n",
        "list_numbers = []\n",
        "\n",
        "# 50 개 뽑기\n",
        "# for n in range(100):\n",
        "#     numbers = gen_numbers_from_probability(ys_pred[0])  \n",
        "#     print('{0} : {1}'.format(n, numbers))    \n",
        "#     list_numbers.append(numbers)  \n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "with open(path.join(lotto_base_dir, \"predict.txt\"), \"w\") as f:\n",
        "  print (\"predict.txt\")\n",
        "\n",
        "  for n in range(50):\n",
        "    numbers = gen_numbers_from_probability(ys_pred[0])  \n",
        "    print('{0} : {1}'.format(n, numbers))    \n",
        "    list_numbers.append(numbers)  \n",
        "    line_str=','.join(str(e) for e in numbers)\n",
        "    line_str += '\\n'\n",
        "    print(line_str)\n",
        "    f.write(line_str)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOjlv9p90YKR"
      },
      "source": [
        "# **Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzmxpXaV0dCc"
      },
      "source": [
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvzmqDDf17WD"
      },
      "source": [
        "# **Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqdRq1Rn2CKo",
        "outputId": "4742e64b-9744-4309-ce71-5dd3e12fa41e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        }
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "new_model = tf.keras.models.load_model('./drive/MyDrive/my_model.h5')\n",
        "new_model.summary()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-42-eb2daf5db835>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./drive/MyDrive/my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m   raise IOError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    119\u001b[0m   \u001b[0;31m# Look for metadata file or parse the SavedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m   \u001b[0mobject_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0mpath_to_metadata_pb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_METADATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n\u001b[0;32m--> 116\u001b[0;31m          constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: ./drive/MyDrive/my_model.h5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LZbvqHB5E3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc18634-71a0-4a2b-aff2-01fbdf9930f3"
      },
      "source": [
        "!cp /content/my_model.h5 /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: failed to access '/content/drive/My Drive/': Transport endpoint is not connected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBxRkkEHVwU1"
      },
      "source": [
        "# FTP 전송"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1v3JlaOWKAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762e78ea-c5c2-4706-ef40-dc8a62b3e09b"
      },
      "source": [
        "from ftplib import FTP\n",
        "\n",
        "ftp = FTP('112.175.184.78')\n",
        "ftp.login('dalasjoe', 'Dalasjoe75!')\n",
        "\n",
        "# ftp.cwd('html') # \"test\"디렉터리로 이동\n",
        "# ftp.retrlines('LIST') # 디렉터리의 내용을 목록화\n",
        "# #ftp.retrbinary('RETR README', open('README', 'wb').write) # README 파일 저장\n",
        "# ftp.quit()\n",
        "\n",
        "\n",
        "ftp.cwd('html')  # 업로드할 FTP 폴더로 이동\n",
        "myfile = open(path.join(lotto_base_dir, \"predict.txt\"),'rb')  # 로컬 파일 열기\n",
        "ftp.storbinary('STOR ' + 'predict.txt', myfile )  # 파일을 FTP로 업로드\n",
        "myfile.close()  # 파일 닫기\n",
        "\n",
        "print (\"File Saved\")"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVn3rfIyW7OG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSlY9i6yW511"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
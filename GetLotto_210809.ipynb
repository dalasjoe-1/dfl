{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GetLotto.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Q8MptZ7cjH8T"
      ],
      "authorship_tag": "ABX9TyNbqP8XAJfE+HYZZhEx2fMD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dalasjoe-1/dfl/blob/master/GetLotto_210809.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kCmxwN5Rkt1"
      },
      "source": [
        "# Another Way"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "\n"
      ],
      "metadata": {
        "id": "YUMnt3-jT5MU",
        "outputId": "c0c357dd-f08d-4572-8c75-1c02a1dbc5ee",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "input = np.loadtxt(\"./gdrive/MyDrive/lotto_data.txt\", delimiter=\",\", dtype='int')\n",
        "print (input)"
      ],
      "metadata": {
        "id": "UgRR8LxrUJWs",
        "outputId": "31d4b73f-3705-48b2-b91d-b32f4eb3c3a6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 193
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-2-a9e7987e1851>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadtxt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"./gdrive/MyDrive/lotto_data.txt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdelimiter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\",\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'int'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl8KqROtQ5oG",
        "outputId": "e32e1167-d6dc-41e5-8b18-3cc85d189502",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "tf.compat.v1.set_random_seed(777)\n",
        "\n",
        "input_data_column_cnt = 45  # 입력데이터의 컬럼 개수(Variable 개수)\n",
        "#output_data_column_cnt = 1  # 결과데이터의 컬럼 개수\n",
        "output_data_column_cnt = 45  # 결과데이터의 컬럼 개수\n",
        "\n",
        "seq_length = 46 #192 #24 #96  # 1개 시퀀스의 길이(시계열데이터 입력 개수)\n",
        "rnn_cell_hidden_dim = 20  # 각 셀의 (hidden)출력 크기\n",
        "forget_bias = 1.0  # 망각편향(기본값 1.0)\n",
        "num_stacked_layers = 1  # stacked LSTM layers 개수\n",
        "keep_prob = 1.0  # dropout할 때 keep할 비율\n",
        "\n",
        "epoch_num = 10000 #20000  # 에폭 횟수(학습용전체데이터를 몇 회 반복해서 학습할 것인가 입력)\n",
        "learning_rate = 0.01  # 학습률\n",
        "\n",
        "# Standardization\n",
        "def data_standardization(x):\n",
        "    x_np = np.asarray(x)\n",
        "    return (x_np - x_np.mean()) / x_np.std()\n",
        "\n",
        "# 너무 작거나 너무 큰 값이 학습을 방해하는 것을 방지하고자 정규화한다\n",
        "# x가 양수라는 가정하에 최소값과 최대값을 이용하여 0~1사이의 값으로 변환\n",
        "# Min-Max scaling\n",
        "def min_max_scaling(x):\n",
        "    x_np = np.asarray(x)\n",
        "    return (x_np - x_np.min()) / (x_np.max() - x_np.min() + 1e-7)  # 1e-7은 0으로 나누는 오류 예방차원\n",
        "\n",
        "# 정규화된 값을 원래의 값으로 되돌린다\n",
        "# 정규화하기 이전의 org_x값과 되돌리고 싶은 x를 입력하면 역정규화된 값을 리턴한다\n",
        "def reverse_min_max_scaling(org_x, x):\n",
        "    org_x_np = np.asarray(org_x)\n",
        "    x_np = np.asarray(x)\n",
        "    return (x_np * (org_x_np.max() - org_x_np.min() + 1e-7)) + org_x_np.min()\n",
        "\n",
        "# data\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "#input = np.loadtxt('./gdrive/MyDrive/ori.data', unpack=True, dtype='int')\n",
        "input = np.loadtxt(\"./gdrive/MyDrive/lotto_data.txt\", delimiter=\",\", dtype='int')\n",
        "#data = np.transpose(input)\n",
        "data = input\n",
        "#win_numbers = data[:,1:data.size]\n",
        "win_numbers = data[:,1:7]\n",
        "\n",
        "#print(win_numbers, \" : \", win_numbers.size)\n",
        "norm_win_numbers = min_max_scaling(win_numbers)  # 가격형태 데이터 정규화 처리\n",
        "\n",
        "#x = min_max_scaling(win_numbers);\n",
        "x = win_numbers\n",
        "y = x[:, :]\n",
        "z = []\n",
        "\n",
        "for i in range(len(data)):\n",
        "    week = win_numbers[i]  \n",
        "    buckets = [0, 0, 0, 0, 0, 0, 0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0]\n",
        "    for j in range(len(week)):\n",
        "        k = week[j]\n",
        "        buckets[k-1] = 100\n",
        "    z.append(buckets)\n",
        "\n",
        "x = z;\n",
        "y = x;\n",
        "\n",
        "dataX = []  # 입력으로 사용될 Sequence Data\n",
        "dataY = []  # 출력(타켓)으로 사용\n",
        "\n",
        "for i in range(0, len(y) - seq_length):\n",
        "    _x = x[i: i + seq_length]\n",
        "    _y = y[i + seq_length]  # 다음 나타날 주가(정답)\n",
        "    # if i is 0:\n",
        "    #      print(\">>> \", _x, \"->\", _y)  # 첫번째 행만 출력해 봄\n",
        "\n",
        "    dataX.append(_x)  # dataX 리스트에 추가\n",
        "    dataY.append(_y)  # dataY 리스트에 추가\n",
        "\n",
        "# 학습용/테스트용 데이터 생성\n",
        "# 전체 70%를 학습용 데이터로 사용\n",
        "train_size = int(len(dataY) * 0.7)\n",
        "# 나머지(30%)를 테스트용 데이터로 사용\n",
        "test_size = len(dataY) - train_size\n",
        "\n",
        "\n",
        "# 데이터를 잘라 학습용 데이터 생성\n",
        "trainX = np.asarray(dataX[0:train_size])\n",
        "trainY = np.asarray(dataY[0:train_size])\n",
        "\n",
        "# for i in range(len(trainX)):\n",
        "#     aaa = trainX[i]\n",
        "#     if (i==0):\n",
        "\n",
        "print(trainX.shape, \" ==== \", trainY.shape)\n",
        "# print(trainX[0], \" **** \", dataX[0])\n",
        "\n",
        "\n",
        "# 데이터를 잘라 테스트용 데이터 생성\n",
        "testX = np.array(dataX[train_size:len(dataX)])\n",
        "testY = np.array(dataY[train_size:len(dataY)])\n",
        "\n",
        "# 텐서플로우 플레이스홀더 생성\n",
        "# 입력 X, 출력 Y를 생성한다\n",
        "X = tf.compat.v1.placeholder(tf.float32, [None, seq_length, input_data_column_cnt])\n",
        "print(\"X: \", X)\n",
        "Y = tf.compat.v1.placeholder(tf.float32, [None, output_data_column_cnt])\n",
        "print(\"Y: \", Y)\n",
        "\n",
        "# 검증용 측정지표를 산출하기 위한 targets, predictions를 생성한다\n",
        "targets = tf.compat.v1.placeholder(tf.float32, [None, output_data_column_cnt])\n",
        "print(\"targets: \", targets)\n",
        "\n",
        "predictions = tf.compat.v1.placeholder(tf.float32, [None, output_data_column_cnt])\n",
        "print(\"predictions: \", predictions)\n",
        "\n",
        "def lstm_cell():\n",
        "    # LSTM셀을 생성\n",
        "    # num_units: 각 Cell 출력 크기\n",
        "    # forget_bias:  to the biases of the forget gate\n",
        "    #              (default: 1)  in order to reduce the scale of forgetting in the beginning of the training.\n",
        "    # state_is_tuple: True ==> accepted and returned states are 2-tuples of the c_state and m_state.\n",
        "    # state_is_tuple: False ==> they are concatenated along the column axis.\n",
        "\n",
        "    cell = tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=rnn_cell_hidden_dim,\n",
        "                                        forget_bias=forget_bias, state_is_tuple=True, activation=tf.nn.softsign)\n",
        "   \n",
        "    # cell = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_cell_hidden_dim,\n",
        "    #                                     forget_bias=forget_bias, state_is_tuple=True, activation=tf.nn.softsign)\n",
        "    #cell = tf.keras.layers.LSTMCell(num_units=rnn_cell_hidden_dim, forget_bias=forget_bias\n",
        "    #                                 , state_is_tuple=True, activation=tf.nn.softsign)\n",
        "    if keep_prob < 1.0:\n",
        "        #cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
        "        cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
        "    return cell\n",
        "\n",
        "\n",
        "# num_stacked_layers개의 층으로 쌓인 Stacked RNNs 생성\n",
        "stackedRNNs = [lstm_cell() for _ in range(num_stacked_layers)]\n",
        "multi_cells = tf.contrib.rnn.MultiRNNCell(stackedRNNs, state_is_tuple=True) if num_stacked_layers > 1 else lstm_cell()\n",
        "\n",
        "hypothesis, _states = tf.compat.v1.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32)\n",
        "#hypothesis, _states = tf.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32)\n",
        "print(\"hypothesis: \", hypothesis)\n",
        "\n",
        "# [:, -1]를 잘 살펴보자. LSTM RNN의 마지막 (hidden)출력만을 사용했다.\n",
        "# 과거 여러 거래일의 주가를 이용해서 다음날의 주가 1개를 예측하기때문에 MANY-TO-ONE형태이다\n",
        "#hypothesis = tf.contrib.layers.fully_connected(hypothesis[:, -1], output_data_column_cnt, activation_fn=tf.identity)\n",
        "hypothesis = tf.compat.v1.layers.dense(hypothesis[:, -1], output_data_column_cnt, activation = tf.identity)\n",
        "\n",
        "\n",
        "\n",
        "# 손실함수로 평균제곱오차를 사용한다\n",
        "loss = tf.reduce_sum(tf.square(hypothesis - Y))\n",
        "# 최적화함수로 AdamOptimizer를 사용한다\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "# optimizer = tf.train.RMSPropOptimizer(learning_rate) # LSTM과 궁합 별로임\n",
        "\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "# RMSE(Root Mean Square Error)\n",
        "# 제곱오차의 평균을 구하고 다시 제곱근을 구하면 평균 오차가 나온다\n",
        "# rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions))) # 아래 코드와 같다\n",
        "rmse = tf.sqrt(tf.reduce_mean(tf.math.squared_difference(targets, predictions)))\n",
        "\n",
        "train_error_summary = []  # 학습용 데이터의 오류를 중간 중간 기록한다\n",
        "test_error_summary = []  # 테스트용 데이터의 오류를 중간 중간 기록한다\n",
        "test_predict = ''  # 테스트용데이터로 예측한 결과\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "print(\"trainX.shape=\", trainX.shape, \"trainY.shape=\", trainY.shape)\n",
        "print(\"trainX=\", trainX)\n",
        "\n",
        "# 학습한다\n",
        "start_time = datetime.datetime.now()  # 시작시간을 기록한다\n",
        "print('학습을 시작합니다...')\n",
        "for epoch in range(epoch_num):\n",
        "    _, _loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
        "    if ((epoch + 1) % 100 == 0) or (epoch == epoch_num - 1):  # 100번째마다 또는 마지막 epoch인 경우\n",
        "        # 학습용데이터로 rmse오차를 구한다\n",
        "        train_predict = sess.run(hypothesis, feed_dict={X: trainX})\n",
        "        train_error = sess.run(rmse, feed_dict={targets: trainY, predictions: train_predict})\n",
        "        train_error_summary.append(train_error)\n",
        "\n",
        "        # 테스트용데이터로 rmse오차를 구한다\n",
        "        test_predict = sess.run(hypothesis, feed_dict={X: testX})\n",
        "        test_error = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
        "        test_error_summary.append(test_error)\n",
        "\n",
        "        # 현재 오류를 출력한다\n",
        "        print(\"epoch: {}, train_error(A): {}, test_error(B): {}, B-A: {}\".format(epoch + 1, train_error, test_error,\n",
        "                                                                                 test_error - train_error))\n",
        "\n",
        "# end_time = datetime.datetime.now()  # 종료시간을 기록한다\n",
        "# elapsed_time = end_time - start_time  # 경과시간을 구한다\n",
        "# print('elapsed_time:', elapsed_time)\n",
        "# print('elapsed_time per epoch:', elapsed_time / epoch_num)\n",
        "#\n",
        "# # 하이퍼파라미터 출력\n",
        "# print('input_data_column_cnt:', input_data_column_cnt, end='')\n",
        "# print(',output_data_column_cnt:', output_data_column_cnt, end='')\n",
        "#\n",
        "# print(',seq_length:', seq_length, end='')\n",
        "# print(',rnn_cell_hidden_dim:', rnn_cell_hidden_dim, end='')\n",
        "# print(',forget_bias:', forget_bias, end='')\n",
        "# print(',num_stacked_layers:', num_stacked_layers, end='')\n",
        "# print(',keep_prob:', keep_prob, end='')\n",
        "#\n",
        "# print(',epoch_num:', epoch_num, end='')\n",
        "# print(',learning_rate:', learning_rate, end='')\n",
        "#\n",
        "# print(',train_error:', train_error_summary[-1], end='')\n",
        "# print(',test_error:', test_error_summary[-1], end='')\n",
        "# print(',min_test_error:', np.min(test_error_summary))\n",
        "#\n",
        "# # 결과 그래프 출력\n",
        "# plt.figure(1)\n",
        "# plt.plot(train_error_summary, 'gold')\n",
        "# plt.plot(test_error_summary, 'b')\n",
        "# plt.xlabel('Epoch(x100)')\n",
        "# plt.ylabel('Root Mean Square Error')\n",
        "#\n",
        "# plt.figure(2)\n",
        "# plt.plot(testY, 'r')\n",
        "# plt.plot(test_predict, 'b')\n",
        "# plt.xlabel('Time Period')\n",
        "# plt.ylabel('Stock Price')\n",
        "# plt.show()\n",
        "\n",
        "# sequence length만큼의 가장 최근 데이터를 슬라이싱한다\n",
        "recent_data = np.array([x[len(x) - seq_length:]])\n",
        "print(\"recent_data.shape:\", recent_data.shape)\n",
        "print(\"recent_data:\", recent_data)\n",
        "\n",
        "# 내일 종가를 예측해본다\n",
        "test_predict = sess.run(hypothesis, feed_dict={X: recent_data})\n",
        "\n",
        "test_predict_2 = []\n",
        "\n",
        "for i in range(len(test_predict[0])):\n",
        "    test_predict[0][i] = int(test_predict[0][i])\n",
        "    test_predict_2.append((test_predict[0][i], i))\n",
        "\n",
        "#final_predict = np.sort(test_predict_2, axis=1)[::-1]\n",
        "print(\"test_predict_2  00000 =\", test_predict_2)\n",
        "test_predict_2.sort(key = lambda element : element[0], reverse=True)\n",
        "\n",
        "\n",
        "# predict_six = []\n",
        "#\n",
        "# for i in final_predict:\n",
        "#     for j in test_predict[0]:\n",
        "#         if (final_predict[i] == test_predict[0][j])\n",
        "\n",
        "\n",
        "#print(\"test_predict 0 \", test_predict[0])\n",
        "print(\"test_predict_2  11111\", test_predict_2)\n",
        "\n",
        "picked = []\n",
        "for i in range(6):\n",
        "    picked.append(test_predict_2[i][1] + 1)\n",
        "\n",
        "print(picked)"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "(698, 46, 45)  ====  (698, 45)\n",
            "X:  Tensor(\"Placeholder:0\", shape=(None, 46, 45), dtype=float32)\n",
            "Y:  Tensor(\"Placeholder_1:0\", shape=(None, 45), dtype=float32)\n",
            "targets:  Tensor(\"Placeholder_2:0\", shape=(None, 45), dtype=float32)\n",
            "predictions:  Tensor(\"Placeholder_3:0\", shape=(None, 45), dtype=float32)\n",
            "WARNING:tensorflow:From <ipython-input-1-8a354922dc42>:156: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:992: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:140: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:988: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  partitioner=maybe_partitioner)\n",
            "/usr/local/lib/python3.7/dist-packages/keras/layers/legacy_rnn/rnn_cell_impl.py:996: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  initializer=initializer)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hypothesis:  Tensor(\"rnn/transpose_1:0\", shape=(None, 46, 20), dtype=float32)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:163: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "/usr/local/lib/python3.7/dist-packages/keras/legacy_tf_layers/core.py:255: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  return layer.apply(inputs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "trainX.shape= (698, 46, 45) trainY.shape= (698, 45)\n",
            "trainX= [[[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [  0   0 100 ...   0   0 100]\n",
            "  [100   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [100   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0 100]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0 100]\n",
            "  [  0   0   0 ...   0   0   0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[100   0   0 ...   0   0   0]\n",
            "  [  0 100   0 ...   0   0   0]\n",
            "  [  0   0 100 ...   0   0   0]\n",
            "  ...\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0 100 100]]\n",
            "\n",
            " [[  0 100   0 ...   0   0   0]\n",
            "  [  0   0 100 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0 100 100]\n",
            "  [  0   0   0 ...   0   0   0]]\n",
            "\n",
            " [[  0   0 100 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0 100   0]\n",
            "  ...\n",
            "  [  0   0   0 ...   0 100 100]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0 100   0]]]\n",
            "학습을 시작합니다...\n",
            "epoch: 100, train_error(A): 33.9439811706543, test_error(B): 34.02452850341797, B-A: 0.08054733276367188\n",
            "epoch: 200, train_error(A): 33.91362762451172, test_error(B): 34.037784576416016, B-A: 0.12415695190429688\n",
            "epoch: 300, train_error(A): 33.88751220703125, test_error(B): 34.04133224487305, B-A: 0.15382003784179688\n",
            "epoch: 400, train_error(A): 33.8645133972168, test_error(B): 34.057186126708984, B-A: 0.1926727294921875\n",
            "epoch: 500, train_error(A): 33.845191955566406, test_error(B): 34.059295654296875, B-A: 0.21410369873046875\n",
            "epoch: 600, train_error(A): 33.82588577270508, test_error(B): 34.07898712158203, B-A: 0.2531013488769531\n",
            "epoch: 700, train_error(A): 33.811771392822266, test_error(B): 34.10657501220703, B-A: 0.2948036193847656\n",
            "epoch: 800, train_error(A): 33.80155944824219, test_error(B): 34.108482360839844, B-A: 0.30692291259765625\n",
            "epoch: 900, train_error(A): 33.79179763793945, test_error(B): 34.125205993652344, B-A: 0.3334083557128906\n",
            "epoch: 1000, train_error(A): 33.784854888916016, test_error(B): 34.12967300415039, B-A: 0.344818115234375\n",
            "epoch: 1100, train_error(A): 33.77903366088867, test_error(B): 34.14544677734375, B-A: 0.3664131164550781\n",
            "epoch: 1200, train_error(A): 33.77963638305664, test_error(B): 34.174476623535156, B-A: 0.3948402404785156\n",
            "epoch: 1300, train_error(A): 33.76907730102539, test_error(B): 34.15998840332031, B-A: 0.3909111022949219\n",
            "epoch: 1400, train_error(A): 33.759742736816406, test_error(B): 34.17363357543945, B-A: 0.4138908386230469\n",
            "epoch: 1500, train_error(A): 33.75507736206055, test_error(B): 34.17243576049805, B-A: 0.4173583984375\n",
            "epoch: 1600, train_error(A): 33.75215530395508, test_error(B): 34.180912017822266, B-A: 0.4287567138671875\n",
            "epoch: 1700, train_error(A): 33.749671936035156, test_error(B): 34.19221878051758, B-A: 0.4425468444824219\n",
            "epoch: 1800, train_error(A): 33.74836349487305, test_error(B): 34.202049255371094, B-A: 0.4536857604980469\n",
            "epoch: 1900, train_error(A): 33.74567413330078, test_error(B): 34.20038604736328, B-A: 0.4547119140625\n",
            "epoch: 2000, train_error(A): 33.74211120605469, test_error(B): 34.18279266357422, B-A: 0.44068145751953125\n",
            "epoch: 2100, train_error(A): 33.74031066894531, test_error(B): 34.212093353271484, B-A: 0.4717826843261719\n",
            "epoch: 2200, train_error(A): 33.74037170410156, test_error(B): 34.22206115722656, B-A: 0.481689453125\n",
            "epoch: 2300, train_error(A): 33.747493743896484, test_error(B): 34.196449279785156, B-A: 0.4489555358886719\n",
            "epoch: 2400, train_error(A): 33.7376594543457, test_error(B): 34.203041076660156, B-A: 0.4653816223144531\n",
            "epoch: 2500, train_error(A): 33.736778259277344, test_error(B): 34.203399658203125, B-A: 0.46662139892578125\n",
            "epoch: 2600, train_error(A): 33.73577880859375, test_error(B): 34.19972229003906, B-A: 0.4639434814453125\n",
            "epoch: 2700, train_error(A): 33.73433303833008, test_error(B): 34.2094841003418, B-A: 0.47515106201171875\n",
            "epoch: 2800, train_error(A): 33.70261764526367, test_error(B): 34.24769592285156, B-A: 0.5450782775878906\n",
            "epoch: 2900, train_error(A): 33.679100036621094, test_error(B): 34.22835922241211, B-A: 0.5492591857910156\n",
            "epoch: 3000, train_error(A): 33.66954803466797, test_error(B): 34.251564025878906, B-A: 0.5820159912109375\n",
            "epoch: 3100, train_error(A): 33.665138244628906, test_error(B): 34.29898452758789, B-A: 0.6338462829589844\n",
            "epoch: 3200, train_error(A): 33.663883209228516, test_error(B): 34.31104278564453, B-A: 0.6471595764160156\n",
            "epoch: 3300, train_error(A): 33.6339225769043, test_error(B): 34.32766342163086, B-A: 0.6937408447265625\n",
            "epoch: 3400, train_error(A): 33.58132553100586, test_error(B): 34.35002136230469, B-A: 0.7686958312988281\n",
            "epoch: 3500, train_error(A): 33.553062438964844, test_error(B): 34.3795280456543, B-A: 0.8264656066894531\n",
            "epoch: 3600, train_error(A): 33.530033111572266, test_error(B): 34.42879867553711, B-A: 0.8987655639648438\n",
            "epoch: 3700, train_error(A): 33.454078674316406, test_error(B): 34.524173736572266, B-A: 1.0700950622558594\n",
            "epoch: 3800, train_error(A): 33.39409255981445, test_error(B): 34.72492599487305, B-A: 1.3308334350585938\n",
            "epoch: 3900, train_error(A): 33.30610656738281, test_error(B): 34.79246139526367, B-A: 1.4863548278808594\n",
            "epoch: 4000, train_error(A): 33.27106475830078, test_error(B): 34.894351959228516, B-A: 1.6232872009277344\n",
            "epoch: 4100, train_error(A): 33.22308349609375, test_error(B): 35.06929397583008, B-A: 1.8462104797363281\n",
            "epoch: 4200, train_error(A): 33.18362045288086, test_error(B): 35.131675720214844, B-A: 1.9480552673339844\n",
            "epoch: 4300, train_error(A): 33.126304626464844, test_error(B): 35.25045394897461, B-A: 2.1241493225097656\n",
            "epoch: 4400, train_error(A): 33.0516357421875, test_error(B): 35.46026611328125, B-A: 2.40863037109375\n",
            "epoch: 4500, train_error(A): 33.0174560546875, test_error(B): 35.55580520629883, B-A: 2.538349151611328\n",
            "epoch: 4600, train_error(A): 32.90666198730469, test_error(B): 35.6562614440918, B-A: 2.7495994567871094\n",
            "epoch: 4700, train_error(A): 32.848487854003906, test_error(B): 35.66181945800781, B-A: 2.8133316040039062\n",
            "epoch: 4800, train_error(A): 32.86568832397461, test_error(B): 35.676025390625, B-A: 2.8103370666503906\n",
            "epoch: 4900, train_error(A): 32.7849006652832, test_error(B): 35.92030715942383, B-A: 3.135406494140625\n",
            "epoch: 5000, train_error(A): 32.74690628051758, test_error(B): 36.00630569458008, B-A: 3.2593994140625\n",
            "epoch: 5100, train_error(A): 32.703826904296875, test_error(B): 36.085697174072266, B-A: 3.3818702697753906\n",
            "epoch: 5200, train_error(A): 32.689697265625, test_error(B): 36.064476013183594, B-A: 3.3747787475585938\n",
            "epoch: 5300, train_error(A): 32.70295715332031, test_error(B): 36.04487991333008, B-A: 3.3419227600097656\n",
            "epoch: 5400, train_error(A): 32.655208587646484, test_error(B): 36.10772705078125, B-A: 3.4525184631347656\n",
            "epoch: 5500, train_error(A): 32.631771087646484, test_error(B): 36.19788360595703, B-A: 3.566112518310547\n",
            "epoch: 5600, train_error(A): 32.60625076293945, test_error(B): 36.22695541381836, B-A: 3.6207046508789062\n",
            "epoch: 5700, train_error(A): 32.534873962402344, test_error(B): 36.2136116027832, B-A: 3.6787376403808594\n",
            "epoch: 5800, train_error(A): 32.5389404296875, test_error(B): 36.21226119995117, B-A: 3.673320770263672\n",
            "epoch: 5900, train_error(A): 32.51083755493164, test_error(B): 36.37268829345703, B-A: 3.8618507385253906\n",
            "epoch: 6000, train_error(A): 32.49864196777344, test_error(B): 36.424686431884766, B-A: 3.926044464111328\n",
            "epoch: 6100, train_error(A): 32.52231979370117, test_error(B): 36.49807357788086, B-A: 3.9757537841796875\n",
            "epoch: 6200, train_error(A): 32.513275146484375, test_error(B): 36.474735260009766, B-A: 3.9614601135253906\n",
            "epoch: 6300, train_error(A): 32.47697067260742, test_error(B): 36.5250244140625, B-A: 4.048053741455078\n",
            "epoch: 6400, train_error(A): 32.426273345947266, test_error(B): 36.58334732055664, B-A: 4.157073974609375\n",
            "epoch: 6500, train_error(A): 32.399906158447266, test_error(B): 36.71635055541992, B-A: 4.316444396972656\n",
            "epoch: 6600, train_error(A): 32.38853073120117, test_error(B): 36.98568344116211, B-A: 4.5971527099609375\n",
            "epoch: 6700, train_error(A): 32.47146224975586, test_error(B): 37.02091598510742, B-A: 4.5494537353515625\n",
            "epoch: 6800, train_error(A): 32.3897819519043, test_error(B): 37.037540435791016, B-A: 4.647758483886719\n",
            "epoch: 6900, train_error(A): 32.3658561706543, test_error(B): 37.15459442138672, B-A: 4.788738250732422\n",
            "epoch: 7000, train_error(A): 32.45029830932617, test_error(B): 37.31694793701172, B-A: 4.866649627685547\n",
            "epoch: 7100, train_error(A): 32.33631134033203, test_error(B): 37.47616958618164, B-A: 5.139858245849609\n",
            "epoch: 7200, train_error(A): 32.34977340698242, test_error(B): 37.356990814208984, B-A: 5.0072174072265625\n",
            "epoch: 7300, train_error(A): 32.416893005371094, test_error(B): 37.48046875, B-A: 5.063575744628906\n",
            "epoch: 7400, train_error(A): 32.300010681152344, test_error(B): 37.52973937988281, B-A: 5.229728698730469\n",
            "epoch: 7500, train_error(A): 32.25544738769531, test_error(B): 37.472373962402344, B-A: 5.216926574707031\n",
            "epoch: 7600, train_error(A): 32.35789489746094, test_error(B): 37.58884048461914, B-A: 5.230945587158203\n",
            "epoch: 7700, train_error(A): 32.39603042602539, test_error(B): 37.47152328491211, B-A: 5.075492858886719\n",
            "epoch: 7800, train_error(A): 32.41965103149414, test_error(B): 37.575111389160156, B-A: 5.155460357666016\n",
            "epoch: 7900, train_error(A): 32.406394958496094, test_error(B): 37.82830047607422, B-A: 5.421905517578125\n",
            "epoch: 8000, train_error(A): 32.27118682861328, test_error(B): 37.57495880126953, B-A: 5.30377197265625\n",
            "epoch: 8100, train_error(A): 32.281219482421875, test_error(B): 37.525115966796875, B-A: 5.243896484375\n",
            "epoch: 8200, train_error(A): 32.17487716674805, test_error(B): 37.726478576660156, B-A: 5.551601409912109\n",
            "epoch: 8300, train_error(A): 32.334693908691406, test_error(B): 37.748233795166016, B-A: 5.413539886474609\n",
            "epoch: 8400, train_error(A): 32.28464889526367, test_error(B): 37.98008728027344, B-A: 5.695438385009766\n",
            "epoch: 8500, train_error(A): 32.222450256347656, test_error(B): 37.92641067504883, B-A: 5.703960418701172\n",
            "epoch: 8600, train_error(A): 32.20323944091797, test_error(B): 38.0280647277832, B-A: 5.824825286865234\n",
            "epoch: 8700, train_error(A): 32.199989318847656, test_error(B): 38.01964569091797, B-A: 5.8196563720703125\n",
            "epoch: 8800, train_error(A): 32.097530364990234, test_error(B): 38.058353424072266, B-A: 5.960823059082031\n",
            "epoch: 8900, train_error(A): 32.245826721191406, test_error(B): 38.12850570678711, B-A: 5.882678985595703\n",
            "epoch: 9000, train_error(A): 32.240455627441406, test_error(B): 37.83448028564453, B-A: 5.594024658203125\n",
            "epoch: 9100, train_error(A): 32.21318054199219, test_error(B): 37.81874084472656, B-A: 5.605560302734375\n",
            "epoch: 9200, train_error(A): 32.12717056274414, test_error(B): 37.93849182128906, B-A: 5.811321258544922\n",
            "epoch: 9300, train_error(A): 32.19039535522461, test_error(B): 37.90546798706055, B-A: 5.7150726318359375\n",
            "epoch: 9400, train_error(A): 32.45305252075195, test_error(B): 37.76643753051758, B-A: 5.313385009765625\n",
            "epoch: 9500, train_error(A): 32.17831039428711, test_error(B): 37.956790924072266, B-A: 5.778480529785156\n",
            "epoch: 9600, train_error(A): 32.112449645996094, test_error(B): 38.22222900390625, B-A: 6.109779357910156\n",
            "epoch: 9700, train_error(A): 32.11896896362305, test_error(B): 38.2373161315918, B-A: 6.11834716796875\n",
            "epoch: 9800, train_error(A): 32.137332916259766, test_error(B): 38.18170928955078, B-A: 6.044376373291016\n",
            "epoch: 9900, train_error(A): 32.04662322998047, test_error(B): 38.42434310913086, B-A: 6.377719879150391\n",
            "epoch: 10000, train_error(A): 32.01655578613281, test_error(B): 38.43909454345703, B-A: 6.422538757324219\n",
            "recent_data.shape: (1, 46, 45)\n",
            "recent_data: [[[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0 100   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0 100   0]\n",
            "  [  0   0   0 ...   0   0 100]]]\n",
            "test_predict_2  00000 = [(2.0, 0), (12.0, 1), (10.0, 2), (-8.0, 3), (40.0, 4), (3.0, 5), (-33.0, 6), (45.0, 7), (5.0, 8), (-28.0, 9), (-25.0, 10), (17.0, 11), (-17.0, 12), (34.0, 13), (64.0, 14), (-7.0, 15), (26.0, 16), (47.0, 17), (47.0, 18), (-33.0, 19), (40.0, 20), (-15.0, 21), (7.0, 22), (38.0, 23), (22.0, 24), (-1.0, 25), (14.0, 26), (-17.0, 27), (-1.0, 28), (33.0, 29), (-9.0, 30), (34.0, 31), (-38.0, 32), (70.0, 33), (57.0, 34), (8.0, 35), (-44.0, 36), (-17.0, 37), (55.0, 38), (21.0, 39), (-3.0, 40), (-12.0, 41), (28.0, 42), (29.0, 43), (54.0, 44)]\n",
            "test_predict_2  11111 [(70.0, 33), (64.0, 14), (57.0, 34), (55.0, 38), (54.0, 44), (47.0, 17), (47.0, 18), (45.0, 7), (40.0, 4), (40.0, 20), (38.0, 23), (34.0, 13), (34.0, 31), (33.0, 29), (29.0, 43), (28.0, 42), (26.0, 16), (22.0, 24), (21.0, 39), (17.0, 11), (14.0, 26), (12.0, 1), (10.0, 2), (8.0, 35), (7.0, 22), (5.0, 8), (3.0, 5), (2.0, 0), (-1.0, 25), (-1.0, 28), (-3.0, 40), (-7.0, 15), (-8.0, 3), (-9.0, 30), (-12.0, 41), (-15.0, 21), (-17.0, 12), (-17.0, 27), (-17.0, 37), (-25.0, 10), (-28.0, 9), (-33.0, 6), (-33.0, 19), (-38.0, 32), (-44.0, 36)]\n",
            "[34, 15, 35, 39, 45, 18]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7T2b3Uw0n_u"
      },
      "source": [
        "# 내일 종가를 예측해본다\n",
        "test_predict = sess.run(hypothesis, feed_dict={X: recent_data})\n",
        "\n",
        "test_predict_2 = []\n",
        "\n",
        "for i in range(len(test_predict[0])):\n",
        "    test_predict[0][i] = int(test_predict[0][i])\n",
        "    test_predict_2.append((test_predict[0][i], i))\n",
        "\n",
        "#final_predict = np.sort(test_predict_2, axis=1)[::-1]\n",
        "print(\"test_predict_2  00000 =\", test_predict_2)\n",
        "test_predict_2.sort(key = lambda element : element[0], reverse=True)\n",
        "\n",
        "\n",
        "# predict_six = []\n",
        "#\n",
        "# for i in final_predict:\n",
        "#     for j in test_predict[0]:\n",
        "#         if (final_predict[i] == test_predict[0][j])\n",
        "\n",
        "\n",
        "#print(\"test_predict 0 \", test_predict[0])\n",
        "print(\"test_predict_2  11111\", test_predict_2)\n",
        "\n",
        "picked = []\n",
        "for i in range(6):\n",
        "    picked.append(test_predict_2[i][1] + 1)\n",
        "\n",
        "print(picked)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP2-bRVzoZ9T"
      },
      "source": [
        "# ALL Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzbFpN7ZokLu",
        "outputId": "187788dc-230b-4f6d-f9e3-bf70ec766642",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "\n",
        "\n",
        "!pip install --user --upgrade pip\n",
        "\n",
        "#!pip install tensorflow-gpu==2.0.0-rc1\n",
        "#!pip install tensorflow-gpu==2.0.1\n",
        "!pip install tensorflow-gpu==2.0.1\n",
        "\n",
        "!pip install 'h5py==2.10.0' --force-reinstall\n",
        "\n",
        "main_url = \"https://www.dhlottery.co.kr/gameResult.do?method=byWin\" # 마지막 회차를 얻기 위한 주소\n",
        "basic_url = \"https://www.dhlottery.co.kr/gameResult.do?method=byWin&drwNo=\" # 임의의 회차를 얻기 위한 주소\n",
        "\n",
        "# 마지막 회차 정보를 가져옴\n",
        "def GetLast(): \n",
        "    resp = requests.get(main_url)\n",
        "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "    result = str(soup.find(\"meta\", {\"id\" : \"desc\", \"name\" : \"description\"})['content'])\n",
        "    s_idx = result.find(\" \")\n",
        "    e_idx = result.find(\"회\")\n",
        "    return int(result[s_idx + 1 : e_idx])\n",
        "\n",
        "# 지정된 파일에 지정된 범위의 회차 정보를 기록함\n",
        "def Crawler(s_count, e_count, fp):\n",
        "    for i in range(s_count , e_count + 1):\n",
        "        crawler_url = basic_url + str(i)\n",
        "        resp = requests.get(crawler_url)\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        text = soup.text\n",
        "\n",
        "        s_idx = text.find(\" 당첨결과\")\n",
        "        s_idx = text.find(\"당첨번호\", s_idx) + 4\n",
        "        e_idx = text.find(\"보너스\", s_idx)\n",
        "        numbers = text[s_idx:e_idx].strip().split()\n",
        "\n",
        "        s_idx = e_idx + 3\n",
        "        e_idx = s_idx + 3\n",
        "        bonus = text[s_idx:e_idx].strip()\n",
        "\n",
        "        s_idx = text.find(\"1등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money1 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"2등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money2 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"3등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money3 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"4등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money4 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"5등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money5 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        line = str(i) + ',' + numbers[0] + ',' + numbers[1] + ',' + numbers[2] + ',' + numbers[3] + ',' + numbers[4] + ',' + numbers[5] + ',' + bonus + ',' + money1 + ',' + money2 + ',' + money3 + ',' + money4 + ',' + money5\n",
        "        print(line)\n",
        "        line += '\\n'\n",
        "        fp.write(line)\n",
        "\n",
        "\n",
        "last = GetLast() # 마지막 회차를 가져옴\n",
        "\n",
        "lotto_dir_name='lotto'\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "if not path.exists(lotto_base_dir):\n",
        "  print('Check your google drive directory. See you file explorer')\n",
        "\n",
        "\n",
        "print (\"Last=\",last)\n",
        "\n",
        "# # 완전히 다시 쓰기\n",
        "# #fp = open(path.join(lotto_base_dir, \"lotto_data.txt\"), 'w')\n",
        "\n",
        "# 기존 데이타에 추가하기\n",
        "fp = open(path.join(lotto_base_dir, \"lotto_data.txt\"), 'a')\n",
        "Crawler(last-2, last, fp) # 처음부터 마지막 회차까지 저장\n",
        "fp.close()\n",
        "\n",
        "#  --------------\n",
        "#  번호 저장 끝\n",
        "#  -------------\n",
        "\n",
        "# 당첨번호를 원핫인코딩벡터(ohbin)으로 변환\n",
        "def numbers2ohbin(numbers):\n",
        "\n",
        "    ohbin = np.zeros(45) #45개의 빈 칸을 만듬\n",
        "\n",
        "    for i in range(6): #여섯개의 당첨번호에 대해서 반복함\n",
        "        ohbin[int(numbers[i])-1] = 1 #로또번호가 1부터 시작하지만 벡터의 인덱스 시작은 0부터 시작하므로 1을 뺌\n",
        "    \n",
        "    return ohbin\n",
        "\n",
        "# 원핫인코딩벡터(ohbin)를 번호로 변환\n",
        "def ohbin2numbers(ohbin):\n",
        "\n",
        "    numbers = []\n",
        "    \n",
        "    for i in range(len(ohbin)):\n",
        "        if ohbin[i] == 1.0: # 1.0으로 설정되어 있으면 해당 번호를 반환값에 추가한다.\n",
        "            numbers.append(i+1)\n",
        "    \n",
        "    return numbers\n",
        "\n",
        "\n",
        "rows = np.loadtxt(\"./gdrive/MyDrive/lotto_data.txt\", delimiter=\",\")\n",
        "row_count = len(rows)\n",
        "\n",
        "numbers = rows[:, 1:7]\n",
        "ohbins = list(map(numbers2ohbin, numbers))\n",
        "\n",
        "x_samples = ohbins[0:row_count-1]\n",
        "y_samples = ohbins[1:row_count]\n",
        "\n",
        "#model = tf.keras.models.load_model('./gdrive/MyDrive/my_model.h5', compile=False)\n",
        "#model = tf.keras.models.load_model('./gdrive/MyDrive/my_model.h5')\n",
        "#model.summary()\n",
        "# 모델을 정의합니다.\n",
        "model = keras.Sequential([\n",
        "    keras.layers.LSTM(128, batch_input_shape=(1, 1, 45), return_sequences=False, stateful=True),\n",
        "    keras.layers.Dense(45, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 모델을 컴파일합니다.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "\n",
        "\n",
        "# 88회부터 지금까지 1등부터 5등까지 상금의 평균낸다.\n",
        "mean_prize = [  np.mean(rows[87:, 8]),\n",
        "            np.mean(rows[87:, 9]),\n",
        "            np.mean(rows[87:, 10]),\n",
        "            np.mean(rows[87:, 11]),\n",
        "            np.mean(rows[87:, 12])]\n",
        "\n",
        "train_total_reward = []\n",
        "train_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "val_total_reward = []\n",
        "val_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "test_total_reward = []\n",
        "test_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "\n",
        "\n",
        "def gen_numbers_from_probability(nums_prob):\n",
        "\n",
        "    ball_box = []\n",
        "\n",
        "    for n in range(45):\n",
        "        ball_count = int(nums_prob[n] * 200 + 1)\n",
        "        ball = np.full((ball_count), n+1) #1부터 시작\n",
        "        ball_box += list(ball)\n",
        "\n",
        "    selected_balls = []\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        if len(selected_balls) == 6:\n",
        "            break\n",
        "        \n",
        "        ball_index = np.random.randint(len(ball_box), size=1)[0]\n",
        "        ball = ball_box[ball_index]\n",
        "\n",
        "        if ball not in selected_balls:\n",
        "            selected_balls.append(ball)\n",
        "\n",
        "   \n",
        "    selected_balls.sort()\n",
        "\n",
        "    return selected_balls\n",
        "\n",
        "print('receive numbers')\n",
        "\n",
        "xs = x_samples[-1].reshape(1, 1, 45)\n",
        "\n",
        "ys_pred = model.predict_on_batch(xs)\n",
        "\n",
        "list_numbers = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 최대 100번 에포크까지 수행\n",
        "for epoch in range(50):\n",
        "\n",
        "    model.reset_states() # 중요! 매 에포크마다 1회부터 다시 훈련하므로 상태 초기화 필요\n",
        "\n",
        "    batch_train_loss = []\n",
        "    batch_train_acc = []\n",
        "\n",
        "    for i in range(len(x_samples)):\n",
        "        \n",
        "        xs = x_samples[i].reshape(1, 1, 45)\n",
        "        ys = y_samples[i].reshape(1, 45)\n",
        "        \n",
        "        loss, acc = model.train_on_batch(xs, ys) #배치만큼 모델에 학습시킴\n",
        "\n",
        "        batch_train_loss.append(loss)\n",
        "        batch_train_acc.append(acc)\n",
        "\n",
        "    train_loss.append(np.mean(batch_train_loss))\n",
        "    train_acc.append(np.mean(batch_train_acc))\n",
        "\n",
        "    print('epoch {0:4d} train acc {1:0.3f} loss {2:0.3f}'.format(epoch, np.mean(batch_train_acc), np.mean(batch_train_loss)))  \n",
        "\n",
        "\n",
        "# Trainning 끝나면 무조건 모델에 저장\n",
        "model.save('my_model.h5')\n",
        "!cp /content/my_model.h5 /content/drive/My Drive/\n",
        "\n",
        "model = tf.keras.models.load_model('./gdrive/MyDrive/my_model.h5', compile=False)\n",
        "model.summary()\n",
        "\n",
        "# 마지막 회차까지 학습한 모델로 다음 회차 추론\n",
        "\n",
        "# 50 개 뽑기\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "with open(path.join(lotto_base_dir, \"predict2.txt\"), \"w\") as f:\n",
        "  print (\"predict.txt\")\n",
        "\n",
        "  for n in range(50):\n",
        "    numbers = gen_numbers_from_probability(ys_pred[0])  \n",
        "    print('{0} : {1}'.format(n, numbers))    \n",
        "    list_numbers.append(numbers)  \n",
        "    line_str=','.join(str(e) for e in numbers)\n",
        "    line_str += '\\n'\n",
        "    print(line_str)\n",
        "    f.write(line_str)\n",
        "f.close()\n",
        "\n",
        "from ftplib import FTP\n",
        "\n",
        "ftp = FTP('112.175.184.78')\n",
        "ftp.login('dalasjoe', 'Dalasjoe75!')\n",
        "\n",
        "# ftp.cwd('html') # \"test\"디렉터리로 이동\n",
        "# ftp.retrlines('LIST') # 디렉터리의 내용을 목록화\n",
        "# #ftp.retrbinary('RETR README', open('README', 'wb').write) # README 파일 저장\n",
        "# ftp.quit()\n",
        "\n",
        "\n",
        "ftp.cwd('html')  # 업로드할 FTP 폴더로 이동\n",
        "myfile = open(path.join(lotto_base_dir, \"predict2.txt\"),'rb')  # 로컬 파일 열기\n",
        "ftp.storbinary('STOR ' + 'predict2.txt', myfile )  # 파일을 FTP로 업로드\n",
        "myfile.close()  # 파일 닫기\n",
        "\n",
        "print (\"File Saved\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pip in /root/.local/lib/python3.7/site-packages (21.3.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Requirement already satisfied: tensorflow-gpu==2.0.1 in /usr/local/lib/python3.7/dist-packages (2.0.1)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (2.0.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (3.3.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.0.8)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.1.2)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (3.17.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.21.5)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.43.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.12.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.2.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.13.3)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.16.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.2.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.37.1)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.8.1)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (2.0.1)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.1) (2.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (1.35.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (1.0.1)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.3.6)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (0.4.6)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (4.10.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.0.4)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.7.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.1.1)\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
            "Collecting h5py==2.10.0\n",
            "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "Collecting six\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Collecting numpy>=1.7\n",
            "  Using cached numpy-1.21.5-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Installing collected packages: six, numpy, h5py\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.5\n",
            "    Uninstalling numpy-1.21.5:\n",
            "      Successfully uninstalled numpy-1.21.5\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.5 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 2.0.2 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 2.0.1 which is incompatible.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-2.10.0 numpy-1.21.5 six-1.16.0\n",
            "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "Last= 998\n",
            "996,6,11,15,24,32,39,28,1491185771,52018109,1350305,50000,5000\n",
            "997,4,7,14,16,24,44,20,1253749560,46708317,1075354,50000,5000\n",
            "998,13,17,18,20,42,45,41,2076499657,78358478,1568354,50000,5000\n",
            "receive numbers\n",
            "epoch    0 train acc 0.023 loss 0.405\n",
            "epoch    1 train acc 0.018 loss 0.396\n",
            "epoch    2 train acc 0.019 loss 0.393\n",
            "epoch    3 train acc 0.021 loss 0.390\n",
            "epoch    4 train acc 0.018 loss 0.387\n",
            "epoch    5 train acc 0.020 loss 0.383\n",
            "epoch    6 train acc 0.031 loss 0.378\n",
            "epoch    7 train acc 0.034 loss 0.372\n",
            "epoch    8 train acc 0.036 loss 0.367\n",
            "epoch    9 train acc 0.038 loss 0.360\n",
            "epoch   10 train acc 0.045 loss 0.354\n",
            "epoch   11 train acc 0.055 loss 0.346\n",
            "epoch   12 train acc 0.066 loss 0.339\n",
            "epoch   13 train acc 0.068 loss 0.330\n",
            "epoch   14 train acc 0.070 loss 0.321\n",
            "epoch   15 train acc 0.080 loss 0.311\n",
            "epoch   16 train acc 0.092 loss 0.301\n",
            "epoch   17 train acc 0.105 loss 0.291\n",
            "epoch   18 train acc 0.109 loss 0.281\n",
            "epoch   19 train acc 0.112 loss 0.270\n",
            "epoch   20 train acc 0.124 loss 0.260\n",
            "epoch   21 train acc 0.119 loss 0.251\n",
            "epoch   22 train acc 0.127 loss 0.241\n",
            "epoch   23 train acc 0.117 loss 0.231\n",
            "epoch   24 train acc 0.108 loss 0.222\n",
            "epoch   25 train acc 0.118 loss 0.213\n",
            "epoch   26 train acc 0.125 loss 0.207\n",
            "epoch   27 train acc 0.124 loss 0.198\n",
            "epoch   28 train acc 0.123 loss 0.191\n",
            "epoch   29 train acc 0.144 loss 0.184\n",
            "epoch   30 train acc 0.145 loss 0.176\n",
            "epoch   31 train acc 0.146 loss 0.172\n",
            "epoch   32 train acc 0.146 loss 0.162\n",
            "epoch   33 train acc 0.146 loss 0.157\n",
            "epoch   34 train acc 0.141 loss 0.148\n",
            "epoch   35 train acc 0.151 loss 0.148\n",
            "epoch   36 train acc 0.168 loss 0.141\n",
            "epoch   37 train acc 0.143 loss 0.133\n",
            "epoch   38 train acc 0.164 loss 0.126\n",
            "epoch   39 train acc 0.157 loss 0.126\n",
            "epoch   40 train acc 0.155 loss 0.119\n",
            "epoch   41 train acc 0.174 loss 0.110\n",
            "epoch   42 train acc 0.169 loss 0.110\n",
            "epoch   43 train acc 0.162 loss 0.106\n",
            "epoch   44 train acc 0.184 loss 0.098\n",
            "epoch   45 train acc 0.160 loss 0.097\n",
            "epoch   46 train acc 0.184 loss 0.093\n",
            "epoch   47 train acc 0.154 loss 0.090\n",
            "epoch   48 train acc 0.170 loss 0.091\n",
            "epoch   49 train acc 0.150 loss 0.086\n",
            "cp: target 'Drive/' is not a directory\n",
            "Mounted at /content/gdrive\n",
            "predict.txt\n",
            "0 : [2, 4, 14, 18, 19, 24]\n",
            "2,4,14,18,19,24\n",
            "\n",
            "1 : [4, 16, 21, 30, 37, 42]\n",
            "4,16,21,30,37,42\n",
            "\n",
            "2 : [1, 6, 10, 13, 27, 37]\n",
            "1,6,10,13,27,37\n",
            "\n",
            "3 : [7, 18, 24, 37, 38, 39]\n",
            "7,18,24,37,38,39\n",
            "\n",
            "4 : [21, 26, 27, 32, 42, 45]\n",
            "21,26,27,32,42,45\n",
            "\n",
            "5 : [18, 26, 27, 32, 36, 39]\n",
            "18,26,27,32,36,39\n",
            "\n",
            "6 : [14, 24, 27, 28, 31, 38]\n",
            "14,24,27,28,31,38\n",
            "\n",
            "7 : [1, 11, 14, 25, 31, 41]\n",
            "1,11,14,25,31,41\n",
            "\n",
            "8 : [4, 28, 29, 31, 41, 42]\n",
            "4,28,29,31,41,42\n",
            "\n",
            "9 : [1, 4, 9, 20, 39, 41]\n",
            "1,4,9,20,39,41\n",
            "\n",
            "10 : [6, 19, 23, 25, 34, 38]\n",
            "6,19,23,25,34,38\n",
            "\n",
            "11 : [8, 23, 24, 30, 44, 45]\n",
            "8,23,24,30,44,45\n",
            "\n",
            "12 : [13, 23, 26, 30, 37, 45]\n",
            "13,23,26,30,37,45\n",
            "\n",
            "13 : [2, 5, 9, 10, 15, 24]\n",
            "2,5,9,10,15,24\n",
            "\n",
            "14 : [9, 12, 13, 17, 33, 35]\n",
            "9,12,13,17,33,35\n",
            "\n",
            "15 : [10, 17, 26, 27, 33, 40]\n",
            "10,17,26,27,33,40\n",
            "\n",
            "16 : [16, 20, 21, 30, 38, 44]\n",
            "16,20,21,30,38,44\n",
            "\n",
            "17 : [10, 11, 14, 15, 29, 42]\n",
            "10,11,14,15,29,42\n",
            "\n",
            "18 : [18, 22, 25, 30, 38, 44]\n",
            "18,22,25,30,38,44\n",
            "\n",
            "19 : [13, 23, 30, 35, 38, 40]\n",
            "13,23,30,35,38,40\n",
            "\n",
            "20 : [4, 19, 23, 29, 35, 39]\n",
            "4,19,23,29,35,39\n",
            "\n",
            "21 : [4, 14, 15, 25, 30, 45]\n",
            "4,14,15,25,30,45\n",
            "\n",
            "22 : [11, 15, 22, 26, 39, 43]\n",
            "11,15,22,26,39,43\n",
            "\n",
            "23 : [8, 14, 22, 24, 36, 41]\n",
            "8,14,22,24,36,41\n",
            "\n",
            "24 : [4, 6, 16, 25, 30, 37]\n",
            "4,6,16,25,30,37\n",
            "\n",
            "25 : [2, 5, 23, 30, 32, 43]\n",
            "2,5,23,30,32,43\n",
            "\n",
            "26 : [14, 16, 19, 23, 29, 43]\n",
            "14,16,19,23,29,43\n",
            "\n",
            "27 : [6, 9, 24, 33, 34, 37]\n",
            "6,9,24,33,34,37\n",
            "\n",
            "28 : [11, 12, 15, 16, 18, 37]\n",
            "11,12,15,16,18,37\n",
            "\n",
            "29 : [7, 13, 16, 23, 44, 45]\n",
            "7,13,16,23,44,45\n",
            "\n",
            "30 : [2, 9, 11, 14, 15, 19]\n",
            "2,9,11,14,15,19\n",
            "\n",
            "31 : [10, 24, 26, 31, 33, 41]\n",
            "10,24,26,31,33,41\n",
            "\n",
            "32 : [27, 28, 29, 36, 38, 42]\n",
            "27,28,29,36,38,42\n",
            "\n",
            "33 : [4, 6, 13, 24, 33, 44]\n",
            "4,6,13,24,33,44\n",
            "\n",
            "34 : [5, 10, 20, 21, 30, 41]\n",
            "5,10,20,21,30,41\n",
            "\n",
            "35 : [2, 5, 27, 31, 34, 36]\n",
            "2,5,27,31,34,36\n",
            "\n",
            "36 : [8, 11, 27, 29, 31, 34]\n",
            "8,11,27,29,31,34\n",
            "\n",
            "37 : [9, 21, 33, 36, 41, 42]\n",
            "9,21,33,36,41,42\n",
            "\n",
            "38 : [7, 11, 14, 21, 33, 43]\n",
            "7,11,14,21,33,43\n",
            "\n",
            "39 : [15, 21, 27, 37, 38, 42]\n",
            "15,21,27,37,38,42\n",
            "\n",
            "40 : [14, 31, 32, 37, 39, 45]\n",
            "14,31,32,37,39,45\n",
            "\n",
            "41 : [8, 18, 22, 30, 32, 37]\n",
            "8,18,22,30,32,37\n",
            "\n",
            "42 : [8, 14, 15, 29, 33, 43]\n",
            "8,14,15,29,33,43\n",
            "\n",
            "43 : [4, 13, 14, 26, 30, 33]\n",
            "4,13,14,26,30,33\n",
            "\n",
            "44 : [2, 6, 7, 12, 24, 31]\n",
            "2,6,7,12,24,31\n",
            "\n",
            "45 : [4, 6, 13, 15, 20, 36]\n",
            "4,6,13,15,20,36\n",
            "\n",
            "46 : [4, 9, 32, 34, 37, 40]\n",
            "4,9,32,34,37,40\n",
            "\n",
            "47 : [5, 20, 21, 30, 33, 34]\n",
            "5,20,21,30,33,34\n",
            "\n",
            "48 : [6, 7, 14, 17, 18, 27]\n",
            "6,7,14,17,18,27\n",
            "\n",
            "49 : [3, 15, 18, 22, 40, 45]\n",
            "3,15,18,22,40,45\n",
            "\n",
            "File Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZRsvoibHt5d"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOjlv9p90YKR"
      },
      "source": [
        "# **Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzmxpXaV0dCc"
      },
      "source": [
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvzmqDDf17WD"
      },
      "source": [
        "# **Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqdRq1Rn2CKo"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#!pip install tensorflow-gpu==2.0.0-rc1\n",
        "!pip install tensorflow-gpu==2.0.1\n",
        "\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "\n",
        "new_model = tf.keras.models.load_model('./gdrive/MyDrive/my_model.h5')\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LZbvqHB5E3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc18634-71a0-4a2b-aff2-01fbdf9930f3"
      },
      "source": [
        "!cp /content/my_model.h5 /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: failed to access '/content/drive/My Drive/': Transport endpoint is not connected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBxRkkEHVwU1"
      },
      "source": [
        "# FTP 전송"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1v3JlaOWKAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762e78ea-c5c2-4706-ef40-dc8a62b3e09b"
      },
      "source": [
        "from ftplib import FTP\n",
        "\n",
        "ftp = FTP('112.175.184.78')\n",
        "ftp.login('dalasjoe', 'Dalasjoe75!')\n",
        "\n",
        "# ftp.cwd('html') # \"test\"디렉터리로 이동\n",
        "# ftp.retrlines('LIST') # 디렉터리의 내용을 목록화\n",
        "# #ftp.retrbinary('RETR README', open('README', 'wb').write) # README 파일 저장\n",
        "# ftp.quit()\n",
        "\n",
        "\n",
        "ftp.cwd('html')  # 업로드할 FTP 폴더로 이동\n",
        "myfile = open(path.join(lotto_base_dir, \"predict.txt\"),'rb')  # 로컬 파일 열기\n",
        "ftp.storbinary('STOR ' + 'predict.txt', myfile )  # 파일을 FTP로 업로드\n",
        "myfile.close()  # 파일 닫기\n",
        "\n",
        "print (\"File Saved\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVn3rfIyW7OG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSlY9i6yW511"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
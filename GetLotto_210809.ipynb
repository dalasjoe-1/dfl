{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GetLotto.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Q8MptZ7cjH8T"
      ],
      "authorship_tag": "ABX9TyOi2W6/UlrEcxJLa/hNhwMK",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dalasjoe-1/dfl/blob/master/GetLotto_210809.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kCmxwN5Rkt1"
      },
      "source": [
        "# Another Way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl8KqROtQ5oG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a5c07ff-a83e-4e2c-98fb-b98d7d198fc8"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "tf.compat.v1.set_random_seed(777)\n",
        "\n",
        "input_data_column_cnt = 45  # 입력데이터의 컬럼 개수(Variable 개수)\n",
        "#output_data_column_cnt = 1  # 결과데이터의 컬럼 개수\n",
        "output_data_column_cnt = 45  # 결과데이터의 컬럼 개수\n",
        "\n",
        "seq_length = 46 #192 #24 #96  # 1개 시퀀스의 길이(시계열데이터 입력 개수)\n",
        "rnn_cell_hidden_dim = 20  # 각 셀의 (hidden)출력 크기\n",
        "forget_bias = 1.0  # 망각편향(기본값 1.0)\n",
        "num_stacked_layers = 1  # stacked LSTM layers 개수\n",
        "keep_prob = 1.0  # dropout할 때 keep할 비율\n",
        "\n",
        "epoch_num = 18000 #20000  # 에폭 횟수(학습용전체데이터를 몇 회 반복해서 학습할 것인가 입력)\n",
        "learning_rate = 0.01  # 학습률\n",
        "\n",
        "# Standardization\n",
        "def data_standardization(x):\n",
        "    x_np = np.asarray(x)\n",
        "    return (x_np - x_np.mean()) / x_np.std()\n",
        "\n",
        "# 너무 작거나 너무 큰 값이 학습을 방해하는 것을 방지하고자 정규화한다\n",
        "# x가 양수라는 가정하에 최소값과 최대값을 이용하여 0~1사이의 값으로 변환\n",
        "# Min-Max scaling\n",
        "def min_max_scaling(x):\n",
        "    x_np = np.asarray(x)\n",
        "    return (x_np - x_np.min()) / (x_np.max() - x_np.min() + 1e-7)  # 1e-7은 0으로 나누는 오류 예방차원\n",
        "\n",
        "# 정규화된 값을 원래의 값으로 되돌린다\n",
        "# 정규화하기 이전의 org_x값과 되돌리고 싶은 x를 입력하면 역정규화된 값을 리턴한다\n",
        "def reverse_min_max_scaling(org_x, x):\n",
        "    org_x_np = np.asarray(org_x)\n",
        "    x_np = np.asarray(x)\n",
        "    return (x_np * (org_x_np.max() - org_x_np.min() + 1e-7)) + org_x_np.min()\n",
        "\n",
        "# data\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "#input = np.loadtxt('./gdrive/MyDrive/ori.data', unpack=True, dtype='int')\n",
        "input = np.loadtxt(\"./gdrive/MyDrive/lotto_data.txt\", delimiter=\",\", dtype='int')\n",
        "#data = np.transpose(input)\n",
        "data = input\n",
        "#win_numbers = data[:,1:data.size]\n",
        "win_numbers = data[:,1:7]\n",
        "\n",
        "#print(win_numbers, \" : \", win_numbers.size)\n",
        "norm_win_numbers = min_max_scaling(win_numbers)  # 가격형태 데이터 정규화 처리\n",
        "\n",
        "#x = min_max_scaling(win_numbers);\n",
        "x = win_numbers\n",
        "y = x[:, :]\n",
        "z = []\n",
        "\n",
        "for i in range(len(data)):\n",
        "    week = win_numbers[i]  \n",
        "    buckets = [0, 0, 0, 0, 0, 0, 0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0]\n",
        "    for j in range(len(week)):\n",
        "        k = week[j]\n",
        "        buckets[k-1] = 100\n",
        "    z.append(buckets)\n",
        "\n",
        "x = z;\n",
        "y = x;\n",
        "\n",
        "dataX = []  # 입력으로 사용될 Sequence Data\n",
        "dataY = []  # 출력(타켓)으로 사용\n",
        "\n",
        "for i in range(0, len(y) - seq_length):\n",
        "    _x = x[i: i + seq_length]\n",
        "    _y = y[i + seq_length]  # 다음 나타날 주가(정답)\n",
        "    # if i is 0:\n",
        "    #      print(\">>> \", _x, \"->\", _y)  # 첫번째 행만 출력해 봄\n",
        "\n",
        "    dataX.append(_x)  # dataX 리스트에 추가\n",
        "    dataY.append(_y)  # dataY 리스트에 추가\n",
        "\n",
        "# 학습용/테스트용 데이터 생성\n",
        "# 전체 70%를 학습용 데이터로 사용\n",
        "train_size = int(len(dataY) * 0.7)\n",
        "# 나머지(30%)를 테스트용 데이터로 사용\n",
        "test_size = len(dataY) - train_size\n",
        "\n",
        "\n",
        "# 데이터를 잘라 학습용 데이터 생성\n",
        "trainX = np.asarray(dataX[0:train_size])\n",
        "trainY = np.asarray(dataY[0:train_size])\n",
        "\n",
        "# for i in range(len(trainX)):\n",
        "#     aaa = trainX[i]\n",
        "#     if (i==0):\n",
        "\n",
        "print(trainX.shape, \" ==== \", trainY.shape)\n",
        "# print(trainX[0], \" **** \", dataX[0])\n",
        "\n",
        "\n",
        "# 데이터를 잘라 테스트용 데이터 생성\n",
        "testX = np.array(dataX[train_size:len(dataX)])\n",
        "testY = np.array(dataY[train_size:len(dataY)])\n",
        "\n",
        "# 텐서플로우 플레이스홀더 생성\n",
        "# 입력 X, 출력 Y를 생성한다\n",
        "X = tf.compat.v1.placeholder(tf.float32, [None, seq_length, input_data_column_cnt])\n",
        "print(\"X: \", X)\n",
        "Y = tf.compat.v1.placeholder(tf.float32, [None, output_data_column_cnt])\n",
        "print(\"Y: \", Y)\n",
        "\n",
        "# 검증용 측정지표를 산출하기 위한 targets, predictions를 생성한다\n",
        "targets = tf.compat.v1.placeholder(tf.float32, [None, output_data_column_cnt])\n",
        "print(\"targets: \", targets)\n",
        "\n",
        "predictions = tf.compat.v1.placeholder(tf.float32, [None, output_data_column_cnt])\n",
        "print(\"predictions: \", predictions)\n",
        "\n",
        "def lstm_cell():\n",
        "    # LSTM셀을 생성\n",
        "    # num_units: 각 Cell 출력 크기\n",
        "    # forget_bias:  to the biases of the forget gate\n",
        "    #              (default: 1)  in order to reduce the scale of forgetting in the beginning of the training.\n",
        "    # state_is_tuple: True ==> accepted and returned states are 2-tuples of the c_state and m_state.\n",
        "    # state_is_tuple: False ==> they are concatenated along the column axis.\n",
        "\n",
        "    cell = tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=rnn_cell_hidden_dim,\n",
        "                                        forget_bias=forget_bias, state_is_tuple=True, activation=tf.nn.softsign)\n",
        "   \n",
        "    # cell = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_cell_hidden_dim,\n",
        "    #                                     forget_bias=forget_bias, state_is_tuple=True, activation=tf.nn.softsign)\n",
        "    #cell = tf.keras.layers.LSTMCell(num_units=rnn_cell_hidden_dim, forget_bias=forget_bias\n",
        "    #                                 , state_is_tuple=True, activation=tf.nn.softsign)\n",
        "    if keep_prob < 1.0:\n",
        "        #cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
        "        cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
        "    return cell\n",
        "\n",
        "\n",
        "# num_stacked_layers개의 층으로 쌓인 Stacked RNNs 생성\n",
        "stackedRNNs = [lstm_cell() for _ in range(num_stacked_layers)]\n",
        "multi_cells = tf.contrib.rnn.MultiRNNCell(stackedRNNs, state_is_tuple=True) if num_stacked_layers > 1 else lstm_cell()\n",
        "\n",
        "hypothesis, _states = tf.compat.v1.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32)\n",
        "#hypothesis, _states = tf.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32)\n",
        "print(\"hypothesis: \", hypothesis)\n",
        "\n",
        "# [:, -1]를 잘 살펴보자. LSTM RNN의 마지막 (hidden)출력만을 사용했다.\n",
        "# 과거 여러 거래일의 주가를 이용해서 다음날의 주가 1개를 예측하기때문에 MANY-TO-ONE형태이다\n",
        "#hypothesis = tf.contrib.layers.fully_connected(hypothesis[:, -1], output_data_column_cnt, activation_fn=tf.identity)\n",
        "hypothesis = tf.compat.v1.layers.dense(hypothesis[:, -1], output_data_column_cnt, activation = tf.identity)\n",
        "\n",
        "\n",
        "\n",
        "# 손실함수로 평균제곱오차를 사용한다\n",
        "loss = tf.reduce_sum(tf.square(hypothesis - Y))\n",
        "# 최적화함수로 AdamOptimizer를 사용한다\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "# optimizer = tf.train.RMSPropOptimizer(learning_rate) # LSTM과 궁합 별로임\n",
        "\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "# RMSE(Root Mean Square Error)\n",
        "# 제곱오차의 평균을 구하고 다시 제곱근을 구하면 평균 오차가 나온다\n",
        "# rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions))) # 아래 코드와 같다\n",
        "rmse = tf.sqrt(tf.reduce_mean(tf.math.squared_difference(targets, predictions)))\n",
        "\n",
        "train_error_summary = []  # 학습용 데이터의 오류를 중간 중간 기록한다\n",
        "test_error_summary = []  # 테스트용 데이터의 오류를 중간 중간 기록한다\n",
        "test_predict = ''  # 테스트용데이터로 예측한 결과\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "print(\"trainX.shape=\", trainX.shape, \"trainY.shape=\", trainY.shape)\n",
        "print(\"trainX=\", trainX)\n",
        "\n",
        "# 학습한다\n",
        "start_time = datetime.datetime.now()  # 시작시간을 기록한다\n",
        "print('학습을 시작합니다...')\n",
        "for epoch in range(epoch_num):\n",
        "    _, _loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
        "    if ((epoch + 1) % 100 == 0) or (epoch == epoch_num - 1):  # 100번째마다 또는 마지막 epoch인 경우\n",
        "        # 학습용데이터로 rmse오차를 구한다\n",
        "        train_predict = sess.run(hypothesis, feed_dict={X: trainX})\n",
        "        train_error = sess.run(rmse, feed_dict={targets: trainY, predictions: train_predict})\n",
        "        train_error_summary.append(train_error)\n",
        "\n",
        "        # 테스트용데이터로 rmse오차를 구한다\n",
        "        test_predict = sess.run(hypothesis, feed_dict={X: testX})\n",
        "        test_error = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
        "        test_error_summary.append(test_error)\n",
        "\n",
        "        # 현재 오류를 출력한다\n",
        "        print(\"epoch: {}, train_error(A): {}, test_error(B): {}, B-A: {}\".format(epoch + 1, train_error, test_error,\n",
        "                                                                                 test_error - train_error))\n",
        "\n",
        "# end_time = datetime.datetime.now()  # 종료시간을 기록한다\n",
        "# elapsed_time = end_time - start_time  # 경과시간을 구한다\n",
        "# print('elapsed_time:', elapsed_time)\n",
        "# print('elapsed_time per epoch:', elapsed_time / epoch_num)\n",
        "#\n",
        "# # 하이퍼파라미터 출력\n",
        "# print('input_data_column_cnt:', input_data_column_cnt, end='')\n",
        "# print(',output_data_column_cnt:', output_data_column_cnt, end='')\n",
        "#\n",
        "# print(',seq_length:', seq_length, end='')\n",
        "# print(',rnn_cell_hidden_dim:', rnn_cell_hidden_dim, end='')\n",
        "# print(',forget_bias:', forget_bias, end='')\n",
        "# print(',num_stacked_layers:', num_stacked_layers, end='')\n",
        "# print(',keep_prob:', keep_prob, end='')\n",
        "#\n",
        "# print(',epoch_num:', epoch_num, end='')\n",
        "# print(',learning_rate:', learning_rate, end='')\n",
        "#\n",
        "# print(',train_error:', train_error_summary[-1], end='')\n",
        "# print(',test_error:', test_error_summary[-1], end='')\n",
        "# print(',min_test_error:', np.min(test_error_summary))\n",
        "#\n",
        "# # 결과 그래프 출력\n",
        "# plt.figure(1)\n",
        "# plt.plot(train_error_summary, 'gold')\n",
        "# plt.plot(test_error_summary, 'b')\n",
        "# plt.xlabel('Epoch(x100)')\n",
        "# plt.ylabel('Root Mean Square Error')\n",
        "#\n",
        "# plt.figure(2)\n",
        "# plt.plot(testY, 'r')\n",
        "# plt.plot(test_predict, 'b')\n",
        "# plt.xlabel('Time Period')\n",
        "# plt.ylabel('Stock Price')\n",
        "# plt.show()\n",
        "\n",
        "# sequence length만큼의 가장 최근 데이터를 슬라이싱한다\n",
        "recent_data = np.array([x[len(x) - seq_length:]])\n",
        "print(\"recent_data.shape:\", recent_data.shape)\n",
        "print(\"recent_data:\", recent_data)\n",
        "\n",
        "# 내일 종가를 예측해본다\n",
        "test_predict = sess.run(hypothesis, feed_dict={X: recent_data})\n",
        "\n",
        "test_predict_2 = []\n",
        "\n",
        "for i in range(len(test_predict[0])):\n",
        "    test_predict[0][i] = int(test_predict[0][i])\n",
        "    test_predict_2.append((test_predict[0][i], i))\n",
        "\n",
        "#final_predict = np.sort(test_predict_2, axis=1)[::-1]\n",
        "print(\"test_predict_2  00000 =\", test_predict_2)\n",
        "test_predict_2.sort(key = lambda element : element[0], reverse=True)\n",
        "\n",
        "\n",
        "# predict_six = []\n",
        "#\n",
        "# for i in final_predict:\n",
        "#     for j in test_predict[0]:\n",
        "#         if (final_predict[i] == test_predict[0][j])\n",
        "\n",
        "\n",
        "#print(\"test_predict 0 \", test_predict[0])\n",
        "print(\"test_predict_2  11111\", test_predict_2)\n",
        "\n",
        "picked = []\n",
        "for i in range(6):\n",
        "    picked.append(test_predict_2[i][1] + 1)\n",
        "\n",
        "print(picked)"
      ],
      "execution_count": 70,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/gdrive\n",
            "(650, 46, 45)  ====  (650, 45)\n",
            "X:  Tensor(\"Placeholder:0\", shape=(None, 46, 45), dtype=float32)\n",
            "Y:  Tensor(\"Placeholder_1:0\", shape=(None, 45), dtype=float32)\n",
            "targets:  Tensor(\"Placeholder_2:0\", shape=(None, 45), dtype=float32)\n",
            "predictions:  Tensor(\"Placeholder_3:0\", shape=(None, 45), dtype=float32)\n",
            "hypothesis:  Tensor(\"rnn/transpose_1:0\", shape=(None, 46, 20), dtype=float32)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/layers/legacy_rnn/rnn_cell_impl.py:909: UserWarning: `tf.nn.rnn_cell.LSTMCell` is deprecated and will be removed in a future version. This class is equivalent as `tf.keras.layers.LSTMCell`, and will be replaced by that in Tensorflow 2.0.\n",
            "  warnings.warn(\"`tf.nn.rnn_cell.LSTMCell` is deprecated and will be \"\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1700: UserWarning: `layer.add_variable` is deprecated and will be removed in a future version. Please use `layer.add_weight` method instead.\n",
            "  warnings.warn('`layer.add_variable` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/legacy_tf_layers/core.py:171: UserWarning: `tf.layers.dense` is deprecated and will be removed in a future version. Please use `tf.keras.layers.Dense` instead.\n",
            "  warnings.warn('`tf.layers.dense` is deprecated and '\n",
            "/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/base_layer_v1.py:1692: UserWarning: `layer.apply` is deprecated and will be removed in a future version. Please use `layer.__call__` method instead.\n",
            "  warnings.warn('`layer.apply` is deprecated and '\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "trainX.shape= (650, 46, 45) trainY.shape= (650, 45)\n",
            "trainX= [[[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [  0   0 100 ...   0   0 100]\n",
            "  [100   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [100   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0 100]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0 100]\n",
            "  [  0   0   0 ...   0   0   0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[  0   0   0 ... 100   0   0]\n",
            "  [  0   0 100 ...   0   0   0]\n",
            "  [  0   0 100 ...   0   0   0]\n",
            "  ...\n",
            "  [  0   0   0 ... 100   0 100]\n",
            "  [  0   0 100 ...   0   0   0]\n",
            "  [100   0   0 ...   0   0   0]]\n",
            "\n",
            " [[  0   0 100 ...   0   0   0]\n",
            "  [  0   0 100 ...   0   0   0]\n",
            "  [  0   0   0 ...   0 100   0]\n",
            "  ...\n",
            "  [  0   0 100 ...   0   0   0]\n",
            "  [100   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ... 100   0   0]]\n",
            "\n",
            " [[  0   0 100 ...   0   0   0]\n",
            "  [  0   0   0 ...   0 100   0]\n",
            "  [  0   0 100 ...   0 100   0]\n",
            "  ...\n",
            "  [100   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ... 100   0   0]\n",
            "  [  0   0   0 ...   0   0   0]]]\n",
            "학습을 시작합니다...\n",
            "epoch: 100, train_error(A): 33.93865203857422, test_error(B): 34.0374641418457, B-A: 0.09881210327148438\n",
            "epoch: 200, train_error(A): 33.901466369628906, test_error(B): 34.0519905090332, B-A: 0.15052413940429688\n",
            "epoch: 300, train_error(A): 33.86268615722656, test_error(B): 34.07196044921875, B-A: 0.2092742919921875\n",
            "epoch: 400, train_error(A): 33.83454132080078, test_error(B): 34.08757400512695, B-A: 0.2530326843261719\n",
            "epoch: 500, train_error(A): 33.812599182128906, test_error(B): 34.10764694213867, B-A: 0.2950477600097656\n",
            "epoch: 600, train_error(A): 33.797401428222656, test_error(B): 34.1435546875, B-A: 0.34615325927734375\n",
            "epoch: 700, train_error(A): 33.77761459350586, test_error(B): 34.140663146972656, B-A: 0.3630485534667969\n",
            "epoch: 800, train_error(A): 33.76446533203125, test_error(B): 34.14935302734375, B-A: 0.3848876953125\n",
            "epoch: 900, train_error(A): 33.75532531738281, test_error(B): 34.19535446166992, B-A: 0.4400291442871094\n",
            "epoch: 1000, train_error(A): 33.7435302734375, test_error(B): 34.19120788574219, B-A: 0.4476776123046875\n",
            "epoch: 1100, train_error(A): 33.735103607177734, test_error(B): 34.205902099609375, B-A: 0.4707984924316406\n",
            "epoch: 1200, train_error(A): 33.72830581665039, test_error(B): 34.21439743041992, B-A: 0.48609161376953125\n",
            "epoch: 1300, train_error(A): 33.72239685058594, test_error(B): 34.23149871826172, B-A: 0.5091018676757812\n",
            "epoch: 1400, train_error(A): 33.71916198730469, test_error(B): 34.24327850341797, B-A: 0.5241165161132812\n",
            "epoch: 1500, train_error(A): 33.71570587158203, test_error(B): 34.257293701171875, B-A: 0.5415878295898438\n",
            "epoch: 1600, train_error(A): 33.71458053588867, test_error(B): 34.2598762512207, B-A: 0.5452957153320312\n",
            "epoch: 1700, train_error(A): 33.70965576171875, test_error(B): 34.27589416503906, B-A: 0.5662384033203125\n",
            "epoch: 1800, train_error(A): 33.705528259277344, test_error(B): 34.29388427734375, B-A: 0.5883560180664062\n",
            "epoch: 1900, train_error(A): 33.70328903198242, test_error(B): 34.316192626953125, B-A: 0.6129035949707031\n",
            "epoch: 2000, train_error(A): 33.702484130859375, test_error(B): 34.29849624633789, B-A: 0.5960121154785156\n",
            "epoch: 2100, train_error(A): 33.69936752319336, test_error(B): 34.333988189697266, B-A: 0.6346206665039062\n",
            "epoch: 2200, train_error(A): 33.6965217590332, test_error(B): 34.33761215209961, B-A: 0.6410903930664062\n",
            "epoch: 2300, train_error(A): 33.692543029785156, test_error(B): 34.3531494140625, B-A: 0.6606063842773438\n",
            "epoch: 2400, train_error(A): 33.690101623535156, test_error(B): 34.358497619628906, B-A: 0.66839599609375\n",
            "epoch: 2500, train_error(A): 33.68935012817383, test_error(B): 34.3369255065918, B-A: 0.6475753784179688\n",
            "epoch: 2600, train_error(A): 33.6881103515625, test_error(B): 34.35456466674805, B-A: 0.6664543151855469\n",
            "epoch: 2700, train_error(A): 33.7026481628418, test_error(B): 34.34122085571289, B-A: 0.6385726928710938\n",
            "epoch: 2800, train_error(A): 33.68604278564453, test_error(B): 34.35957336425781, B-A: 0.6735305786132812\n",
            "epoch: 2900, train_error(A): 33.68557357788086, test_error(B): 34.373600006103516, B-A: 0.6880264282226562\n",
            "epoch: 3000, train_error(A): 33.685672760009766, test_error(B): 34.39007568359375, B-A: 0.7044029235839844\n",
            "epoch: 3100, train_error(A): 33.685935974121094, test_error(B): 34.37800979614258, B-A: 0.6920738220214844\n",
            "epoch: 3200, train_error(A): 33.68617248535156, test_error(B): 34.389984130859375, B-A: 0.7038116455078125\n",
            "epoch: 3300, train_error(A): 33.68101501464844, test_error(B): 34.36709213256836, B-A: 0.6860771179199219\n",
            "epoch: 3400, train_error(A): 33.671897888183594, test_error(B): 34.37984085083008, B-A: 0.7079429626464844\n",
            "epoch: 3500, train_error(A): 33.631126403808594, test_error(B): 34.413883209228516, B-A: 0.7827568054199219\n",
            "epoch: 3600, train_error(A): 33.55250549316406, test_error(B): 34.446537017822266, B-A: 0.8940315246582031\n",
            "epoch: 3700, train_error(A): 33.51256561279297, test_error(B): 34.55425262451172, B-A: 1.04168701171875\n",
            "epoch: 3800, train_error(A): 33.464603424072266, test_error(B): 34.57352066040039, B-A: 1.108917236328125\n",
            "epoch: 3900, train_error(A): 33.320518493652344, test_error(B): 34.6646728515625, B-A: 1.3441543579101562\n",
            "epoch: 4000, train_error(A): 33.15428161621094, test_error(B): 34.746665954589844, B-A: 1.5923843383789062\n",
            "epoch: 4100, train_error(A): 33.075748443603516, test_error(B): 34.9043083190918, B-A: 1.8285598754882812\n",
            "epoch: 4200, train_error(A): 33.00504684448242, test_error(B): 34.95452117919922, B-A: 1.9494743347167969\n",
            "epoch: 4300, train_error(A): 32.94398498535156, test_error(B): 35.074615478515625, B-A: 2.1306304931640625\n",
            "epoch: 4400, train_error(A): 32.899898529052734, test_error(B): 35.14778518676758, B-A: 2.2478866577148438\n",
            "epoch: 4500, train_error(A): 32.8378791809082, test_error(B): 35.34370422363281, B-A: 2.5058250427246094\n",
            "epoch: 4600, train_error(A): 32.78939437866211, test_error(B): 35.50767517089844, B-A: 2.718280792236328\n",
            "epoch: 4700, train_error(A): 32.76796340942383, test_error(B): 35.54206466674805, B-A: 2.7741012573242188\n",
            "epoch: 4800, train_error(A): 32.76824951171875, test_error(B): 35.607845306396484, B-A: 2.8395957946777344\n",
            "epoch: 4900, train_error(A): 32.68841552734375, test_error(B): 35.76042556762695, B-A: 3.072010040283203\n",
            "epoch: 5000, train_error(A): 32.70123291015625, test_error(B): 36.03007888793945, B-A: 3.328845977783203\n",
            "epoch: 5100, train_error(A): 32.63720703125, test_error(B): 35.97894287109375, B-A: 3.34173583984375\n",
            "epoch: 5200, train_error(A): 32.590579986572266, test_error(B): 36.22184371948242, B-A: 3.6312637329101562\n",
            "epoch: 5300, train_error(A): 32.54522705078125, test_error(B): 36.384525299072266, B-A: 3.8392982482910156\n",
            "epoch: 5400, train_error(A): 32.566871643066406, test_error(B): 36.34376525878906, B-A: 3.7768936157226562\n",
            "epoch: 5500, train_error(A): 32.55851745605469, test_error(B): 36.553340911865234, B-A: 3.994823455810547\n",
            "epoch: 5600, train_error(A): 32.550537109375, test_error(B): 36.568397521972656, B-A: 4.017860412597656\n",
            "epoch: 5700, train_error(A): 32.468326568603516, test_error(B): 36.708595275878906, B-A: 4.240268707275391\n",
            "epoch: 5800, train_error(A): 32.68423080444336, test_error(B): 36.76011276245117, B-A: 4.0758819580078125\n",
            "epoch: 5900, train_error(A): 32.522037506103516, test_error(B): 36.82331466674805, B-A: 4.301277160644531\n",
            "epoch: 6000, train_error(A): 32.48456954956055, test_error(B): 36.88003921508789, B-A: 4.395469665527344\n",
            "epoch: 6100, train_error(A): 32.45269775390625, test_error(B): 36.94429397583008, B-A: 4.491596221923828\n",
            "epoch: 6200, train_error(A): 32.42964553833008, test_error(B): 36.848575592041016, B-A: 4.4189300537109375\n",
            "epoch: 6300, train_error(A): 32.40227508544922, test_error(B): 36.959068298339844, B-A: 4.556793212890625\n",
            "epoch: 6400, train_error(A): 32.39238739013672, test_error(B): 37.21284866333008, B-A: 4.820461273193359\n",
            "epoch: 6500, train_error(A): 32.360843658447266, test_error(B): 37.228816986083984, B-A: 4.867973327636719\n",
            "epoch: 6600, train_error(A): 32.345787048339844, test_error(B): 37.33601379394531, B-A: 4.990226745605469\n",
            "epoch: 6700, train_error(A): 32.341583251953125, test_error(B): 37.3282470703125, B-A: 4.986663818359375\n",
            "epoch: 6800, train_error(A): 32.30752944946289, test_error(B): 37.27739715576172, B-A: 4.969867706298828\n",
            "epoch: 6900, train_error(A): 32.37399673461914, test_error(B): 37.318939208984375, B-A: 4.944942474365234\n",
            "epoch: 7000, train_error(A): 32.37036895751953, test_error(B): 37.246639251708984, B-A: 4.876270294189453\n",
            "epoch: 7100, train_error(A): 32.33515548706055, test_error(B): 37.26368713378906, B-A: 4.928531646728516\n",
            "epoch: 7200, train_error(A): 32.253814697265625, test_error(B): 37.36808776855469, B-A: 5.1142730712890625\n",
            "epoch: 7300, train_error(A): 32.28886413574219, test_error(B): 37.51441955566406, B-A: 5.225555419921875\n",
            "epoch: 7400, train_error(A): 32.338497161865234, test_error(B): 37.408348083496094, B-A: 5.069850921630859\n",
            "epoch: 7500, train_error(A): 32.26749801635742, test_error(B): 37.42344284057617, B-A: 5.15594482421875\n",
            "epoch: 7600, train_error(A): 32.53172302246094, test_error(B): 37.70402145385742, B-A: 5.172298431396484\n",
            "epoch: 7700, train_error(A): 32.360355377197266, test_error(B): 37.49188232421875, B-A: 5.131526947021484\n",
            "epoch: 7800, train_error(A): 32.37104034423828, test_error(B): 37.273441314697266, B-A: 4.902400970458984\n",
            "epoch: 7900, train_error(A): 32.305450439453125, test_error(B): 37.428462982177734, B-A: 5.123012542724609\n",
            "epoch: 8000, train_error(A): 32.312435150146484, test_error(B): 37.47819137573242, B-A: 5.1657562255859375\n",
            "epoch: 8100, train_error(A): 32.25104904174805, test_error(B): 37.60976028442383, B-A: 5.358711242675781\n",
            "epoch: 8200, train_error(A): 32.349647521972656, test_error(B): 37.50276184082031, B-A: 5.153114318847656\n",
            "epoch: 8300, train_error(A): 32.24510192871094, test_error(B): 37.443939208984375, B-A: 5.1988372802734375\n",
            "epoch: 8400, train_error(A): 32.23931884765625, test_error(B): 37.49394989013672, B-A: 5.254631042480469\n",
            "epoch: 8500, train_error(A): 32.206111907958984, test_error(B): 37.544925689697266, B-A: 5.338813781738281\n",
            "epoch: 8600, train_error(A): 32.351863861083984, test_error(B): 37.491943359375, B-A: 5.140079498291016\n",
            "epoch: 8700, train_error(A): 32.29930114746094, test_error(B): 37.46974563598633, B-A: 5.170444488525391\n",
            "epoch: 8800, train_error(A): 32.27703094482422, test_error(B): 37.59292984008789, B-A: 5.315898895263672\n",
            "epoch: 8900, train_error(A): 32.24544143676758, test_error(B): 37.48272705078125, B-A: 5.237285614013672\n",
            "epoch: 9000, train_error(A): 32.207706451416016, test_error(B): 37.6186408996582, B-A: 5.4109344482421875\n",
            "epoch: 9100, train_error(A): 32.227027893066406, test_error(B): 37.69569778442383, B-A: 5.468669891357422\n",
            "epoch: 9200, train_error(A): 32.15250015258789, test_error(B): 37.712013244628906, B-A: 5.559513092041016\n",
            "epoch: 9300, train_error(A): 32.181846618652344, test_error(B): 37.7200927734375, B-A: 5.538246154785156\n",
            "epoch: 9400, train_error(A): 32.280311584472656, test_error(B): 37.656280517578125, B-A: 5.375968933105469\n",
            "epoch: 9500, train_error(A): 32.32391357421875, test_error(B): 37.68242263793945, B-A: 5.358509063720703\n",
            "epoch: 9600, train_error(A): 32.14051818847656, test_error(B): 37.42610168457031, B-A: 5.28558349609375\n",
            "epoch: 9700, train_error(A): 32.1091423034668, test_error(B): 37.44811248779297, B-A: 5.338970184326172\n",
            "epoch: 9800, train_error(A): 32.12303924560547, test_error(B): 37.62075424194336, B-A: 5.497714996337891\n",
            "epoch: 9900, train_error(A): 32.1743049621582, test_error(B): 37.64210891723633, B-A: 5.467803955078125\n",
            "epoch: 10000, train_error(A): 32.13823699951172, test_error(B): 37.699214935302734, B-A: 5.560977935791016\n",
            "epoch: 10100, train_error(A): 32.0556526184082, test_error(B): 38.009193420410156, B-A: 5.953540802001953\n",
            "epoch: 10200, train_error(A): 32.150325775146484, test_error(B): 37.70407485961914, B-A: 5.553749084472656\n",
            "epoch: 10300, train_error(A): 32.149845123291016, test_error(B): 37.425270080566406, B-A: 5.275424957275391\n",
            "epoch: 10400, train_error(A): 32.3483772277832, test_error(B): 37.58711242675781, B-A: 5.238735198974609\n",
            "epoch: 10500, train_error(A): 32.14208984375, test_error(B): 37.61411666870117, B-A: 5.472026824951172\n",
            "epoch: 10600, train_error(A): 32.139434814453125, test_error(B): 37.777713775634766, B-A: 5.638278961181641\n",
            "epoch: 10700, train_error(A): 32.18058395385742, test_error(B): 37.72941207885742, B-A: 5.548828125\n",
            "epoch: 10800, train_error(A): 32.34944534301758, test_error(B): 37.52537155151367, B-A: 5.175926208496094\n",
            "epoch: 10900, train_error(A): 32.162506103515625, test_error(B): 37.44750213623047, B-A: 5.284996032714844\n",
            "epoch: 11000, train_error(A): 32.07819366455078, test_error(B): 37.42240524291992, B-A: 5.344211578369141\n",
            "epoch: 11100, train_error(A): 32.084571838378906, test_error(B): 37.59965896606445, B-A: 5.515087127685547\n",
            "epoch: 11200, train_error(A): 32.23889923095703, test_error(B): 37.73430633544922, B-A: 5.4954071044921875\n",
            "epoch: 11300, train_error(A): 32.16350173950195, test_error(B): 37.72224044799805, B-A: 5.558738708496094\n",
            "epoch: 11400, train_error(A): 32.11519241333008, test_error(B): 37.77878952026367, B-A: 5.663597106933594\n",
            "epoch: 11500, train_error(A): 32.20088577270508, test_error(B): 37.73482894897461, B-A: 5.533943176269531\n",
            "epoch: 11600, train_error(A): 32.13435745239258, test_error(B): 37.7342414855957, B-A: 5.599884033203125\n",
            "epoch: 11700, train_error(A): 32.065330505371094, test_error(B): 37.94359588623047, B-A: 5.878265380859375\n",
            "epoch: 11800, train_error(A): 32.00027847290039, test_error(B): 37.922950744628906, B-A: 5.922672271728516\n",
            "epoch: 11900, train_error(A): 32.15781021118164, test_error(B): 37.773216247558594, B-A: 5.615406036376953\n",
            "epoch: 12000, train_error(A): 32.01129150390625, test_error(B): 37.76167678833008, B-A: 5.750385284423828\n",
            "epoch: 12100, train_error(A): 32.07014846801758, test_error(B): 37.847618103027344, B-A: 5.777469635009766\n",
            "epoch: 12200, train_error(A): 32.235557556152344, test_error(B): 37.843997955322266, B-A: 5.608440399169922\n",
            "epoch: 12300, train_error(A): 32.11388397216797, test_error(B): 37.738277435302734, B-A: 5.624393463134766\n",
            "epoch: 12400, train_error(A): 32.0124626159668, test_error(B): 37.76810836791992, B-A: 5.755645751953125\n",
            "epoch: 12500, train_error(A): 32.03294372558594, test_error(B): 37.91429138183594, B-A: 5.88134765625\n",
            "epoch: 12600, train_error(A): 32.06725311279297, test_error(B): 37.73701477050781, B-A: 5.669761657714844\n",
            "epoch: 12700, train_error(A): 32.00882339477539, test_error(B): 37.7485466003418, B-A: 5.739723205566406\n",
            "epoch: 12800, train_error(A): 32.09935760498047, test_error(B): 37.810306549072266, B-A: 5.710948944091797\n",
            "epoch: 12900, train_error(A): 32.05937957763672, test_error(B): 37.85026168823242, B-A: 5.790882110595703\n",
            "epoch: 13000, train_error(A): 32.13753890991211, test_error(B): 37.94121170043945, B-A: 5.803672790527344\n",
            "epoch: 13100, train_error(A): 32.122947692871094, test_error(B): 37.987213134765625, B-A: 5.864265441894531\n",
            "epoch: 13200, train_error(A): 32.260562896728516, test_error(B): 37.85319519042969, B-A: 5.592632293701172\n",
            "epoch: 13300, train_error(A): 32.0639762878418, test_error(B): 37.776737213134766, B-A: 5.712760925292969\n",
            "epoch: 13400, train_error(A): 32.03568649291992, test_error(B): 37.8485221862793, B-A: 5.812835693359375\n",
            "epoch: 13500, train_error(A): 31.955747604370117, test_error(B): 37.99443817138672, B-A: 6.038690567016602\n",
            "epoch: 13600, train_error(A): 31.926786422729492, test_error(B): 38.042015075683594, B-A: 6.115228652954102\n",
            "epoch: 13700, train_error(A): 31.998716354370117, test_error(B): 38.15884017944336, B-A: 6.160123825073242\n",
            "epoch: 13800, train_error(A): 31.957279205322266, test_error(B): 38.228965759277344, B-A: 6.271686553955078\n",
            "epoch: 13900, train_error(A): 31.94764518737793, test_error(B): 38.14528274536133, B-A: 6.197637557983398\n",
            "epoch: 14000, train_error(A): 32.12940979003906, test_error(B): 38.0042610168457, B-A: 5.874851226806641\n",
            "epoch: 14100, train_error(A): 31.979169845581055, test_error(B): 37.91975021362305, B-A: 5.940580368041992\n",
            "epoch: 14200, train_error(A): 31.94549560546875, test_error(B): 37.873687744140625, B-A: 5.928192138671875\n",
            "epoch: 14300, train_error(A): 31.963815689086914, test_error(B): 37.89303970336914, B-A: 5.929224014282227\n",
            "epoch: 14400, train_error(A): 31.917213439941406, test_error(B): 38.052650451660156, B-A: 6.13543701171875\n",
            "epoch: 14500, train_error(A): 31.975082397460938, test_error(B): 38.12041473388672, B-A: 6.145332336425781\n",
            "epoch: 14600, train_error(A): 31.962343215942383, test_error(B): 37.99781036376953, B-A: 6.035467147827148\n",
            "epoch: 14700, train_error(A): 31.89887046813965, test_error(B): 38.240966796875, B-A: 6.342096328735352\n",
            "epoch: 14800, train_error(A): 31.878755569458008, test_error(B): 38.331573486328125, B-A: 6.452817916870117\n",
            "epoch: 14900, train_error(A): 31.892013549804688, test_error(B): 38.46158218383789, B-A: 6.569568634033203\n",
            "epoch: 15000, train_error(A): 31.961057662963867, test_error(B): 38.442134857177734, B-A: 6.481077194213867\n",
            "epoch: 15100, train_error(A): 32.09682846069336, test_error(B): 38.132286071777344, B-A: 6.035457611083984\n",
            "epoch: 15200, train_error(A): 31.925867080688477, test_error(B): 38.08517074584961, B-A: 6.159303665161133\n",
            "epoch: 15300, train_error(A): 32.00014877319336, test_error(B): 38.31554412841797, B-A: 6.315395355224609\n",
            "epoch: 15400, train_error(A): 31.93364715576172, test_error(B): 38.450008392333984, B-A: 6.516361236572266\n",
            "epoch: 15500, train_error(A): 32.06107711791992, test_error(B): 38.465431213378906, B-A: 6.404354095458984\n",
            "epoch: 15600, train_error(A): 32.03125, test_error(B): 38.31904983520508, B-A: 6.287799835205078\n",
            "epoch: 15700, train_error(A): 32.10988998413086, test_error(B): 38.339237213134766, B-A: 6.229347229003906\n",
            "epoch: 15800, train_error(A): 32.077144622802734, test_error(B): 38.634517669677734, B-A: 6.557373046875\n",
            "epoch: 15900, train_error(A): 31.983793258666992, test_error(B): 38.583038330078125, B-A: 6.599245071411133\n",
            "epoch: 16000, train_error(A): 31.98269271850586, test_error(B): 38.72710037231445, B-A: 6.744407653808594\n",
            "epoch: 16100, train_error(A): 31.995302200317383, test_error(B): 38.86215591430664, B-A: 6.866853713989258\n",
            "epoch: 16200, train_error(A): 32.00017166137695, test_error(B): 38.711997985839844, B-A: 6.711826324462891\n",
            "epoch: 16300, train_error(A): 31.97928810119629, test_error(B): 38.80664825439453, B-A: 6.827360153198242\n",
            "epoch: 16400, train_error(A): 32.0794792175293, test_error(B): 38.75340270996094, B-A: 6.673923492431641\n",
            "epoch: 16500, train_error(A): 31.949676513671875, test_error(B): 38.66159439086914, B-A: 6.711917877197266\n",
            "epoch: 16600, train_error(A): 31.9591064453125, test_error(B): 38.78248596191406, B-A: 6.8233795166015625\n",
            "epoch: 16700, train_error(A): 31.922685623168945, test_error(B): 38.921112060546875, B-A: 6.99842643737793\n",
            "epoch: 16800, train_error(A): 31.841445922851562, test_error(B): 38.959869384765625, B-A: 7.1184234619140625\n",
            "epoch: 16900, train_error(A): 32.02516555786133, test_error(B): 38.82539749145508, B-A: 6.80023193359375\n",
            "epoch: 17000, train_error(A): 31.944778442382812, test_error(B): 38.77711486816406, B-A: 6.83233642578125\n",
            "epoch: 17100, train_error(A): 31.960180282592773, test_error(B): 38.942352294921875, B-A: 6.982172012329102\n",
            "epoch: 17200, train_error(A): 31.822473526000977, test_error(B): 39.10918426513672, B-A: 7.286710739135742\n",
            "epoch: 17300, train_error(A): 31.9071044921875, test_error(B): 39.1445198059082, B-A: 7.237415313720703\n",
            "epoch: 17400, train_error(A): 31.941585540771484, test_error(B): 39.048763275146484, B-A: 7.107177734375\n",
            "epoch: 17500, train_error(A): 31.878782272338867, test_error(B): 38.76324462890625, B-A: 6.884462356567383\n",
            "epoch: 17600, train_error(A): 31.884037017822266, test_error(B): 38.96707534790039, B-A: 7.083038330078125\n",
            "epoch: 17700, train_error(A): 31.847396850585938, test_error(B): 39.28645324707031, B-A: 7.439056396484375\n",
            "epoch: 17800, train_error(A): 31.912694931030273, test_error(B): 39.25404357910156, B-A: 7.341348648071289\n",
            "epoch: 17900, train_error(A): 31.8333740234375, test_error(B): 39.260353088378906, B-A: 7.426979064941406\n",
            "epoch: 18000, train_error(A): 31.94414520263672, test_error(B): 39.20840072631836, B-A: 7.264255523681641\n",
            "recent_data.shape: (1, 46, 45)\n",
            "recent_data: [[[  0   0   0 ...   0 100   0]\n",
            "  [  0   0   0 ... 100   0   0]\n",
            "  [100   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [  0 100   0 ...   0   0   0]\n",
            "  [  0   0 100 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]]]\n",
            "test_predict_2  00000 = [(-28.0, 0), (9.0, 1), (7.0, 2), (3.0, 3), (31.0, 4), (10.0, 5), (40.0, 6), (9.0, 7), (8.0, 8), (15.0, 9), (6.0, 10), (11.0, 11), (24.0, 12), (28.0, 13), (38.0, 14), (17.0, 15), (20.0, 16), (1.0, 17), (55.0, 18), (-1.0, 19), (-9.0, 20), (4.0, 21), (0.0, 22), (0.0, 23), (-4.0, 24), (39.0, 25), (-16.0, 26), (18.0, 27), (-2.0, 28), (20.0, 29), (23.0, 30), (4.0, 31), (-20.0, 32), (-2.0, 33), (0.0, 34), (24.0, 35), (34.0, 36), (35.0, 37), (-4.0, 38), (8.0, 39), (12.0, 40), (29.0, 41), (71.0, 42), (2.0, 43), (22.0, 44)]\n",
            "test_predict_2  11111 [(71.0, 42), (55.0, 18), (40.0, 6), (39.0, 25), (38.0, 14), (35.0, 37), (34.0, 36), (31.0, 4), (29.0, 41), (28.0, 13), (24.0, 12), (24.0, 35), (23.0, 30), (22.0, 44), (20.0, 16), (20.0, 29), (18.0, 27), (17.0, 15), (15.0, 9), (12.0, 40), (11.0, 11), (10.0, 5), (9.0, 1), (9.0, 7), (8.0, 8), (8.0, 39), (7.0, 2), (6.0, 10), (4.0, 21), (4.0, 31), (3.0, 3), (2.0, 43), (1.0, 17), (0.0, 22), (0.0, 23), (0.0, 34), (-1.0, 19), (-2.0, 28), (-2.0, 33), (-4.0, 24), (-4.0, 38), (-9.0, 20), (-16.0, 26), (-20.0, 32), (-28.0, 0)]\n",
            "[43, 19, 7, 26, 15, 38]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O7T2b3Uw0n_u",
        "outputId": "ce55fe8f-e208-4fa5-fba8-d4b7a9d3891e"
      },
      "source": [
        "# 내일 종가를 예측해본다\n",
        "test_predict = sess.run(hypothesis, feed_dict={X: recent_data})\n",
        "\n",
        "test_predict_2 = []\n",
        "\n",
        "for i in range(len(test_predict[0])):\n",
        "    test_predict[0][i] = int(test_predict[0][i])\n",
        "    test_predict_2.append((test_predict[0][i], i))\n",
        "\n",
        "#final_predict = np.sort(test_predict_2, axis=1)[::-1]\n",
        "print(\"test_predict_2  00000 =\", test_predict_2)\n",
        "test_predict_2.sort(key = lambda element : element[0], reverse=True)\n",
        "\n",
        "\n",
        "# predict_six = []\n",
        "#\n",
        "# for i in final_predict:\n",
        "#     for j in test_predict[0]:\n",
        "#         if (final_predict[i] == test_predict[0][j])\n",
        "\n",
        "\n",
        "#print(\"test_predict 0 \", test_predict[0])\n",
        "print(\"test_predict_2  11111\", test_predict_2)\n",
        "\n",
        "picked = []\n",
        "for i in range(6):\n",
        "    picked.append(test_predict_2[i][1] + 1)\n",
        "\n",
        "print(picked)"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "test_predict_2  00000 = [(-28.0, 0), (9.0, 1), (7.0, 2), (3.0, 3), (31.0, 4), (10.0, 5), (40.0, 6), (9.0, 7), (8.0, 8), (15.0, 9), (6.0, 10), (11.0, 11), (24.0, 12), (28.0, 13), (38.0, 14), (17.0, 15), (20.0, 16), (1.0, 17), (55.0, 18), (-1.0, 19), (-9.0, 20), (4.0, 21), (0.0, 22), (0.0, 23), (-4.0, 24), (39.0, 25), (-16.0, 26), (18.0, 27), (-2.0, 28), (20.0, 29), (23.0, 30), (4.0, 31), (-20.0, 32), (-2.0, 33), (0.0, 34), (24.0, 35), (34.0, 36), (35.0, 37), (-4.0, 38), (8.0, 39), (12.0, 40), (29.0, 41), (71.0, 42), (2.0, 43), (22.0, 44)]\n",
            "test_predict_2  11111 [(71.0, 42), (55.0, 18), (40.0, 6), (39.0, 25), (38.0, 14), (35.0, 37), (34.0, 36), (31.0, 4), (29.0, 41), (28.0, 13), (24.0, 12), (24.0, 35), (23.0, 30), (22.0, 44), (20.0, 16), (20.0, 29), (18.0, 27), (17.0, 15), (15.0, 9), (12.0, 40), (11.0, 11), (10.0, 5), (9.0, 1), (9.0, 7), (8.0, 8), (8.0, 39), (7.0, 2), (6.0, 10), (4.0, 21), (4.0, 31), (3.0, 3), (2.0, 43), (1.0, 17), (0.0, 22), (0.0, 23), (0.0, 34), (-1.0, 19), (-2.0, 28), (-2.0, 33), (-4.0, 24), (-4.0, 38), (-9.0, 20), (-16.0, 26), (-20.0, 32), (-28.0, 0)]\n",
            "[43, 19, 7, 26, 15, 38]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP2-bRVzoZ9T"
      },
      "source": [
        "# ALL Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zzbFpN7ZokLu",
        "outputId": "f2b191c9-ba12-4dbe-cdfc-eec081315e9b"
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "!pip install tensorflow-gpu==2.0.0-rc1\n",
        "\n",
        "main_url = \"https://www.dhlottery.co.kr/gameResult.do?method=byWin\" # 마지막 회차를 얻기 위한 주소\n",
        "basic_url = \"https://www.dhlottery.co.kr/gameResult.do?method=byWin&drwNo=\" # 임의의 회차를 얻기 위한 주소\n",
        "\n",
        "# 마지막 회차 정보를 가져옴\n",
        "def GetLast(): \n",
        "    resp = requests.get(main_url)\n",
        "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "    result = str(soup.find(\"meta\", {\"id\" : \"desc\", \"name\" : \"description\"})['content'])\n",
        "    s_idx = result.find(\" \")\n",
        "    e_idx = result.find(\"회\")\n",
        "    return int(result[s_idx + 1 : e_idx])\n",
        "\n",
        "# 지정된 파일에 지정된 범위의 회차 정보를 기록함\n",
        "def Crawler(s_count, e_count, fp):\n",
        "    for i in range(s_count , e_count + 1):\n",
        "        crawler_url = basic_url + str(i)\n",
        "        resp = requests.get(crawler_url)\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        text = soup.text\n",
        "\n",
        "        s_idx = text.find(\" 당첨결과\")\n",
        "        s_idx = text.find(\"당첨번호\", s_idx) + 4\n",
        "        e_idx = text.find(\"보너스\", s_idx)\n",
        "        numbers = text[s_idx:e_idx].strip().split()\n",
        "\n",
        "        s_idx = e_idx + 3\n",
        "        e_idx = s_idx + 3\n",
        "        bonus = text[s_idx:e_idx].strip()\n",
        "\n",
        "        s_idx = text.find(\"1등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money1 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"2등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money2 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"3등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money3 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"4등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money4 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"5등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money5 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        line = str(i) + ',' + numbers[0] + ',' + numbers[1] + ',' + numbers[2] + ',' + numbers[3] + ',' + numbers[4] + ',' + numbers[5] + ',' + bonus + ',' + money1 + ',' + money2 + ',' + money3 + ',' + money4 + ',' + money5\n",
        "        print(line)\n",
        "        line += '\\n'\n",
        "        fp.write(line)\n",
        "\n",
        "last = GetLast() # 마지막 회차를 가져옴\n",
        "\n",
        "lotto_dir_name='lotto'\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "if not path.exists(lotto_base_dir):\n",
        "  print('Check your google drive directory. See you file explorer')\n",
        "\n",
        "\n",
        "print (\"Last=\",last)\n",
        "\n",
        "# 완전히 다시 쓰기\n",
        "#fp = open(path.join(lotto_base_dir, \"lotto_data.txt\"), 'w')\n",
        "\n",
        "# 기존 데이타에 추가하기\n",
        "fp = open(path.join(lotto_base_dir, \"lotto_data.txt\"), 'a')\n",
        "Crawler(last-1, last, fp) # 처음부터 마지막 회차까지 저장\n",
        "fp.close()\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rows = np.loadtxt(\"./gdrive/MyDrive/lotto_data.txt\", delimiter=\",\")\n",
        "row_count = len(rows)\n",
        "\n",
        "# 당첨번호를 원핫인코딩벡터(ohbin)으로 변환\n",
        "def numbers2ohbin(numbers):\n",
        "\n",
        "    ohbin = np.zeros(45) #45개의 빈 칸을 만듬\n",
        "\n",
        "    for i in range(6): #여섯개의 당첨번호에 대해서 반복함\n",
        "        ohbin[int(numbers[i])-1] = 1 #로또번호가 1부터 시작하지만 벡터의 인덱스 시작은 0부터 시작하므로 1을 뺌\n",
        "    \n",
        "    return ohbin\n",
        "\n",
        "# 원핫인코딩벡터(ohbin)를 번호로 변환\n",
        "def ohbin2numbers(ohbin):\n",
        "\n",
        "    numbers = []\n",
        "    \n",
        "    for i in range(len(ohbin)):\n",
        "        if ohbin[i] == 1.0: # 1.0으로 설정되어 있으면 해당 번호를 반환값에 추가한다.\n",
        "            numbers.append(i+1)\n",
        "    \n",
        "    return numbers\n",
        "\n",
        "\n",
        "numbers = rows[:, 1:7]\n",
        "ohbins = list(map(numbers2ohbin, numbers))\n",
        "\n",
        "x_samples = ohbins[0:row_count-1]\n",
        "y_samples = ohbins[1:row_count]\n",
        "\n",
        "model = tf.keras.models.load_model('./gdrive/MyDrive/my_model.h5')\n",
        "model.summary()\n",
        "\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "\n",
        "\n",
        "# 88회부터 지금까지 1등부터 5등까지 상금의 평균낸다.\n",
        "mean_prize = [  np.mean(rows[87:, 8]),\n",
        "            np.mean(rows[87:, 9]),\n",
        "            np.mean(rows[87:, 10]),\n",
        "            np.mean(rows[87:, 11]),\n",
        "            np.mean(rows[87:, 12])]\n",
        "\n",
        "print(mean_prize)   \n",
        "\n",
        "train_total_reward = []\n",
        "train_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "val_total_reward = []\n",
        "val_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "test_total_reward = []\n",
        "test_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "\n",
        " # 등수와 상금을 반환함\n",
        "# 순위에 오르지 못한 경우에는 등수가 0으로 반환함\n",
        "def calc_reward(true_numbers, true_bonus, pred_numbers):\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for ps in pred_numbers:\n",
        "        if ps in true_numbers:\n",
        "            count += 1\n",
        "\n",
        "    if count == 6:\n",
        "        return 0, mean_prize[0]\n",
        "    elif count == 5 and true_bonus in pred_numbers:\n",
        "        return 1, mean_prize[1]\n",
        "    elif count == 5:\n",
        "        return 2, mean_prize[2]\n",
        "    elif count == 4:\n",
        "        return 3, mean_prize[3]\n",
        "    elif count == 3:\n",
        "        return 4, mean_prize[4]\n",
        "\n",
        "    return 5, 0\n",
        "\n",
        "def gen_numbers_from_probability(nums_prob):\n",
        "\n",
        "    ball_box = []\n",
        "\n",
        "    for n in range(45):\n",
        "        ball_count = int(nums_prob[n] * 200 + 1)\n",
        "        ball = np.full((ball_count), n+1) #1부터 시작\n",
        "        ball_box += list(ball)\n",
        "\n",
        "    selected_balls = []\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        if len(selected_balls) == 6:\n",
        "            break\n",
        "        \n",
        "        ball_index = np.random.randint(len(ball_box), size=1)[0]\n",
        "        ball = ball_box[ball_index]\n",
        "\n",
        "        if ball not in selected_balls:\n",
        "            selected_balls.append(ball)\n",
        "\n",
        "   \n",
        "    selected_balls.sort()\n",
        "\n",
        "    return selected_balls\n",
        "\n",
        "print('receive numbers')\n",
        "\n",
        "xs = x_samples[-1].reshape(1, 1, 45)\n",
        "\n",
        "ys_pred = model.predict_on_batch(xs)\n",
        "\n",
        "list_numbers = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 최대 100번 에포크까지 수행\n",
        "for epoch in range(50):\n",
        "\n",
        "    model.reset_states() # 중요! 매 에포크마다 1회부터 다시 훈련하므로 상태 초기화 필요\n",
        "\n",
        "    batch_train_loss = []\n",
        "    batch_train_acc = []\n",
        "\n",
        "    for i in range(len(x_samples)):\n",
        "        \n",
        "        xs = x_samples[i].reshape(1, 1, 45)\n",
        "        ys = y_samples[i].reshape(1, 45)\n",
        "        \n",
        "        loss, acc = model.train_on_batch(xs, ys) #배치만큼 모델에 학습시킴\n",
        "\n",
        "        batch_train_loss.append(loss)\n",
        "        batch_train_acc.append(acc)\n",
        "\n",
        "    train_loss.append(np.mean(batch_train_loss))\n",
        "    train_acc.append(np.mean(batch_train_acc))\n",
        "\n",
        "    print('epoch {0:4d} train acc {1:0.3f} loss {2:0.3f}'.format(epoch, np.mean(batch_train_acc), np.mean(batch_train_loss)))  \n",
        "\n",
        "\n",
        "# Trainning 끝나면 무조건 모델에 저장\n",
        "model.save('my_model.h5')\n",
        "!cp /content/my_model.h5 /content/drive/My\\ Drive/\n",
        "\n",
        "\n",
        "# 마지막 회차까지 학습한 모델로 다음 회차 추론\n",
        "\n",
        "# 50 개 뽑기\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "with open(path.join(lotto_base_dir, \"predict2.txt\"), \"w\") as f:\n",
        "  print (\"predict.txt\")\n",
        "\n",
        "  for n in range(50):\n",
        "    numbers = gen_numbers_from_probability(ys_pred[0])  \n",
        "    print('{0} : {1}'.format(n, numbers))    \n",
        "    list_numbers.append(numbers)  \n",
        "    line_str=','.join(str(e) for e in numbers)\n",
        "    line_str += '\\n'\n",
        "    print(line_str)\n",
        "    f.write(line_str)\n",
        "f.close()\n",
        "\n",
        "from ftplib import FTP\n",
        "\n",
        "ftp = FTP('112.175.184.78')\n",
        "ftp.login('dalasjoe', 'Dalasjoe75!')\n",
        "\n",
        "# ftp.cwd('html') # \"test\"디렉터리로 이동\n",
        "# ftp.retrlines('LIST') # 디렉터리의 내용을 목록화\n",
        "# #ftp.retrbinary('RETR README', open('README', 'wb').write) # README 파일 저장\n",
        "# ftp.quit()\n",
        "\n",
        "\n",
        "ftp.cwd('html')  # 업로드할 FTP 폴더로 이동\n",
        "myfile = open(path.join(lotto_base_dir, \"predict2.txt\"),'rb')  # 로컬 파일 열기\n",
        "ftp.storbinary('STOR ' + 'predict2.txt', myfile )  # 파일을 FTP로 업로드\n",
        "myfile.close()  # 파일 닫기\n",
        "\n",
        "print (\"File Saved\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-rc1 in /usr/local/lib/python3.7/dist-packages (2.0.0rc1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.36.2)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.12.1)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.1.2)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.8.1)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.0.8)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.12.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.34.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (3.17.3)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.4.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.19.5)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.14.0.dev2019080601)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.15.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.2.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (3.3.0)\n",
            "Requirement already satisfied: tb-nightly<1.15.0a20190807,>=1.15.0a20190806 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.15.0a20190806)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.1.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0-rc1) (3.1.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (3.3.4)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (1.0.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (57.2.0)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (4.6.1)\n",
            "Requirement already satisfied: cached-property in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==2.0.0-rc1) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (3.5.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (3.7.4.3)\n",
            "Mounted at /content/gdrive\n",
            "Last= 975\n",
            "970,9,11,16,21,28,36,5,1611544045,43221488,1516850,50000,5000\n",
            "971,2,6,17,18,21,26,7,3725880250,60094843,1387665,50000,5000\n",
            "972,3,6,17,23,37,39,26,1124886244,46870261,1458997,50000,5000\n",
            "973,22,26,31,37,41,42,24,2912742750,92468024,1720717,50000,5000\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (1, 128)                  89088     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (1, 45)                   5805      \n",
            "=================================================================\n",
            "Total params: 94,893\n",
            "Trainable params: 94,893\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n",
            "[2432049266.472973, 57215317.41891892, 1451261.6441441441, 52782.632882882885, 5000.0]\n",
            "receive numbers\n",
            "epoch    0 train acc 0.174 loss 0.026\n",
            "epoch    1 train acc 0.190 loss 0.026\n",
            "epoch    2 train acc 0.164 loss 0.015\n",
            "epoch    3 train acc 0.189 loss 0.011\n",
            "epoch    4 train acc 0.181 loss 0.010\n",
            "epoch    5 train acc 0.182 loss 0.010\n",
            "epoch    6 train acc 0.166 loss 0.012\n",
            "epoch    7 train acc 0.189 loss 0.012\n",
            "epoch    8 train acc 0.167 loss 0.010\n",
            "epoch    9 train acc 0.180 loss 0.010\n",
            "epoch   10 train acc 0.165 loss 0.012\n",
            "epoch   11 train acc 0.162 loss 0.015\n",
            "epoch   12 train acc 0.163 loss 0.010\n",
            "epoch   13 train acc 0.182 loss 0.009\n",
            "epoch   14 train acc 0.174 loss 0.007\n",
            "epoch   15 train acc 0.201 loss 0.008\n",
            "epoch   16 train acc 0.178 loss 0.011\n",
            "epoch   17 train acc 0.186 loss 0.012\n",
            "epoch   18 train acc 0.182 loss 0.011\n",
            "epoch   19 train acc 0.164 loss 0.012\n",
            "epoch   20 train acc 0.169 loss 0.010\n",
            "epoch   21 train acc 0.150 loss 0.009\n",
            "epoch   22 train acc 0.149 loss 0.010\n",
            "epoch   23 train acc 0.188 loss 0.007\n",
            "epoch   24 train acc 0.172 loss 0.008\n",
            "epoch   25 train acc 0.175 loss 0.008\n",
            "epoch   26 train acc 0.176 loss 0.009\n",
            "epoch   27 train acc 0.163 loss 0.009\n",
            "epoch   28 train acc 0.169 loss 0.009\n",
            "epoch   29 train acc 0.164 loss 0.010\n",
            "epoch   30 train acc 0.160 loss 0.010\n",
            "epoch   31 train acc 0.159 loss 0.007\n",
            "epoch   32 train acc 0.174 loss 0.006\n",
            "epoch   33 train acc 0.175 loss 0.006\n",
            "epoch   34 train acc 0.141 loss 0.011\n",
            "epoch   35 train acc 0.157 loss 0.013\n",
            "epoch   36 train acc 0.161 loss 0.008\n",
            "epoch   37 train acc 0.162 loss 0.007\n",
            "epoch   38 train acc 0.178 loss 0.009\n",
            "epoch   39 train acc 0.155 loss 0.008\n",
            "epoch   40 train acc 0.175 loss 0.004\n",
            "epoch   41 train acc 0.180 loss 0.007\n",
            "epoch   42 train acc 0.154 loss 0.010\n",
            "epoch   43 train acc 0.158 loss 0.010\n",
            "epoch   44 train acc 0.175 loss 0.009\n",
            "epoch   45 train acc 0.158 loss 0.005\n",
            "epoch   46 train acc 0.149 loss 0.004\n",
            "epoch   47 train acc 0.162 loss 0.006\n",
            "epoch   48 train acc 0.188 loss 0.007\n",
            "epoch   49 train acc 0.170 loss 0.007\n",
            "cp: cannot create regular file '/content/drive/My Drive/': No such file or directory\n",
            "Mounted at /content/gdrive\n",
            "predict.txt\n",
            "0 : [6, 10, 29, 32, 34, 41]\n",
            "6,10,29,32,34,41\n",
            "\n",
            "1 : [6, 10, 14, 19, 30, 32]\n",
            "6,10,14,19,30,32\n",
            "\n",
            "2 : [6, 10, 15, 31, 32, 34]\n",
            "6,10,15,31,32,34\n",
            "\n",
            "3 : [4, 6, 10, 26, 32, 41]\n",
            "4,6,10,26,32,41\n",
            "\n",
            "4 : [6, 10, 11, 12, 18, 26]\n",
            "6,10,11,12,18,26\n",
            "\n",
            "5 : [6, 10, 12, 30, 32, 34]\n",
            "6,10,12,30,32,34\n",
            "\n",
            "6 : [3, 6, 31, 32, 34, 41]\n",
            "3,6,31,32,34,41\n",
            "\n",
            "7 : [6, 10, 12, 26, 32, 40]\n",
            "6,10,12,26,32,40\n",
            "\n",
            "8 : [6, 10, 26, 32, 34, 40]\n",
            "6,10,26,32,34,40\n",
            "\n",
            "9 : [6, 10, 32, 34, 37, 40]\n",
            "6,10,32,34,37,40\n",
            "\n",
            "10 : [7, 26, 30, 32, 34, 41]\n",
            "7,26,30,32,34,41\n",
            "\n",
            "11 : [6, 12, 20, 26, 32, 34]\n",
            "6,12,20,26,32,34\n",
            "\n",
            "12 : [10, 26, 32, 34, 40, 41]\n",
            "10,26,32,34,40,41\n",
            "\n",
            "13 : [6, 10, 16, 32, 34, 41]\n",
            "6,10,16,32,34,41\n",
            "\n",
            "14 : [6, 10, 18, 26, 30, 32]\n",
            "6,10,18,26,30,32\n",
            "\n",
            "15 : [6, 18, 32, 34, 37, 44]\n",
            "6,18,32,34,37,44\n",
            "\n",
            "16 : [6, 26, 31, 32, 34, 37]\n",
            "6,26,31,32,34,37\n",
            "\n",
            "17 : [6, 10, 12, 18, 32, 34]\n",
            "6,10,12,18,32,34\n",
            "\n",
            "18 : [6, 10, 14, 26, 32, 41]\n",
            "6,10,14,26,32,41\n",
            "\n",
            "19 : [10, 18, 32, 34, 40, 41]\n",
            "10,18,32,34,40,41\n",
            "\n",
            "20 : [6, 10, 26, 32, 34, 40]\n",
            "6,10,26,32,34,40\n",
            "\n",
            "21 : [6, 10, 18, 26, 32, 34]\n",
            "6,10,18,26,32,34\n",
            "\n",
            "22 : [6, 13, 14, 25, 26, 34]\n",
            "6,13,14,25,26,34\n",
            "\n",
            "23 : [6, 10, 26, 32, 34, 37]\n",
            "6,10,26,32,34,37\n",
            "\n",
            "24 : [6, 10, 18, 31, 32, 34]\n",
            "6,10,18,31,32,34\n",
            "\n",
            "25 : [6, 18, 19, 32, 34, 42]\n",
            "6,18,19,32,34,42\n",
            "\n",
            "26 : [4, 26, 30, 31, 32, 34]\n",
            "4,26,30,31,32,34\n",
            "\n",
            "27 : [6, 10, 31, 32, 34, 37]\n",
            "6,10,31,32,34,37\n",
            "\n",
            "28 : [6, 10, 26, 32, 42, 43]\n",
            "6,10,26,32,42,43\n",
            "\n",
            "29 : [6, 18, 26, 30, 32, 34]\n",
            "6,18,26,30,32,34\n",
            "\n",
            "30 : [6, 10, 22, 26, 31, 32]\n",
            "6,10,22,26,31,32\n",
            "\n",
            "31 : [3, 6, 10, 12, 32, 34]\n",
            "3,6,10,12,32,34\n",
            "\n",
            "32 : [6, 12, 16, 26, 31, 32]\n",
            "6,12,16,26,31,32\n",
            "\n",
            "33 : [6, 10, 14, 26, 32, 34]\n",
            "6,10,14,26,32,34\n",
            "\n",
            "34 : [6, 10, 13, 18, 31, 32]\n",
            "6,10,13,18,31,32\n",
            "\n",
            "35 : [6, 10, 12, 26, 32, 34]\n",
            "6,10,12,26,32,34\n",
            "\n",
            "36 : [6, 10, 30, 31, 32, 34]\n",
            "6,10,30,31,32,34\n",
            "\n",
            "37 : [6, 10, 12, 32, 34, 36]\n",
            "6,10,12,32,34,36\n",
            "\n",
            "38 : [4, 6, 10, 14, 32, 34]\n",
            "4,6,10,14,32,34\n",
            "\n",
            "39 : [10, 18, 26, 32, 34, 40]\n",
            "10,18,26,32,34,40\n",
            "\n",
            "40 : [6, 10, 18, 32, 40, 41]\n",
            "6,10,18,32,40,41\n",
            "\n",
            "41 : [6, 10, 31, 32, 34, 42]\n",
            "6,10,31,32,34,42\n",
            "\n",
            "42 : [6, 10, 31, 32, 34, 37]\n",
            "6,10,31,32,34,37\n",
            "\n",
            "43 : [6, 10, 26, 32, 34, 39]\n",
            "6,10,26,32,34,39\n",
            "\n",
            "44 : [6, 12, 26, 30, 32, 34]\n",
            "6,12,26,30,32,34\n",
            "\n",
            "45 : [6, 10, 12, 16, 18, 32]\n",
            "6,10,12,16,18,32\n",
            "\n",
            "46 : [6, 10, 31, 32, 40, 41]\n",
            "6,10,31,32,40,41\n",
            "\n",
            "47 : [6, 26, 32, 34, 40, 41]\n",
            "6,26,32,34,40,41\n",
            "\n",
            "48 : [10, 18, 31, 32, 34, 40]\n",
            "10,18,31,32,34,40\n",
            "\n",
            "49 : [15, 30, 32, 34, 37, 40]\n",
            "15,30,32,34,37,40\n",
            "\n",
            "File Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BSm4U5BjfO1R"
      },
      "source": [
        "# 저장된 데이터 확인"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MABE44rDfTf8"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from os import path\n",
        "import numpy as np\n",
        "from google.colab import drive\n",
        "\n",
        "otto_dir_name='lotto'\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "\n",
        "if not path.exists(lotto_base_dir):\n",
        "  print('Check your google drive directory. See you file explorer')\n",
        "\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "with open(path.join(lotto_base_dir, \"lotto_data.txt\"), \"r\") as f:\n",
        "  print(\"11111\")\n",
        "  line = f.read()\n",
        "  print(\">>>\" + line)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZRsvoibHt5d"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q8MptZ7cjH8T"
      },
      "source": [
        "# **데이터 수집**  - Crawling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6kMeQ2_aCfm"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "main_url = \"https://www.dhlottery.co.kr/gameResult.do?method=byWin\" # 마지막 회차를 얻기 위한 주소\n",
        "basic_url = \"https://www.dhlottery.co.kr/gameResult.do?method=byWin&drwNo=\" # 임의의 회차를 얻기 위한 주소\n",
        "\n",
        "# 마지막 회차 정보를 가져옴\n",
        "def GetLast(): \n",
        "    resp = requests.get(main_url)\n",
        "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "    result = str(soup.find(\"meta\", {\"id\" : \"desc\", \"name\" : \"description\"})['content'])\n",
        "    s_idx = result.find(\" \")\n",
        "    e_idx = result.find(\"회\")\n",
        "    return int(result[s_idx + 1 : e_idx])\n",
        "\n",
        "# 지정된 파일에 지정된 범위의 회차 정보를 기록함\n",
        "def Crawler(s_count, e_count, fp):\n",
        "    for i in range(s_count , e_count + 1):\n",
        "        crawler_url = basic_url + str(i)\n",
        "        resp = requests.get(crawler_url)\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        text = soup.text\n",
        "\n",
        "        s_idx = text.find(\" 당첨결과\")\n",
        "        s_idx = text.find(\"당첨번호\", s_idx) + 4\n",
        "        e_idx = text.find(\"보너스\", s_idx)\n",
        "        numbers = text[s_idx:e_idx].strip().split()\n",
        "\n",
        "        s_idx = e_idx + 3\n",
        "        e_idx = s_idx + 3\n",
        "        bonus = text[s_idx:e_idx].strip()\n",
        "\n",
        "        s_idx = text.find(\"1등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money1 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"2등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money2 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"3등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money3 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"4등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money4 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"5등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money5 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        line = str(i) + ',' + numbers[0] + ',' + numbers[1] + ',' + numbers[2] + ',' + numbers[3] + ',' + numbers[4] + ',' + numbers[5] + ',' + bonus + ',' + money1 + ',' + money2 + ',' + money3 + ',' + money4 + ',' + money5\n",
        "        print(line)\n",
        "        line += '\\n'\n",
        "        fp.write(line)\n",
        "\n",
        "last = GetLast() # 마지막 회차를 가져옴\n",
        "\n",
        "lotto_dir_name='lotto'\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "if not path.exists(lotto_base_dir):\n",
        "  print('Check your google drive directory. See you file explorer')\n",
        "\n",
        "#with open(path.join(lotto_base_dir, \"lotto_data.txt\"), \"r\") as f:\n",
        "#  print(\"11111\")\n",
        "#   line = f.read()\n",
        "#   print(\">>>\" + line)\n",
        "\n",
        "print (\"Last=\",last)\n",
        "\n",
        "# 완전히 다시 쓰기\n",
        "#fp = open(path.join(lotto_base_dir, \"lotto_data.txt\"), 'w')\n",
        "\n",
        "# 기존 데이타에 추가하기\n",
        "fp = open(path.join(lotto_base_dir, \"lotto_data.txt\"), 'a')\n",
        "\n",
        "Crawler(1, last, fp) # 처음부터 마지막 회차까지 저장\n",
        "fp.close()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b59-XaqSHwpT"
      },
      "source": [
        "## 데이터 로딩"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CGhgk1uAH1KF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a8244175-9b54-4adc-a5ed-2d08cac781fd"
      },
      "source": [
        "from os import path\n",
        "from google.colab import drive\n",
        "import numpy as np\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "rows = np.loadtxt(\"./drive/MyDrive/lotto_data.txt\", delimiter=\",\")\n",
        "row_count = len(rows)\n",
        "\n",
        "print(\"Data Loaded : row count: \" + str(row_count))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "Data Loaded : row count: 965\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjyyBnKlfNfS"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lJ0yqd9Xjmwh"
      },
      "source": [
        "# **TensorFlow 설치 및 모델 초기화**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t-vuJrQn2pNt"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "!pip install tensorflow-gpu==2.0.0-rc1\n",
        "\n",
        "# 당첨번호를 원핫인코딩벡터(ohbin)으로 변환\n",
        "def numbers2ohbin(numbers):\n",
        "\n",
        "    ohbin = np.zeros(45) #45개의 빈 칸을 만듬\n",
        "\n",
        "    for i in range(6): #여섯개의 당첨번호에 대해서 반복함\n",
        "        ohbin[int(numbers[i])-1] = 1 #로또번호가 1부터 시작하지만 벡터의 인덱스 시작은 0부터 시작하므로 1을 뺌\n",
        "    \n",
        "    return ohbin\n",
        "\n",
        "# 원핫인코딩벡터(ohbin)를 번호로 변환\n",
        "def ohbin2numbers(ohbin):\n",
        "\n",
        "    numbers = []\n",
        "    \n",
        "    for i in range(len(ohbin)):\n",
        "        if ohbin[i] == 1.0: # 1.0으로 설정되어 있으면 해당 번호를 반환값에 추가한다.\n",
        "            numbers.append(i+1)\n",
        "    \n",
        "    return numbers\n",
        "\n",
        "\n",
        "numbers = rows[:, 1:7]\n",
        "ohbins = list(map(numbers2ohbin, numbers))\n",
        "\n",
        "x_samples = ohbins[0:row_count-1]\n",
        "y_samples = ohbins[1:row_count]\n",
        "\n",
        "#원핫인코딩으로 표시\n",
        "print(\"ohbins\")\n",
        "print(\"X[0]: \" + str(x_samples[0]))\n",
        "print(\"Y[0]: \" + str(y_samples[0]))\n",
        "\n",
        "#번호로 표시\n",
        "print(\"numbers\")\n",
        "print(\"X[0]: \" + str(ohbin2numbers(x_samples[0])))\n",
        "print(\"Y[0]: \" + str(ohbin2numbers(y_samples[0])))\n",
        "\n",
        "# 데이터셋 구성\n",
        "train_idx = (0, 700) # 훈련셋 \n",
        "val_idx = (700, 800) # 검증셋\n",
        "test_idx = (800, len(x_samples)) #시험셋\n",
        "\n",
        "print(\"train: {0}, val: {1}, test: {2}\".format(train_idx, val_idx, test_idx))\n",
        "\n",
        "# 모델을 정의합니다. - LSTM\n",
        "#타입스텝은 1인 대신, 상태유지(stateful 옵션을 True)으로 설정했습니다.\n",
        "#45개의 벡터로 출력합니다.\n",
        "#각각의 벡터는 0.0과 1.0사이의 실수값으로 나옵니다. 각 벡터가 독립적으로 모두 1.0이 나오거나 0.0이 나올 수 있는 멀티레이블 문제입니다.\n",
        "#멀티레이블 문제라 출력층의 활성화함수가 softmax가 아닌 sigmoid로 설정하였습니다.\n",
        "model = keras.Sequential([\n",
        "    keras.layers.LSTM(128, batch_input_shape=(1, 1, 45), return_sequences=False, stateful=True),\n",
        "    keras.layers.Dense(45, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 모델을 컴파일합니다.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "# 매 에포크마다 훈련과 검증의 손실 및 정확도를 기록하기 위한 변수\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "\n",
        "\n",
        "# 88회부터 지금까지 1등부터 5등까지 상금의 평균낸다.\n",
        "mean_prize = [  np.mean(rows[87:, 8]),\n",
        "            np.mean(rows[87:, 9]),\n",
        "            np.mean(rows[87:, 10]),\n",
        "            np.mean(rows[87:, 11]),\n",
        "            np.mean(rows[87:, 12])]\n",
        "\n",
        "print(mean_prize)   \n",
        "\n",
        "train_total_reward = []\n",
        "train_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "val_total_reward = []\n",
        "val_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "test_total_reward = []\n",
        "test_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "model.reset_states()\n",
        "\n",
        "# print('[No. ] 1st 2nd 3rd 4th 5th 6th Rewards')\n",
        "\n",
        "# for i in range(len(x_samples)):\n",
        "#     xs = x_samples[i].reshape(1, 1, 45)\n",
        "#     ys_pred = model.predict_on_batch(xs) # 모델의 출력값을 얻음\n",
        "    \n",
        "#     sum_reward = 0\n",
        "#     sum_grade = np.zeros(6, dtype=int) # 6등까지 변수\n",
        "\n",
        "#     for n in range(10): # 10판 수행\n",
        "#         numbers = gen_numbers_from_probability(ys_pred[0])\n",
        "        \n",
        "#         #i회차 입력 후 나온 출력을 i+1회차와 비교함\n",
        "#         grade, reward = calc_reward(rows[i+1,1:7], rows[i+1,7], numbers) \n",
        "\n",
        "#         sum_reward += reward\n",
        "#         sum_grade[grade] += 1\n",
        "\n",
        "#         if i >= train_idx[0] and i < train_idx[1]:\n",
        "#             train_total_grade[grade] += 1\n",
        "#         elif i >= val_idx[0] and i < val_idx[1]:\n",
        "#             val_total_grade[grade] += 1\n",
        "#         elif i >= test_idx[0] and i < test_idx[1]:\n",
        "#             val_total_grade[grade] += 1\n",
        "    \n",
        "#     if i >= train_idx[0] and i < train_idx[1]:\n",
        "#         train_total_reward.append(sum_reward)\n",
        "#     elif i >= val_idx[0] and i < val_idx[1]:\n",
        "#         val_total_reward.append(sum_reward)\n",
        "#     elif i >= test_idx[0] and i < test_idx[1]:\n",
        "#         test_total_reward.append(sum_reward)\n",
        "                        \n",
        "#     print('[{0:4d}] {1:3d} {2:3d} {3:3d} {4:3d} {5:3d} {6:3d} {7:15,d}'.format(i+1, sum_grade[0], sum_grade[1], sum_grade[2], sum_grade[3], sum_grade[4], sum_grade[5], int(sum_reward)))\n",
        "\n",
        "# print('Total') \n",
        "# print('==========')    \n",
        "# print('Train {0:5d} {1:5d} {2:5d} {3:5d} {4:5d} {5:5d} {6:15,d}'.format(train_total_grade[0], train_total_grade[1], train_total_grade[2], train_total_grade[3], train_total_grade[4], train_total_grade[5], int(sum(train_total_reward))))\n",
        "# print('Val   {0:5d} {1:5d} {2:5d} {3:5d} {4:5d} {5:5d} {6:15,d}'.format(val_total_grade[0], val_total_grade[1], val_total_grade[2], val_total_grade[3], val_total_grade[4], val_total_grade[5], int(sum(val_total_reward))))\n",
        "# print('Test  {0:5d} {1:5d} {2:5d} {3:5d} {4:5d} {5:5d} {6:15,d}'.format(test_total_grade[0], test_total_grade[1], test_total_grade[2], test_total_grade[3], test_total_grade[4], test_total_grade[5], int(sum(test_total_reward))))\n",
        "# print('==========')   "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jigZv6GgC35S"
      },
      "source": [
        "model = tf.keras.models.load_model('./drive/MyDrive/my_model.h5')\n",
        "model.summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2S6KaOT3lyq5"
      },
      "source": [
        "# **Trainning**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iyJvMPVl5QV8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 394
        },
        "outputId": "1a70fd96-2cbd-4d8b-aebf-0d016f103efc"
      },
      "source": [
        "# 최대 100번 에포크까지 수행\n",
        "\n",
        "# function ClickConnect(){\n",
        "#     console.log(\"코랩 연결 끊김 방지\"); \n",
        "#     document.querySelector(\"colab-toolbar-button#connect\").click() \n",
        "# }\n",
        "# setInterval(ClickConnect, 60 * 1000)\n",
        "\n",
        "model = tf.keras.models.load_model('./drive/MyDrive/my_model.h5')\n",
        "model.summary()\n",
        "\n",
        "for epoch in range(50):\n",
        "\n",
        "    model.reset_states() # 중요! 매 에포크마다 1회부터 다시 훈련하므로 상태 초기화 필요\n",
        "\n",
        "    batch_train_loss = []\n",
        "    batch_train_acc = []\n",
        "\n",
        "    for i in range(len(x_samples)):\n",
        "        \n",
        "        xs = x_samples[i].reshape(1, 1, 45)\n",
        "        ys = y_samples[i].reshape(1, 45)\n",
        "        \n",
        "        loss, acc = model.train_on_batch(xs, ys) #배치만큼 모델에 학습시킴\n",
        "\n",
        "        batch_train_loss.append(loss)\n",
        "        batch_train_acc.append(acc)\n",
        "\n",
        "    train_loss.append(np.mean(batch_train_loss))\n",
        "    train_acc.append(np.mean(batch_train_acc))\n",
        "\n",
        "    print('epoch {0:4d} train acc {1:0.3f} loss {2:0.3f}'.format(epoch, np.mean(batch_train_acc), np.mean(batch_train_loss)))  \n",
        "\n",
        "\n",
        "# Trainning 끝나면 무조건 모델에 저장\n",
        "model.save('my_model.h5')\n",
        "!cp /content/my_model.h5 /content/drive/My\\ Drive/\n",
        "\n",
        "# 마지막 회차까지 학습한 모델로 다음 회차 추론\n",
        "\n",
        " # 등수와 상금을 반환함\n",
        "# 순위에 오르지 못한 경우에는 등수가 0으로 반환함\n",
        "def calc_reward(true_numbers, true_bonus, pred_numbers):\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for ps in pred_numbers:\n",
        "        if ps in true_numbers:\n",
        "            count += 1\n",
        "\n",
        "    if count == 6:\n",
        "        return 0, mean_prize[0]\n",
        "    elif count == 5 and true_bonus in pred_numbers:\n",
        "        return 1, mean_prize[1]\n",
        "    elif count == 5:\n",
        "        return 2, mean_prize[2]\n",
        "    elif count == 4:\n",
        "        return 3, mean_prize[3]\n",
        "    elif count == 3:\n",
        "        return 4, mean_prize[4]\n",
        "\n",
        "    return 5, 0\n",
        "\n",
        "def gen_numbers_from_probability(nums_prob):\n",
        "\n",
        "    ball_box = []\n",
        "\n",
        "    for n in range(45):\n",
        "        ball_count = int(nums_prob[n] * 100 + 1)\n",
        "        ball = np.full((ball_count), n+1) #1부터 시작\n",
        "        ball_box += list(ball)\n",
        "\n",
        "    selected_balls = []\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        if len(selected_balls) == 6:\n",
        "            break\n",
        "        \n",
        "        ball_index = np.random.randint(len(ball_box), size=1)[0]\n",
        "        ball = ball_box[ball_index]\n",
        "\n",
        "        if ball not in selected_balls:\n",
        "            selected_balls.append(ball)\n",
        "\n",
        "   \n",
        "    selected_balls.sort()\n",
        "\n",
        "    return selected_balls\n",
        "\n",
        "print('receive numbers')\n",
        "\n",
        "xs = x_samples[-1].reshape(1, 1, 45)\n",
        "\n",
        "ys_pred = model.predict_on_batch(xs)\n",
        "\n",
        "list_numbers = []\n",
        "\n",
        "# 50 개 뽑기\n",
        "# for n in range(100):\n",
        "#     numbers = gen_numbers_from_probability(ys_pred[0])  \n",
        "#     print('{0} : {1}'.format(n, numbers))    \n",
        "#     list_numbers.append(numbers)  \n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "with open(path.join(lotto_base_dir, \"predict.txt\"), \"w\") as f:\n",
        "  print (\"predict.txt\")\n",
        "\n",
        "  for n in range(50):\n",
        "    numbers = gen_numbers_from_probability(ys_pred[0])  \n",
        "    print('{0} : {1}'.format(n, numbers))    \n",
        "    list_numbers.append(numbers)  \n",
        "    line_str=','.join(str(e) for e in numbers)\n",
        "    line_str += '\\n'\n",
        "    print(line_str)\n",
        "    f.write(line_str)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-35-b46e1ab03b0b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;31m# setInterval(ClickConnect, 60 * 1000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./drive/MyDrive/my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile, options)\u001b[0m\n\u001b[1;32m    204\u001b[0m         \u001b[0mfilepath\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpath_to_string\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    205\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 206\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    207\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m   raise IOError(\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/saving/saved_model/load.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(path, compile, options)\u001b[0m\n\u001b[1;32m    119\u001b[0m   \u001b[0;31m# Look for metadata file or parse the SavedModel\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m   \u001b[0mmetadata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msaved_metadata_pb2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSavedMetadata\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 121\u001b[0;31m   \u001b[0mmeta_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmeta_graphs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    122\u001b[0m   \u001b[0mobject_graph_def\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmeta_graph_def\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobject_graph_def\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m   \u001b[0mpath_to_metadata_pb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_METADATA_PATH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m    114\u001b[0m         \u001b[0;34m\"SavedModel file does not exist at: %s%s{%s|%s}\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         (export_dir, os.path.sep, constants.SAVED_MODEL_FILENAME_PBTXT,\n\u001b[0;32m--> 116\u001b[0;31m          constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: ./drive/MyDrive/my_model.h5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzXKsUUUmCyg"
      },
      "source": [
        "# **Prediction**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wcvMpyO_oXCF"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TzZWF0ns5Xk6"
      },
      "source": [
        "# 마지막 회차까지 학습한 모델로 다음 회차 추론\n",
        "\n",
        " # 등수와 상금을 반환함\n",
        "# 순위에 오르지 못한 경우에는 등수가 0으로 반환함\n",
        "def calc_reward(true_numbers, true_bonus, pred_numbers):\n",
        "\n",
        "    count = 0\n",
        "\n",
        "    for ps in pred_numbers:\n",
        "        if ps in true_numbers:\n",
        "            count += 1\n",
        "\n",
        "    if count == 6:\n",
        "        return 0, mean_prize[0]\n",
        "    elif count == 5 and true_bonus in pred_numbers:\n",
        "        return 1, mean_prize[1]\n",
        "    elif count == 5:\n",
        "        return 2, mean_prize[2]\n",
        "    elif count == 4:\n",
        "        return 3, mean_prize[3]\n",
        "    elif count == 3:\n",
        "        return 4, mean_prize[4]\n",
        "\n",
        "    return 5, 0\n",
        "\n",
        "def gen_numbers_from_probability(nums_prob):\n",
        "\n",
        "    ball_box = []\n",
        "\n",
        "    for n in range(45):\n",
        "        ball_count = int(nums_prob[n] * 100 + 1)\n",
        "        ball = np.full((ball_count), n+1) #1부터 시작\n",
        "        ball_box += list(ball)\n",
        "\n",
        "    selected_balls = []\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        if len(selected_balls) == 6:\n",
        "            break\n",
        "        \n",
        "        ball_index = np.random.randint(len(ball_box), size=1)[0]\n",
        "        ball = ball_box[ball_index]\n",
        "\n",
        "        if ball not in selected_balls:\n",
        "            selected_balls.append(ball)\n",
        "\n",
        "   \n",
        "    selected_balls.sort()\n",
        "\n",
        "    return selected_balls\n",
        "\n",
        "print('receive numbers')\n",
        "\n",
        "xs = x_samples[-1].reshape(1, 1, 45)\n",
        "\n",
        "ys_pred = model.predict_on_batch(xs)\n",
        "\n",
        "list_numbers = []\n",
        "\n",
        "# 50 개 뽑기\n",
        "# for n in range(100):\n",
        "#     numbers = gen_numbers_from_probability(ys_pred[0])  \n",
        "#     print('{0} : {1}'.format(n, numbers))    \n",
        "#     list_numbers.append(numbers)  \n",
        "\n",
        "\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "with open(path.join(lotto_base_dir, \"predict.txt\"), \"w\") as f:\n",
        "  print (\"predict.txt\")\n",
        "\n",
        "  for n in range(50):\n",
        "    numbers = gen_numbers_from_probability(ys_pred[0])  \n",
        "    print('{0} : {1}'.format(n, numbers))    \n",
        "    list_numbers.append(numbers)  \n",
        "    line_str=','.join(str(e) for e in numbers)\n",
        "    line_str += '\\n'\n",
        "    print(line_str)\n",
        "    f.write(line_str)\n",
        "f.close()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOjlv9p90YKR"
      },
      "source": [
        "# **Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzmxpXaV0dCc"
      },
      "source": [
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvzmqDDf17WD"
      },
      "source": [
        "# **Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqdRq1Rn2CKo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66a5cbfd-918e-4ed9-ef0c-4390a006fd69"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "!pip install tensorflow-gpu==2.0.0-rc1\n",
        "\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "\n",
        "new_model = tf.keras.models.load_model('./gdrive/MyDrive/my_model.h5')\n",
        "new_model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.0-rc1 in /usr/local/lib/python3.7/dist-packages (2.0.0rc1)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.34.1)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (3.12.4)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.19.5)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.2.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.1.2)\n",
            "Requirement already satisfied: tb-nightly<1.15.0a20190807,>=1.15.0a20190806 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.15.0a20190806)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.36.2)\n",
            "Requirement already satisfied: tf-estimator-nightly<1.14.0.dev2019080602,>=1.14.0.dev2019080601 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.14.0.dev2019080601)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.1.0)\n",
            "Requirement already satisfied: gast>=0.2.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.4.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.0.8)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (3.3.0)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.8.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (1.12.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.0-rc1) (0.12.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from protobuf>=3.6.1->tensorflow-gpu==2.0.0-rc1) (56.1.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (3.3.4)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.0-rc1) (3.1.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (4.0.1)\n",
            "Requirement already satisfied: cached-property; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from h5py->keras-applications>=1.0.8->tensorflow-gpu==2.0.0-rc1) (1.5.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tb-nightly<1.15.0a20190807,>=1.15.0a20190806->tensorflow-gpu==2.0.0-rc1) (3.7.4.3)\n",
            "Mounted at /content/gdrive\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "lstm (LSTM)                  (1, 128)                  89088     \n",
            "_________________________________________________________________\n",
            "dense (Dense)                (1, 45)                   5805      \n",
            "=================================================================\n",
            "Total params: 94,893\n",
            "Trainable params: 94,893\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LZbvqHB5E3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc18634-71a0-4a2b-aff2-01fbdf9930f3"
      },
      "source": [
        "!cp /content/my_model.h5 /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: failed to access '/content/drive/My Drive/': Transport endpoint is not connected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBxRkkEHVwU1"
      },
      "source": [
        "# FTP 전송"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1v3JlaOWKAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762e78ea-c5c2-4706-ef40-dc8a62b3e09b"
      },
      "source": [
        "from ftplib import FTP\n",
        "\n",
        "ftp = FTP('112.175.184.78')\n",
        "ftp.login('dalasjoe', 'Dalasjoe75!')\n",
        "\n",
        "# ftp.cwd('html') # \"test\"디렉터리로 이동\n",
        "# ftp.retrlines('LIST') # 디렉터리의 내용을 목록화\n",
        "# #ftp.retrbinary('RETR README', open('README', 'wb').write) # README 파일 저장\n",
        "# ftp.quit()\n",
        "\n",
        "\n",
        "ftp.cwd('html')  # 업로드할 FTP 폴더로 이동\n",
        "myfile = open(path.join(lotto_base_dir, \"predict.txt\"),'rb')  # 로컬 파일 열기\n",
        "ftp.storbinary('STOR ' + 'predict.txt', myfile )  # 파일을 FTP로 업로드\n",
        "myfile.close()  # 파일 닫기\n",
        "\n",
        "print (\"File Saved\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVn3rfIyW7OG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSlY9i6yW511"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
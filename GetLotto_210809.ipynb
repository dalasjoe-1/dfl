{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "GetLotto.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Q8MptZ7cjH8T"
      ],
      "authorship_tag": "ABX9TyM4tguC4nFR1Pnm3JM2QbZZ",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/dalasjoe-1/dfl/blob/master/GetLotto_210809.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0kCmxwN5Rkt1"
      },
      "source": [
        "# Another Way"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vl8KqROtQ5oG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f42c3241-55c4-45dc-b2f1-93fd5b3c733b"
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import datetime\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "tf.compat.v1.reset_default_graph()\n",
        "\n",
        "tf.compat.v1.disable_eager_execution()\n",
        "\n",
        "tf.compat.v1.set_random_seed(777)\n",
        "\n",
        "input_data_column_cnt = 45  # 입력데이터의 컬럼 개수(Variable 개수)\n",
        "#output_data_column_cnt = 1  # 결과데이터의 컬럼 개수\n",
        "output_data_column_cnt = 45  # 결과데이터의 컬럼 개수\n",
        "\n",
        "seq_length = 46 #192 #24 #96  # 1개 시퀀스의 길이(시계열데이터 입력 개수)\n",
        "rnn_cell_hidden_dim = 20  # 각 셀의 (hidden)출력 크기\n",
        "forget_bias = 1.0  # 망각편향(기본값 1.0)\n",
        "num_stacked_layers = 1  # stacked LSTM layers 개수\n",
        "keep_prob = 1.0  # dropout할 때 keep할 비율\n",
        "\n",
        "epoch_num = 10000 #20000  # 에폭 횟수(학습용전체데이터를 몇 회 반복해서 학습할 것인가 입력)\n",
        "learning_rate = 0.01  # 학습률\n",
        "\n",
        "# Standardization\n",
        "def data_standardization(x):\n",
        "    x_np = np.asarray(x)\n",
        "    return (x_np - x_np.mean()) / x_np.std()\n",
        "\n",
        "# 너무 작거나 너무 큰 값이 학습을 방해하는 것을 방지하고자 정규화한다\n",
        "# x가 양수라는 가정하에 최소값과 최대값을 이용하여 0~1사이의 값으로 변환\n",
        "# Min-Max scaling\n",
        "def min_max_scaling(x):\n",
        "    x_np = np.asarray(x)\n",
        "    return (x_np - x_np.min()) / (x_np.max() - x_np.min() + 1e-7)  # 1e-7은 0으로 나누는 오류 예방차원\n",
        "\n",
        "# 정규화된 값을 원래의 값으로 되돌린다\n",
        "# 정규화하기 이전의 org_x값과 되돌리고 싶은 x를 입력하면 역정규화된 값을 리턴한다\n",
        "def reverse_min_max_scaling(org_x, x):\n",
        "    org_x_np = np.asarray(org_x)\n",
        "    x_np = np.asarray(x)\n",
        "    return (x_np * (org_x_np.max() - org_x_np.min() + 1e-7)) + org_x_np.min()\n",
        "\n",
        "# data\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "#input = np.loadtxt('./gdrive/MyDrive/ori.data', unpack=True, dtype='int')\n",
        "input = np.loadtxt(\"./gdrive/MyDrive/lotto_data.txt\", delimiter=\",\", dtype='int')\n",
        "#data = np.transpose(input)\n",
        "data = input\n",
        "#win_numbers = data[:,1:data.size]\n",
        "win_numbers = data[:,1:7]\n",
        "\n",
        "#print(win_numbers, \" : \", win_numbers.size)\n",
        "norm_win_numbers = min_max_scaling(win_numbers)  # 가격형태 데이터 정규화 처리\n",
        "\n",
        "#x = min_max_scaling(win_numbers);\n",
        "x = win_numbers\n",
        "y = x[:, :]\n",
        "z = []\n",
        "\n",
        "for i in range(len(data)):\n",
        "    week = win_numbers[i]  \n",
        "    buckets = [0, 0, 0, 0, 0, 0, 0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0,0,0,0,0,\n",
        "        0,0,0]\n",
        "    for j in range(len(week)):\n",
        "        k = week[j]\n",
        "        buckets[k-1] = 100\n",
        "    z.append(buckets)\n",
        "\n",
        "x = z;\n",
        "y = x;\n",
        "\n",
        "dataX = []  # 입력으로 사용될 Sequence Data\n",
        "dataY = []  # 출력(타켓)으로 사용\n",
        "\n",
        "for i in range(0, len(y) - seq_length):\n",
        "    _x = x[i: i + seq_length]\n",
        "    _y = y[i + seq_length]  # 다음 나타날 주가(정답)\n",
        "    # if i is 0:\n",
        "    #      print(\">>> \", _x, \"->\", _y)  # 첫번째 행만 출력해 봄\n",
        "\n",
        "    dataX.append(_x)  # dataX 리스트에 추가\n",
        "    dataY.append(_y)  # dataY 리스트에 추가\n",
        "\n",
        "# 학습용/테스트용 데이터 생성\n",
        "# 전체 70%를 학습용 데이터로 사용\n",
        "train_size = int(len(dataY) * 0.7)\n",
        "# 나머지(30%)를 테스트용 데이터로 사용\n",
        "test_size = len(dataY) - train_size\n",
        "\n",
        "\n",
        "# 데이터를 잘라 학습용 데이터 생성\n",
        "trainX = np.asarray(dataX[0:train_size])\n",
        "trainY = np.asarray(dataY[0:train_size])\n",
        "\n",
        "# for i in range(len(trainX)):\n",
        "#     aaa = trainX[i]\n",
        "#     if (i==0):\n",
        "\n",
        "print(trainX.shape, \" ==== \", trainY.shape)\n",
        "# print(trainX[0], \" **** \", dataX[0])\n",
        "\n",
        "\n",
        "# 데이터를 잘라 테스트용 데이터 생성\n",
        "testX = np.array(dataX[train_size:len(dataX)])\n",
        "testY = np.array(dataY[train_size:len(dataY)])\n",
        "\n",
        "# 텐서플로우 플레이스홀더 생성\n",
        "# 입력 X, 출력 Y를 생성한다\n",
        "X = tf.compat.v1.placeholder(tf.float32, [None, seq_length, input_data_column_cnt])\n",
        "print(\"X: \", X)\n",
        "Y = tf.compat.v1.placeholder(tf.float32, [None, output_data_column_cnt])\n",
        "print(\"Y: \", Y)\n",
        "\n",
        "# 검증용 측정지표를 산출하기 위한 targets, predictions를 생성한다\n",
        "targets = tf.compat.v1.placeholder(tf.float32, [None, output_data_column_cnt])\n",
        "print(\"targets: \", targets)\n",
        "\n",
        "predictions = tf.compat.v1.placeholder(tf.float32, [None, output_data_column_cnt])\n",
        "print(\"predictions: \", predictions)\n",
        "\n",
        "def lstm_cell():\n",
        "    # LSTM셀을 생성\n",
        "    # num_units: 각 Cell 출력 크기\n",
        "    # forget_bias:  to the biases of the forget gate\n",
        "    #              (default: 1)  in order to reduce the scale of forgetting in the beginning of the training.\n",
        "    # state_is_tuple: True ==> accepted and returned states are 2-tuples of the c_state and m_state.\n",
        "    # state_is_tuple: False ==> they are concatenated along the column axis.\n",
        "\n",
        "    cell = tf.compat.v1.nn.rnn_cell.LSTMCell(num_units=rnn_cell_hidden_dim,\n",
        "                                        forget_bias=forget_bias, state_is_tuple=True, activation=tf.nn.softsign)\n",
        "   \n",
        "    # cell = tf.contrib.rnn.BasicLSTMCell(num_units=rnn_cell_hidden_dim,\n",
        "    #                                     forget_bias=forget_bias, state_is_tuple=True, activation=tf.nn.softsign)\n",
        "    #cell = tf.keras.layers.LSTMCell(num_units=rnn_cell_hidden_dim, forget_bias=forget_bias\n",
        "    #                                 , state_is_tuple=True, activation=tf.nn.softsign)\n",
        "    if keep_prob < 1.0:\n",
        "        #cell = tf.contrib.rnn.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
        "        cell = tf.compat.v1.nn.rnn_cell.DropoutWrapper(cell, output_keep_prob=keep_prob)\n",
        "    return cell\n",
        "\n",
        "\n",
        "# num_stacked_layers개의 층으로 쌓인 Stacked RNNs 생성\n",
        "stackedRNNs = [lstm_cell() for _ in range(num_stacked_layers)]\n",
        "multi_cells = tf.contrib.rnn.MultiRNNCell(stackedRNNs, state_is_tuple=True) if num_stacked_layers > 1 else lstm_cell()\n",
        "\n",
        "hypothesis, _states = tf.compat.v1.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32)\n",
        "#hypothesis, _states = tf.nn.dynamic_rnn(multi_cells, X, dtype=tf.float32)\n",
        "print(\"hypothesis: \", hypothesis)\n",
        "\n",
        "# [:, -1]를 잘 살펴보자. LSTM RNN의 마지막 (hidden)출력만을 사용했다.\n",
        "# 과거 여러 거래일의 주가를 이용해서 다음날의 주가 1개를 예측하기때문에 MANY-TO-ONE형태이다\n",
        "#hypothesis = tf.contrib.layers.fully_connected(hypothesis[:, -1], output_data_column_cnt, activation_fn=tf.identity)\n",
        "hypothesis = tf.compat.v1.layers.dense(hypothesis[:, -1], output_data_column_cnt, activation = tf.identity)\n",
        "\n",
        "\n",
        "\n",
        "# 손실함수로 평균제곱오차를 사용한다\n",
        "loss = tf.reduce_sum(tf.square(hypothesis - Y))\n",
        "# 최적화함수로 AdamOptimizer를 사용한다\n",
        "optimizer = tf.compat.v1.train.AdamOptimizer(learning_rate)\n",
        "# optimizer = tf.train.RMSPropOptimizer(learning_rate) # LSTM과 궁합 별로임\n",
        "\n",
        "train = optimizer.minimize(loss)\n",
        "\n",
        "# RMSE(Root Mean Square Error)\n",
        "# 제곱오차의 평균을 구하고 다시 제곱근을 구하면 평균 오차가 나온다\n",
        "# rmse = tf.sqrt(tf.reduce_mean(tf.square(targets-predictions))) # 아래 코드와 같다\n",
        "rmse = tf.sqrt(tf.reduce_mean(tf.math.squared_difference(targets, predictions)))\n",
        "\n",
        "train_error_summary = []  # 학습용 데이터의 오류를 중간 중간 기록한다\n",
        "test_error_summary = []  # 테스트용 데이터의 오류를 중간 중간 기록한다\n",
        "test_predict = ''  # 테스트용데이터로 예측한 결과\n",
        "\n",
        "sess = tf.compat.v1.Session()\n",
        "sess.run(tf.compat.v1.global_variables_initializer())\n",
        "\n",
        "print(\"trainX.shape=\", trainX.shape, \"trainY.shape=\", trainY.shape)\n",
        "print(\"trainX=\", trainX)\n",
        "\n",
        "# 학습한다\n",
        "start_time = datetime.datetime.now()  # 시작시간을 기록한다\n",
        "print('학습을 시작합니다...')\n",
        "for epoch in range(epoch_num):\n",
        "    _, _loss = sess.run([train, loss], feed_dict={X: trainX, Y: trainY})\n",
        "    if ((epoch + 1) % 100 == 0) or (epoch == epoch_num - 1):  # 100번째마다 또는 마지막 epoch인 경우\n",
        "        # 학습용데이터로 rmse오차를 구한다\n",
        "        train_predict = sess.run(hypothesis, feed_dict={X: trainX})\n",
        "        train_error = sess.run(rmse, feed_dict={targets: trainY, predictions: train_predict})\n",
        "        train_error_summary.append(train_error)\n",
        "\n",
        "        # 테스트용데이터로 rmse오차를 구한다\n",
        "        test_predict = sess.run(hypothesis, feed_dict={X: testX})\n",
        "        test_error = sess.run(rmse, feed_dict={targets: testY, predictions: test_predict})\n",
        "        test_error_summary.append(test_error)\n",
        "\n",
        "        # 현재 오류를 출력한다\n",
        "        print(\"epoch: {}, train_error(A): {}, test_error(B): {}, B-A: {}\".format(epoch + 1, train_error, test_error,\n",
        "                                                                                 test_error - train_error))\n",
        "\n",
        "# end_time = datetime.datetime.now()  # 종료시간을 기록한다\n",
        "# elapsed_time = end_time - start_time  # 경과시간을 구한다\n",
        "# print('elapsed_time:', elapsed_time)\n",
        "# print('elapsed_time per epoch:', elapsed_time / epoch_num)\n",
        "#\n",
        "# # 하이퍼파라미터 출력\n",
        "# print('input_data_column_cnt:', input_data_column_cnt, end='')\n",
        "# print(',output_data_column_cnt:', output_data_column_cnt, end='')\n",
        "#\n",
        "# print(',seq_length:', seq_length, end='')\n",
        "# print(',rnn_cell_hidden_dim:', rnn_cell_hidden_dim, end='')\n",
        "# print(',forget_bias:', forget_bias, end='')\n",
        "# print(',num_stacked_layers:', num_stacked_layers, end='')\n",
        "# print(',keep_prob:', keep_prob, end='')\n",
        "#\n",
        "# print(',epoch_num:', epoch_num, end='')\n",
        "# print(',learning_rate:', learning_rate, end='')\n",
        "#\n",
        "# print(',train_error:', train_error_summary[-1], end='')\n",
        "# print(',test_error:', test_error_summary[-1], end='')\n",
        "# print(',min_test_error:', np.min(test_error_summary))\n",
        "#\n",
        "# # 결과 그래프 출력\n",
        "# plt.figure(1)\n",
        "# plt.plot(train_error_summary, 'gold')\n",
        "# plt.plot(test_error_summary, 'b')\n",
        "# plt.xlabel('Epoch(x100)')\n",
        "# plt.ylabel('Root Mean Square Error')\n",
        "#\n",
        "# plt.figure(2)\n",
        "# plt.plot(testY, 'r')\n",
        "# plt.plot(test_predict, 'b')\n",
        "# plt.xlabel('Time Period')\n",
        "# plt.ylabel('Stock Price')\n",
        "# plt.show()\n",
        "\n",
        "# sequence length만큼의 가장 최근 데이터를 슬라이싱한다\n",
        "recent_data = np.array([x[len(x) - seq_length:]])\n",
        "print(\"recent_data.shape:\", recent_data.shape)\n",
        "print(\"recent_data:\", recent_data)\n",
        "\n",
        "# 내일 종가를 예측해본다\n",
        "test_predict = sess.run(hypothesis, feed_dict={X: recent_data})\n",
        "\n",
        "test_predict_2 = []\n",
        "\n",
        "for i in range(len(test_predict[0])):\n",
        "    test_predict[0][i] = int(test_predict[0][i])\n",
        "    test_predict_2.append((test_predict[0][i], i))\n",
        "\n",
        "#final_predict = np.sort(test_predict_2, axis=1)[::-1]\n",
        "print(\"test_predict_2  00000 =\", test_predict_2)\n",
        "test_predict_2.sort(key = lambda element : element[0], reverse=True)\n",
        "\n",
        "\n",
        "# predict_six = []\n",
        "#\n",
        "# for i in final_predict:\n",
        "#     for j in test_predict[0]:\n",
        "#         if (final_predict[i] == test_predict[0][j])\n",
        "\n",
        "\n",
        "#print(\"test_predict 0 \", test_predict[0])\n",
        "print(\"test_predict_2  11111\", test_predict_2)\n",
        "\n",
        "picked = []\n",
        "for i in range(6):\n",
        "    picked.append(test_predict_2[i][1] + 1)\n",
        "\n",
        "print(picked)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "(692, 46, 45)  ====  (692, 45)\n",
            "X:  Tensor(\"Placeholder:0\", shape=(None, 46, 45), dtype=float32)\n",
            "Y:  Tensor(\"Placeholder_1:0\", shape=(None, 45), dtype=float32)\n",
            "targets:  Tensor(\"Placeholder_2:0\", shape=(None, 45), dtype=float32)\n",
            "predictions:  Tensor(\"Placeholder_3:0\", shape=(None, 45), dtype=float32)\n",
            "WARNING:tensorflow:From <ipython-input-9-8a354922dc42>:140: LSTMCell.__init__ (from tensorflow.python.ops.rnn_cell_impl) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "This class is equivalent as tf.keras.layers.LSTMCell, and will be replaced by that in Tensorflow 2.0.\n",
            "WARNING:tensorflow:From <ipython-input-9-8a354922dc42>:156: dynamic_rnn (from tensorflow.python.ops.rnn) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `keras.layers.RNN(cell)`, which is equivalent to this API\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:958: Layer.add_variable (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.add_weight` method instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/resource_variable_ops.py:1630: calling BaseResourceVariable.__init__ (from tensorflow.python.ops.resource_variable_ops) with constraint is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "If using Keras pass *_constraint arguments to layers.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/ops/rnn_cell_impl.py:962: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
            "hypothesis:  Tensor(\"rnn/transpose_1:0\", shape=(None, 46, 20), dtype=float32)\n",
            "WARNING:tensorflow:From <ipython-input-9-8a354922dc42>:163: dense (from tensorflow.python.layers.core) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Use keras.layers.Dense instead.\n",
            "WARNING:tensorflow:From /usr/local/lib/python3.7/dist-packages/tensorflow_core/python/layers/core.py:187: Layer.apply (from tensorflow.python.keras.engine.base_layer) is deprecated and will be removed in a future version.\n",
            "Instructions for updating:\n",
            "Please use `layer.__call__` method instead.\n",
            "trainX.shape= (692, 46, 45) trainY.shape= (692, 45)\n",
            "trainX= [[[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [  0   0 100 ...   0   0 100]\n",
            "  [100   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [100   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0 100]]\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0 100]\n",
            "  [  0   0   0 ...   0   0   0]]\n",
            "\n",
            " ...\n",
            "\n",
            " [[  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ... 100   0 100]\n",
            "  [  0   0 100 ...   0   0   0]\n",
            "  ...\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0 100]\n",
            "  [  0   0   0 ...   0   0   0]]\n",
            "\n",
            " [[  0   0   0 ... 100   0 100]\n",
            "  [  0   0 100 ...   0   0   0]\n",
            "  [100   0   0 ...   0   0   0]\n",
            "  ...\n",
            "  [  0   0   0 ...   0   0 100]\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0 100   0 ...   0   0   0]]\n",
            "\n",
            " [[  0   0 100 ...   0   0   0]\n",
            "  [100   0   0 ...   0   0   0]\n",
            "  [  0   0   0 ... 100   0   0]\n",
            "  ...\n",
            "  [  0   0   0 ...   0   0   0]\n",
            "  [  0 100   0 ...   0   0   0]\n",
            "  [  0   0   0 ...   0   0   0]]]\n",
            "학습을 시작합니다...\n",
            "epoch: 100, train_error(A): 33.92649459838867, test_error(B): 34.02009582519531, B-A: 0.09360122680664062\n",
            "epoch: 200, train_error(A): 33.87558364868164, test_error(B): 34.04808044433594, B-A: 0.17249679565429688\n",
            "epoch: 300, train_error(A): 33.83763122558594, test_error(B): 34.06380081176758, B-A: 0.22616958618164062\n",
            "epoch: 400, train_error(A): 33.80518341064453, test_error(B): 34.0809326171875, B-A: 0.27574920654296875\n",
            "epoch: 500, train_error(A): 33.78272247314453, test_error(B): 34.10517120361328, B-A: 0.32244873046875\n",
            "epoch: 600, train_error(A): 33.763648986816406, test_error(B): 34.139827728271484, B-A: 0.3761787414550781\n",
            "epoch: 700, train_error(A): 33.747032165527344, test_error(B): 34.16126251220703, B-A: 0.4142303466796875\n",
            "epoch: 800, train_error(A): 33.735233306884766, test_error(B): 34.160343170166016, B-A: 0.42510986328125\n",
            "epoch: 900, train_error(A): 33.72495651245117, test_error(B): 34.190216064453125, B-A: 0.4652595520019531\n",
            "epoch: 1000, train_error(A): 33.715816497802734, test_error(B): 34.185855865478516, B-A: 0.47003936767578125\n",
            "epoch: 1100, train_error(A): 33.708290100097656, test_error(B): 34.21406555175781, B-A: 0.5057754516601562\n",
            "epoch: 1200, train_error(A): 33.70246124267578, test_error(B): 34.23341369628906, B-A: 0.5309524536132812\n",
            "epoch: 1300, train_error(A): 33.6966552734375, test_error(B): 34.242610931396484, B-A: 0.5459556579589844\n",
            "epoch: 1400, train_error(A): 33.690345764160156, test_error(B): 34.211917877197266, B-A: 0.5215721130371094\n",
            "epoch: 1500, train_error(A): 33.68950271606445, test_error(B): 34.20061111450195, B-A: 0.5111083984375\n",
            "epoch: 1600, train_error(A): 33.68531799316406, test_error(B): 34.20864486694336, B-A: 0.5233268737792969\n",
            "epoch: 1700, train_error(A): 33.68241882324219, test_error(B): 34.22502899169922, B-A: 0.5426101684570312\n",
            "epoch: 1800, train_error(A): 33.67937469482422, test_error(B): 34.240726470947266, B-A: 0.5613517761230469\n",
            "epoch: 1900, train_error(A): 33.67827606201172, test_error(B): 34.241050720214844, B-A: 0.562774658203125\n",
            "epoch: 2000, train_error(A): 33.67462158203125, test_error(B): 34.251827239990234, B-A: 0.5772056579589844\n",
            "epoch: 2100, train_error(A): 33.67395782470703, test_error(B): 34.25679016113281, B-A: 0.5828323364257812\n",
            "epoch: 2200, train_error(A): 33.66813659667969, test_error(B): 34.26345443725586, B-A: 0.5953178405761719\n",
            "epoch: 2300, train_error(A): 33.664859771728516, test_error(B): 34.263267517089844, B-A: 0.5984077453613281\n",
            "epoch: 2400, train_error(A): 33.66255187988281, test_error(B): 34.27467346191406, B-A: 0.61212158203125\n",
            "epoch: 2500, train_error(A): 33.661766052246094, test_error(B): 34.272987365722656, B-A: 0.6112213134765625\n",
            "epoch: 2600, train_error(A): 33.661617279052734, test_error(B): 34.28133010864258, B-A: 0.6197128295898438\n",
            "epoch: 2700, train_error(A): 33.65952682495117, test_error(B): 34.2805290222168, B-A: 0.621002197265625\n",
            "epoch: 2800, train_error(A): 33.65872573852539, test_error(B): 34.288089752197266, B-A: 0.629364013671875\n",
            "epoch: 2900, train_error(A): 33.658416748046875, test_error(B): 34.29523468017578, B-A: 0.6368179321289062\n",
            "epoch: 3000, train_error(A): 33.660396575927734, test_error(B): 34.298255920410156, B-A: 0.6378593444824219\n",
            "epoch: 3100, train_error(A): 33.6590690612793, test_error(B): 34.2893180847168, B-A: 0.6302490234375\n",
            "epoch: 3200, train_error(A): 33.6134147644043, test_error(B): 34.31073760986328, B-A: 0.6973228454589844\n",
            "epoch: 3300, train_error(A): 33.57807540893555, test_error(B): 34.3378791809082, B-A: 0.7598037719726562\n",
            "epoch: 3400, train_error(A): 33.55215072631836, test_error(B): 34.36897659301758, B-A: 0.8168258666992188\n",
            "epoch: 3500, train_error(A): 33.54224395751953, test_error(B): 34.426048278808594, B-A: 0.8838043212890625\n",
            "epoch: 3600, train_error(A): 33.53474426269531, test_error(B): 34.455196380615234, B-A: 0.9204521179199219\n",
            "epoch: 3700, train_error(A): 33.52506637573242, test_error(B): 34.47394561767578, B-A: 0.9488792419433594\n",
            "epoch: 3800, train_error(A): 33.50112533569336, test_error(B): 34.47531509399414, B-A: 0.9741897583007812\n",
            "epoch: 3900, train_error(A): 33.477333068847656, test_error(B): 34.4505500793457, B-A: 0.9732170104980469\n",
            "epoch: 4000, train_error(A): 33.4736213684082, test_error(B): 34.444679260253906, B-A: 0.9710578918457031\n",
            "epoch: 4100, train_error(A): 33.46318435668945, test_error(B): 34.47880935668945, B-A: 1.015625\n",
            "epoch: 4200, train_error(A): 33.455482482910156, test_error(B): 34.509552001953125, B-A: 1.0540695190429688\n",
            "epoch: 4300, train_error(A): 33.4360237121582, test_error(B): 34.49854278564453, B-A: 1.0625190734863281\n",
            "epoch: 4400, train_error(A): 33.42111587524414, test_error(B): 34.59591293334961, B-A: 1.1747970581054688\n",
            "epoch: 4500, train_error(A): 33.39759826660156, test_error(B): 34.5819206237793, B-A: 1.1843223571777344\n",
            "epoch: 4600, train_error(A): 33.38753128051758, test_error(B): 34.625179290771484, B-A: 1.2376480102539062\n",
            "epoch: 4700, train_error(A): 33.37458038330078, test_error(B): 34.603946685791016, B-A: 1.2293663024902344\n",
            "epoch: 4800, train_error(A): 33.33850860595703, test_error(B): 34.62197494506836, B-A: 1.2834663391113281\n",
            "epoch: 4900, train_error(A): 33.292842864990234, test_error(B): 34.69681930541992, B-A: 1.4039764404296875\n",
            "epoch: 5000, train_error(A): 33.2172966003418, test_error(B): 34.72632598876953, B-A: 1.5090293884277344\n",
            "epoch: 5100, train_error(A): 33.149681091308594, test_error(B): 34.83129119873047, B-A: 1.681610107421875\n",
            "epoch: 5200, train_error(A): 33.04762268066406, test_error(B): 35.005027770996094, B-A: 1.9574050903320312\n",
            "epoch: 5300, train_error(A): 32.98582458496094, test_error(B): 35.0673942565918, B-A: 2.0815696716308594\n",
            "epoch: 5400, train_error(A): 32.92775344848633, test_error(B): 35.21150207519531, B-A: 2.2837486267089844\n",
            "epoch: 5500, train_error(A): 32.874183654785156, test_error(B): 35.407493591308594, B-A: 2.5333099365234375\n",
            "epoch: 5600, train_error(A): 32.812530517578125, test_error(B): 35.624393463134766, B-A: 2.8118629455566406\n",
            "epoch: 5700, train_error(A): 32.7452392578125, test_error(B): 35.74489212036133, B-A: 2.999652862548828\n",
            "epoch: 5800, train_error(A): 32.69390869140625, test_error(B): 35.87582015991211, B-A: 3.1819114685058594\n",
            "epoch: 5900, train_error(A): 32.66411590576172, test_error(B): 35.93904495239258, B-A: 3.2749290466308594\n",
            "epoch: 6000, train_error(A): 32.608612060546875, test_error(B): 36.16383743286133, B-A: 3.555225372314453\n",
            "epoch: 6100, train_error(A): 32.59191131591797, test_error(B): 36.22573471069336, B-A: 3.6338233947753906\n",
            "epoch: 6200, train_error(A): 32.53476333618164, test_error(B): 36.27021408081055, B-A: 3.7354507446289062\n",
            "epoch: 6300, train_error(A): 32.54203796386719, test_error(B): 36.258182525634766, B-A: 3.716144561767578\n",
            "epoch: 6400, train_error(A): 32.536617279052734, test_error(B): 36.4818229675293, B-A: 3.9452056884765625\n",
            "epoch: 6500, train_error(A): 32.449642181396484, test_error(B): 36.57208251953125, B-A: 4.122440338134766\n",
            "epoch: 6600, train_error(A): 32.46346664428711, test_error(B): 36.69657897949219, B-A: 4.233112335205078\n",
            "epoch: 6700, train_error(A): 32.42195129394531, test_error(B): 36.67445373535156, B-A: 4.25250244140625\n",
            "epoch: 6800, train_error(A): 32.339744567871094, test_error(B): 36.923702239990234, B-A: 4.583957672119141\n",
            "epoch: 6900, train_error(A): 32.33922576904297, test_error(B): 37.02737808227539, B-A: 4.688152313232422\n",
            "epoch: 7000, train_error(A): 32.31300354003906, test_error(B): 36.98488998413086, B-A: 4.671886444091797\n",
            "epoch: 7100, train_error(A): 32.301795959472656, test_error(B): 37.05482482910156, B-A: 4.753028869628906\n",
            "epoch: 7200, train_error(A): 32.321048736572266, test_error(B): 37.21894454956055, B-A: 4.897895812988281\n",
            "epoch: 7300, train_error(A): 32.243045806884766, test_error(B): 37.31550979614258, B-A: 5.0724639892578125\n",
            "epoch: 7400, train_error(A): 32.27922439575195, test_error(B): 37.18925094604492, B-A: 4.910026550292969\n",
            "epoch: 7500, train_error(A): 32.23805618286133, test_error(B): 37.25875473022461, B-A: 5.020698547363281\n",
            "epoch: 7600, train_error(A): 32.244178771972656, test_error(B): 37.575469970703125, B-A: 5.331291198730469\n",
            "epoch: 7700, train_error(A): 32.15237808227539, test_error(B): 37.700618743896484, B-A: 5.548240661621094\n",
            "epoch: 7800, train_error(A): 32.158042907714844, test_error(B): 37.77494430541992, B-A: 5.616901397705078\n",
            "epoch: 7900, train_error(A): 32.220863342285156, test_error(B): 37.676151275634766, B-A: 5.455287933349609\n",
            "epoch: 8000, train_error(A): 32.17165756225586, test_error(B): 37.67656707763672, B-A: 5.504909515380859\n",
            "epoch: 8100, train_error(A): 32.08831024169922, test_error(B): 37.686126708984375, B-A: 5.597816467285156\n",
            "epoch: 8200, train_error(A): 32.300987243652344, test_error(B): 37.50722122192383, B-A: 5.206233978271484\n",
            "epoch: 8300, train_error(A): 32.189842224121094, test_error(B): 37.74000930786133, B-A: 5.550167083740234\n",
            "epoch: 8400, train_error(A): 32.15919494628906, test_error(B): 37.81892013549805, B-A: 5.659725189208984\n",
            "epoch: 8500, train_error(A): 32.194580078125, test_error(B): 37.9361686706543, B-A: 5.741588592529297\n",
            "epoch: 8600, train_error(A): 32.17897033691406, test_error(B): 37.93169021606445, B-A: 5.752719879150391\n",
            "epoch: 8700, train_error(A): 32.127891540527344, test_error(B): 38.004249572753906, B-A: 5.8763580322265625\n",
            "epoch: 8800, train_error(A): 32.290611267089844, test_error(B): 38.04822540283203, B-A: 5.7576141357421875\n",
            "epoch: 8900, train_error(A): 32.179908752441406, test_error(B): 37.85918426513672, B-A: 5.6792755126953125\n",
            "epoch: 9000, train_error(A): 32.18666458129883, test_error(B): 37.7823371887207, B-A: 5.595672607421875\n",
            "epoch: 9100, train_error(A): 32.1684455871582, test_error(B): 37.79997634887695, B-A: 5.63153076171875\n",
            "epoch: 9200, train_error(A): 32.135108947753906, test_error(B): 37.892578125, B-A: 5.757469177246094\n",
            "epoch: 9300, train_error(A): 32.11695098876953, test_error(B): 38.035369873046875, B-A: 5.918418884277344\n",
            "epoch: 9400, train_error(A): 32.055747985839844, test_error(B): 38.10743713378906, B-A: 6.051689147949219\n",
            "epoch: 9500, train_error(A): 32.08016586303711, test_error(B): 38.06151580810547, B-A: 5.981349945068359\n",
            "epoch: 9600, train_error(A): 32.07356643676758, test_error(B): 38.19338607788086, B-A: 6.119819641113281\n",
            "epoch: 9700, train_error(A): 32.22389602661133, test_error(B): 38.19301986694336, B-A: 5.969123840332031\n",
            "epoch: 9800, train_error(A): 32.09782791137695, test_error(B): 38.032501220703125, B-A: 5.934673309326172\n",
            "epoch: 9900, train_error(A): 32.07244873046875, test_error(B): 38.15367126464844, B-A: 6.0812225341796875\n",
            "epoch: 10000, train_error(A): 32.02252197265625, test_error(B): 38.239410400390625, B-A: 6.216888427734375\n",
            "recent_data.shape: (1, 46, 45)\n",
            "recent_data: [[[  0   0 100 ...   0   0   0]\n",
            "  [  0   0 100 ...   0   0   0]\n",
            "  [  0   0   0 ... 100   0 100]\n",
            "  ...\n",
            "  [  0   0   0 ...   0 100   0]\n",
            "  [  0   0   0 ...   0 100 100]\n",
            "  [  0   0   0 ...   0   0   0]]]\n",
            "test_predict_2  00000 = [(-31.0, 0), (80.0, 1), (-33.0, 2), (19.0, 3), (66.0, 4), (29.0, 5), (17.0, 6), (81.0, 7), (13.0, 8), (0.0, 9), (16.0, 10), (-44.0, 11), (19.0, 12), (5.0, 13), (-6.0, 14), (8.0, 15), (-5.0, 16), (-26.0, 17), (12.0, 18), (-35.0, 19), (28.0, 20), (12.0, 21), (25.0, 22), (25.0, 23), (69.0, 24), (-55.0, 25), (2.0, 26), (46.0, 27), (-42.0, 28), (46.0, 29), (-11.0, 30), (-25.0, 31), (-7.0, 32), (7.0, 33), (17.0, 34), (-2.0, 35), (84.0, 36), (74.0, 37), (48.0, 38), (-26.0, 39), (44.0, 40), (0.0, 41), (-19.0, 42), (7.0, 43), (8.0, 44)]\n",
            "test_predict_2  11111 [(84.0, 36), (81.0, 7), (80.0, 1), (74.0, 37), (69.0, 24), (66.0, 4), (48.0, 38), (46.0, 27), (46.0, 29), (44.0, 40), (29.0, 5), (28.0, 20), (25.0, 22), (25.0, 23), (19.0, 3), (19.0, 12), (17.0, 6), (17.0, 34), (16.0, 10), (13.0, 8), (12.0, 18), (12.0, 21), (8.0, 15), (8.0, 44), (7.0, 33), (7.0, 43), (5.0, 13), (2.0, 26), (0.0, 9), (0.0, 41), (-2.0, 35), (-5.0, 16), (-6.0, 14), (-7.0, 32), (-11.0, 30), (-19.0, 42), (-25.0, 31), (-26.0, 17), (-26.0, 39), (-31.0, 0), (-33.0, 2), (-35.0, 19), (-42.0, 28), (-44.0, 11), (-55.0, 25)]\n",
            "[37, 8, 2, 38, 25, 5]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O7T2b3Uw0n_u"
      },
      "source": [
        "# 내일 종가를 예측해본다\n",
        "test_predict = sess.run(hypothesis, feed_dict={X: recent_data})\n",
        "\n",
        "test_predict_2 = []\n",
        "\n",
        "for i in range(len(test_predict[0])):\n",
        "    test_predict[0][i] = int(test_predict[0][i])\n",
        "    test_predict_2.append((test_predict[0][i], i))\n",
        "\n",
        "#final_predict = np.sort(test_predict_2, axis=1)[::-1]\n",
        "print(\"test_predict_2  00000 =\", test_predict_2)\n",
        "test_predict_2.sort(key = lambda element : element[0], reverse=True)\n",
        "\n",
        "\n",
        "# predict_six = []\n",
        "#\n",
        "# for i in final_predict:\n",
        "#     for j in test_predict[0]:\n",
        "#         if (final_predict[i] == test_predict[0][j])\n",
        "\n",
        "\n",
        "#print(\"test_predict 0 \", test_predict[0])\n",
        "print(\"test_predict_2  11111\", test_predict_2)\n",
        "\n",
        "picked = []\n",
        "for i in range(6):\n",
        "    picked.append(test_predict_2[i][1] + 1)\n",
        "\n",
        "print(picked)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jP2-bRVzoZ9T"
      },
      "source": [
        "# ALL Together"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zzbFpN7ZokLu",
        "outputId": "db4961ab-6b39-43ee-bb7b-eff03d902b26",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#!pip install tensorflow-gpu==2.0.0-rc1\n",
        "!pip install tensorflow-gpu==2.0.1\n",
        "\n",
        "!pip install 'h5py==2.10.0' --force-reinstall\n",
        "\n",
        "main_url = \"https://www.dhlottery.co.kr/gameResult.do?method=byWin\" # 마지막 회차를 얻기 위한 주소\n",
        "basic_url = \"https://www.dhlottery.co.kr/gameResult.do?method=byWin&drwNo=\" # 임의의 회차를 얻기 위한 주소\n",
        "\n",
        "# 마지막 회차 정보를 가져옴\n",
        "def GetLast(): \n",
        "    resp = requests.get(main_url)\n",
        "    soup = BeautifulSoup(resp.text, \"lxml\")\n",
        "    result = str(soup.find(\"meta\", {\"id\" : \"desc\", \"name\" : \"description\"})['content'])\n",
        "    s_idx = result.find(\" \")\n",
        "    e_idx = result.find(\"회\")\n",
        "    return int(result[s_idx + 1 : e_idx])\n",
        "\n",
        "# 지정된 파일에 지정된 범위의 회차 정보를 기록함\n",
        "def Crawler(s_count, e_count, fp):\n",
        "    for i in range(s_count , e_count + 1):\n",
        "        crawler_url = basic_url + str(i)\n",
        "        resp = requests.get(crawler_url)\n",
        "        soup = BeautifulSoup(resp.text, \"html.parser\")\n",
        "\n",
        "        text = soup.text\n",
        "\n",
        "        s_idx = text.find(\" 당첨결과\")\n",
        "        s_idx = text.find(\"당첨번호\", s_idx) + 4\n",
        "        e_idx = text.find(\"보너스\", s_idx)\n",
        "        numbers = text[s_idx:e_idx].strip().split()\n",
        "\n",
        "        s_idx = e_idx + 3\n",
        "        e_idx = s_idx + 3\n",
        "        bonus = text[s_idx:e_idx].strip()\n",
        "\n",
        "        s_idx = text.find(\"1등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money1 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"2등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money2 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"3등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money3 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"4등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money4 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        s_idx = text.find(\"5등\", e_idx) + 2\n",
        "        e_idx = text.find(\"원\", s_idx) + 1\n",
        "        e_idx = text.find(\"원\", e_idx)\n",
        "        money5 = text[s_idx:e_idx].strip().replace(',','').split()[2]\n",
        "\n",
        "        line = str(i) + ',' + numbers[0] + ',' + numbers[1] + ',' + numbers[2] + ',' + numbers[3] + ',' + numbers[4] + ',' + numbers[5] + ',' + bonus + ',' + money1 + ',' + money2 + ',' + money3 + ',' + money4 + ',' + money5\n",
        "        print(line)\n",
        "        line += '\\n'\n",
        "        fp.write(line)\n",
        "\n",
        "\n",
        "# last = GetLast() # 마지막 회차를 가져옴\n",
        "\n",
        "# lotto_dir_name='lotto'\n",
        "# drive.mount('/content/gdrive',force_remount=True)\n",
        "# lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "# if not path.exists(lotto_base_dir):\n",
        "#   print('Check your google drive directory. See you file explorer')\n",
        "\n",
        "\n",
        "# print (\"Last=\",last)\n",
        "\n",
        "# # 완전히 다시 쓰기\n",
        "# #fp = open(path.join(lotto_base_dir, \"lotto_data.txt\"), 'w')\n",
        "\n",
        "# # 기존 데이타에 추가하기\n",
        "# fp = open(path.join(lotto_base_dir, \"lotto_data.txt\"), 'a')\n",
        "# Crawler(last-2, last, fp) # 처음부터 마지막 회차까지 저장\n",
        "# fp.close()\n",
        "\n",
        "#  --------------\n",
        "#  번호 저장 끝\n",
        "#  -------------\n",
        "\n",
        "# 당첨번호를 원핫인코딩벡터(ohbin)으로 변환\n",
        "def numbers2ohbin(numbers):\n",
        "\n",
        "    ohbin = np.zeros(45) #45개의 빈 칸을 만듬\n",
        "\n",
        "    for i in range(6): #여섯개의 당첨번호에 대해서 반복함\n",
        "        ohbin[int(numbers[i])-1] = 1 #로또번호가 1부터 시작하지만 벡터의 인덱스 시작은 0부터 시작하므로 1을 뺌\n",
        "    \n",
        "    return ohbin\n",
        "\n",
        "# 원핫인코딩벡터(ohbin)를 번호로 변환\n",
        "def ohbin2numbers(ohbin):\n",
        "\n",
        "    numbers = []\n",
        "    \n",
        "    for i in range(len(ohbin)):\n",
        "        if ohbin[i] == 1.0: # 1.0으로 설정되어 있으면 해당 번호를 반환값에 추가한다.\n",
        "            numbers.append(i+1)\n",
        "    \n",
        "    return numbers\n",
        "\n",
        "\n",
        "rows = np.loadtxt(\"./gdrive/MyDrive/lotto_data.txt\", delimiter=\",\")\n",
        "row_count = len(rows)\n",
        "\n",
        "numbers = rows[:, 1:7]\n",
        "ohbins = list(map(numbers2ohbin, numbers))\n",
        "\n",
        "x_samples = ohbins[0:row_count-1]\n",
        "y_samples = ohbins[1:row_count]\n",
        "\n",
        "#model = tf.keras.models.load_model('./gdrive/MyDrive/my_model.h5', compile=False)\n",
        "#model = tf.keras.models.load_model('./gdrive/MyDrive/my_model.h5')\n",
        "#model.summary()\n",
        "# 모델을 정의합니다.\n",
        "model = keras.Sequential([\n",
        "    keras.layers.LSTM(128, batch_input_shape=(1, 1, 45), return_sequences=False, stateful=True),\n",
        "    keras.layers.Dense(45, activation='sigmoid')\n",
        "])\n",
        "\n",
        "# 모델을 컴파일합니다.\n",
        "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
        "\n",
        "train_loss = []\n",
        "train_acc = []\n",
        "val_loss = []\n",
        "val_acc = []\n",
        "\n",
        "\n",
        "# 88회부터 지금까지 1등부터 5등까지 상금의 평균낸다.\n",
        "mean_prize = [  np.mean(rows[87:, 8]),\n",
        "            np.mean(rows[87:, 9]),\n",
        "            np.mean(rows[87:, 10]),\n",
        "            np.mean(rows[87:, 11]),\n",
        "            np.mean(rows[87:, 12])]\n",
        "\n",
        "train_total_reward = []\n",
        "train_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "val_total_reward = []\n",
        "val_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "test_total_reward = []\n",
        "test_total_grade = np.zeros(6, dtype=int)\n",
        "\n",
        "\n",
        "\n",
        "def gen_numbers_from_probability(nums_prob):\n",
        "\n",
        "    ball_box = []\n",
        "\n",
        "    for n in range(45):\n",
        "        ball_count = int(nums_prob[n] * 200 + 1)\n",
        "        ball = np.full((ball_count), n+1) #1부터 시작\n",
        "        ball_box += list(ball)\n",
        "\n",
        "    selected_balls = []\n",
        "\n",
        "    while True:\n",
        "        \n",
        "        if len(selected_balls) == 6:\n",
        "            break\n",
        "        \n",
        "        ball_index = np.random.randint(len(ball_box), size=1)[0]\n",
        "        ball = ball_box[ball_index]\n",
        "\n",
        "        if ball not in selected_balls:\n",
        "            selected_balls.append(ball)\n",
        "\n",
        "   \n",
        "    selected_balls.sort()\n",
        "\n",
        "    return selected_balls\n",
        "\n",
        "print('receive numbers')\n",
        "\n",
        "xs = x_samples[-1].reshape(1, 1, 45)\n",
        "\n",
        "ys_pred = model.predict_on_batch(xs)\n",
        "\n",
        "list_numbers = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "# 최대 100번 에포크까지 수행\n",
        "for epoch in range(50):\n",
        "\n",
        "    model.reset_states() # 중요! 매 에포크마다 1회부터 다시 훈련하므로 상태 초기화 필요\n",
        "\n",
        "    batch_train_loss = []\n",
        "    batch_train_acc = []\n",
        "\n",
        "    for i in range(len(x_samples)):\n",
        "        \n",
        "        xs = x_samples[i].reshape(1, 1, 45)\n",
        "        ys = y_samples[i].reshape(1, 45)\n",
        "        \n",
        "        loss, acc = model.train_on_batch(xs, ys) #배치만큼 모델에 학습시킴\n",
        "\n",
        "        batch_train_loss.append(loss)\n",
        "        batch_train_acc.append(acc)\n",
        "\n",
        "    train_loss.append(np.mean(batch_train_loss))\n",
        "    train_acc.append(np.mean(batch_train_acc))\n",
        "\n",
        "    print('epoch {0:4d} train acc {1:0.3f} loss {2:0.3f}'.format(epoch, np.mean(batch_train_acc), np.mean(batch_train_loss)))  \n",
        "\n",
        "\n",
        "# Trainning 끝나면 무조건 모델에 저장\n",
        "model.save('my_model.h5')\n",
        "!cp /content/my_model.h5 /content/drive/My\\ Drive/\n",
        "\n",
        "\n",
        "# 마지막 회차까지 학습한 모델로 다음 회차 추론\n",
        "\n",
        "# 50 개 뽑기\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "lotto_base_dir=path.join('./gdrive/My Drive/', '')\n",
        "with open(path.join(lotto_base_dir, \"predict2.txt\"), \"w\") as f:\n",
        "  print (\"predict.txt\")\n",
        "\n",
        "  for n in range(50):\n",
        "    numbers = gen_numbers_from_probability(ys_pred[0])  \n",
        "    print('{0} : {1}'.format(n, numbers))    \n",
        "    list_numbers.append(numbers)  \n",
        "    line_str=','.join(str(e) for e in numbers)\n",
        "    line_str += '\\n'\n",
        "    print(line_str)\n",
        "    f.write(line_str)\n",
        "f.close()\n",
        "\n",
        "from ftplib import FTP\n",
        "\n",
        "ftp = FTP('112.175.184.78')\n",
        "ftp.login('dalasjoe', 'Dalasjoe75!')\n",
        "\n",
        "# ftp.cwd('html') # \"test\"디렉터리로 이동\n",
        "# ftp.retrlines('LIST') # 디렉터리의 내용을 목록화\n",
        "# #ftp.retrbinary('RETR README', open('README', 'wb').write) # README 파일 저장\n",
        "# ftp.quit()\n",
        "\n",
        "\n",
        "ftp.cwd('html')  # 업로드할 FTP 폴더로 이동\n",
        "myfile = open(path.join(lotto_base_dir, \"predict2.txt\"),'rb')  # 로컬 파일 열기\n",
        "ftp.storbinary('STOR ' + 'predict2.txt', myfile )  # 파일을 FTP로 업로드\n",
        "myfile.close()  # 파일 닫기\n",
        "\n",
        "print (\"File Saved\")"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.1 in /usr/local/lib/python3.7/dist-packages (2.0.1)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.16.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (3.3.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.1.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.1.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.2.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (2.0.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (2.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.37.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.0.8)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.12.0)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (3.17.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.13.3)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.21.4)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.42.0)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.2.2)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.1) (2.10.0)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (1.35.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (0.4.6)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (2.23.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.3.6)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (57.4.0)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (1.0.1)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (0.2.8)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (4.2.4)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (4.8.2)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.10.0.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.6.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (0.4.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (2021.10.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.1.1)\n",
            "Collecting h5py==2.10.0\n",
            "  Using cached h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n",
            "Collecting numpy>=1.7\n",
            "  Using cached numpy-1.21.4-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (15.7 MB)\n",
            "Collecting six\n",
            "  Using cached six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Installing collected packages: six, numpy, h5py\n",
            "  Attempting uninstall: six\n",
            "    Found existing installation: six 1.16.0\n",
            "    Uninstalling six-1.16.0:\n",
            "      Successfully uninstalled six-1.16.0\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.21.4\n",
            "    Uninstalling numpy-1.21.4:\n",
            "      Successfully uninstalled numpy-1.21.4\n",
            "  Attempting uninstall: h5py\n",
            "    Found existing installation: h5py 2.10.0\n",
            "    Uninstalling h5py-2.10.0:\n",
            "      Successfully uninstalled h5py-2.10.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "yellowbrick 1.3.post1 requires numpy<1.20,>=1.16.0, but you have numpy 1.21.4 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorboard~=2.6, but you have tensorboard 2.0.2 which is incompatible.\n",
            "tensorflow 2.7.0 requires tensorflow-estimator<2.8,~=2.7.0rc0, but you have tensorflow-estimator 2.0.1 which is incompatible.\n",
            "tensorflow-probability 0.15.0 requires gast>=0.3.2, but you have gast 0.2.2 which is incompatible.\n",
            "google-colab 1.0.0 requires six~=1.15.0, but you have six 1.16.0 which is incompatible.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed h5py-2.10.0 numpy-1.21.4 six-1.16.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "h5py",
                  "numpy",
                  "six"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "receive numbers\n",
            "epoch    0 train acc 0.864 loss 0.405\n",
            "epoch    1 train acc 0.867 loss 0.396\n",
            "epoch    2 train acc 0.867 loss 0.393\n",
            "epoch    3 train acc 0.867 loss 0.391\n",
            "epoch    4 train acc 0.867 loss 0.388\n",
            "epoch    5 train acc 0.867 loss 0.383\n",
            "epoch    6 train acc 0.867 loss 0.377\n",
            "epoch    7 train acc 0.868 loss 0.372\n",
            "epoch    8 train acc 0.868 loss 0.365\n",
            "epoch    9 train acc 0.869 loss 0.359\n",
            "epoch   10 train acc 0.869 loss 0.352\n",
            "epoch   11 train acc 0.870 loss 0.344\n",
            "epoch   12 train acc 0.871 loss 0.335\n",
            "epoch   13 train acc 0.873 loss 0.326\n",
            "epoch   14 train acc 0.875 loss 0.316\n",
            "epoch   15 train acc 0.877 loss 0.305\n",
            "epoch   16 train acc 0.880 loss 0.295\n",
            "epoch   17 train acc 0.884 loss 0.284\n",
            "epoch   18 train acc 0.889 loss 0.273\n",
            "epoch   19 train acc 0.893 loss 0.262\n",
            "epoch   20 train acc 0.898 loss 0.251\n",
            "epoch   21 train acc 0.903 loss 0.239\n",
            "epoch   22 train acc 0.908 loss 0.229\n",
            "epoch   23 train acc 0.911 loss 0.221\n",
            "epoch   24 train acc 0.916 loss 0.211\n",
            "epoch   25 train acc 0.921 loss 0.201\n",
            "epoch   26 train acc 0.926 loss 0.192\n",
            "epoch   27 train acc 0.930 loss 0.185\n",
            "epoch   28 train acc 0.933 loss 0.178\n",
            "epoch   29 train acc 0.937 loss 0.171\n",
            "epoch   30 train acc 0.939 loss 0.164\n",
            "epoch   31 train acc 0.943 loss 0.156\n",
            "epoch   32 train acc 0.945 loss 0.151\n",
            "epoch   33 train acc 0.949 loss 0.147\n",
            "epoch   34 train acc 0.951 loss 0.141\n",
            "epoch   35 train acc 0.955 loss 0.134\n",
            "epoch   36 train acc 0.956 loss 0.131\n",
            "epoch   37 train acc 0.958 loss 0.126\n",
            "epoch   38 train acc 0.962 loss 0.118\n",
            "epoch   39 train acc 0.962 loss 0.116\n",
            "epoch   40 train acc 0.964 loss 0.114\n",
            "epoch   41 train acc 0.967 loss 0.106\n",
            "epoch   42 train acc 0.969 loss 0.101\n",
            "epoch   43 train acc 0.970 loss 0.098\n",
            "epoch   44 train acc 0.972 loss 0.095\n",
            "epoch   45 train acc 0.973 loss 0.092\n",
            "epoch   46 train acc 0.974 loss 0.088\n",
            "epoch   47 train acc 0.974 loss 0.087\n",
            "epoch   48 train acc 0.978 loss 0.078\n",
            "epoch   49 train acc 0.981 loss 0.074\n",
            "cp: cannot create regular file '/content/drive/My Drive/': No such file or directory\n",
            "Mounted at /content/gdrive\n",
            "predict.txt\n",
            "0 : [3, 11, 14, 15, 19, 30]\n",
            "3,11,14,15,19,30\n",
            "\n",
            "1 : [5, 12, 15, 18, 22, 23]\n",
            "5,12,15,18,22,23\n",
            "\n",
            "2 : [2, 4, 10, 12, 39, 44]\n",
            "2,4,10,12,39,44\n",
            "\n",
            "3 : [6, 8, 16, 23, 38, 43]\n",
            "6,8,16,23,38,43\n",
            "\n",
            "4 : [12, 15, 22, 26, 39, 45]\n",
            "12,15,22,26,39,45\n",
            "\n",
            "5 : [7, 8, 12, 15, 18, 32]\n",
            "7,8,12,15,18,32\n",
            "\n",
            "6 : [19, 22, 25, 34, 39, 44]\n",
            "19,22,25,34,39,44\n",
            "\n",
            "7 : [3, 15, 18, 21, 28, 29]\n",
            "3,15,18,21,28,29\n",
            "\n",
            "8 : [12, 29, 31, 34, 41, 42]\n",
            "12,29,31,34,41,42\n",
            "\n",
            "9 : [7, 24, 26, 27, 33, 39]\n",
            "7,24,26,27,33,39\n",
            "\n",
            "10 : [1, 6, 20, 32, 33, 39]\n",
            "1,6,20,32,33,39\n",
            "\n",
            "11 : [4, 5, 17, 20, 32, 40]\n",
            "4,5,17,20,32,40\n",
            "\n",
            "12 : [9, 12, 14, 21, 24, 41]\n",
            "9,12,14,21,24,41\n",
            "\n",
            "13 : [24, 28, 32, 38, 43, 45]\n",
            "24,28,32,38,43,45\n",
            "\n",
            "14 : [6, 7, 19, 27, 37, 40]\n",
            "6,7,19,27,37,40\n",
            "\n",
            "15 : [2, 4, 9, 12, 14, 37]\n",
            "2,4,9,12,14,37\n",
            "\n",
            "16 : [5, 8, 22, 24, 25, 41]\n",
            "5,8,22,24,25,41\n",
            "\n",
            "17 : [2, 6, 13, 17, 29, 33]\n",
            "2,6,13,17,29,33\n",
            "\n",
            "18 : [14, 19, 23, 28, 34, 37]\n",
            "14,19,23,28,34,37\n",
            "\n",
            "19 : [9, 12, 18, 19, 25, 45]\n",
            "9,12,18,19,25,45\n",
            "\n",
            "20 : [4, 25, 27, 30, 34, 36]\n",
            "4,25,27,30,34,36\n",
            "\n",
            "21 : [10, 13, 15, 24, 31, 34]\n",
            "10,13,15,24,31,34\n",
            "\n",
            "22 : [5, 23, 25, 40, 42, 44]\n",
            "5,23,25,40,42,44\n",
            "\n",
            "23 : [4, 5, 10, 11, 16, 32]\n",
            "4,5,10,11,16,32\n",
            "\n",
            "24 : [4, 10, 18, 22, 27, 42]\n",
            "4,10,18,22,27,42\n",
            "\n",
            "25 : [5, 14, 21, 25, 35, 36]\n",
            "5,14,21,25,35,36\n",
            "\n",
            "26 : [12, 33, 35, 41, 43, 45]\n",
            "12,33,35,41,43,45\n",
            "\n",
            "27 : [15, 32, 33, 38, 39, 44]\n",
            "15,32,33,38,39,44\n",
            "\n",
            "28 : [6, 17, 26, 28, 35, 36]\n",
            "6,17,26,28,35,36\n",
            "\n",
            "29 : [22, 30, 31, 33, 37, 41]\n",
            "22,30,31,33,37,41\n",
            "\n",
            "30 : [2, 9, 12, 21, 27, 39]\n",
            "2,9,12,21,27,39\n",
            "\n",
            "31 : [3, 5, 15, 22, 35, 40]\n",
            "3,5,15,22,35,40\n",
            "\n",
            "32 : [4, 6, 9, 16, 28, 43]\n",
            "4,6,9,16,28,43\n",
            "\n",
            "33 : [1, 3, 4, 8, 11, 33]\n",
            "1,3,4,8,11,33\n",
            "\n",
            "34 : [13, 27, 30, 39, 44, 45]\n",
            "13,27,30,39,44,45\n",
            "\n",
            "35 : [4, 11, 24, 31, 36, 39]\n",
            "4,11,24,31,36,39\n",
            "\n",
            "36 : [4, 5, 9, 10, 17, 44]\n",
            "4,5,9,10,17,44\n",
            "\n",
            "37 : [12, 30, 31, 32, 39, 44]\n",
            "12,30,31,32,39,44\n",
            "\n",
            "38 : [11, 27, 34, 35, 42, 45]\n",
            "11,27,34,35,42,45\n",
            "\n",
            "39 : [2, 5, 11, 20, 27, 34]\n",
            "2,5,11,20,27,34\n",
            "\n",
            "40 : [2, 14, 17, 33, 35, 43]\n",
            "2,14,17,33,35,43\n",
            "\n",
            "41 : [4, 10, 15, 24, 25, 41]\n",
            "4,10,15,24,25,41\n",
            "\n",
            "42 : [2, 8, 9, 12, 38, 40]\n",
            "2,8,9,12,38,40\n",
            "\n",
            "43 : [2, 14, 15, 18, 21, 36]\n",
            "2,14,15,18,21,36\n",
            "\n",
            "44 : [13, 16, 17, 24, 32, 34]\n",
            "13,16,17,24,32,34\n",
            "\n",
            "45 : [7, 15, 24, 29, 32, 36]\n",
            "7,15,24,29,32,36\n",
            "\n",
            "46 : [4, 9, 18, 24, 30, 42]\n",
            "4,9,18,24,30,42\n",
            "\n",
            "47 : [10, 23, 33, 38, 39, 40]\n",
            "10,23,33,38,39,40\n",
            "\n",
            "48 : [3, 5, 11, 16, 17, 44]\n",
            "3,5,11,16,17,44\n",
            "\n",
            "49 : [6, 9, 19, 29, 35, 41]\n",
            "6,9,19,29,35,41\n",
            "\n",
            "File Saved\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NZRsvoibHt5d"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UOjlv9p90YKR"
      },
      "source": [
        "# **Save Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xzmxpXaV0dCc"
      },
      "source": [
        "model.save('my_model.h5')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MvzmqDDf17WD"
      },
      "source": [
        "# **Load Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rqdRq1Rn2CKo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "3639fccf-e5c2-4619-d80a-1de5a47c0e33"
      },
      "source": [
        "import pandas as pd\n",
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from os import path\n",
        "from google.colab import drive\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import models\n",
        "\n",
        "from __future__ import absolute_import, division, print_function, unicode_literals\n",
        "#!pip install tensorflow-gpu==2.0.0-rc1\n",
        "!pip install tensorflow-gpu==2.0.1\n",
        "\n",
        "drive.mount('/content/gdrive',force_remount=True)\n",
        "\n",
        "new_model = tf.keras.models.load_model('./gdrive/MyDrive/my_model.h5')\n",
        "new_model.summary()"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tensorflow-gpu==2.0.1 in /usr/local/lib/python3.7/dist-packages (2.0.1)\n",
            "Requirement already satisfied: gast==0.2.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.2.2)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (3.3.0)\n",
            "Requirement already satisfied: numpy<2.0,>=1.16.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.21.4)\n",
            "Requirement already satisfied: protobuf>=3.6.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (3.17.3)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.1.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.42.0)\n",
            "Requirement already satisfied: tensorboard<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (2.0.2)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.16.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.2.0)\n",
            "Requirement already satisfied: keras-applications>=1.0.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.0.8)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.13.3)\n",
            "Requirement already satisfied: astor>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.8.1)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.12.0)\n",
            "Requirement already satisfied: keras-preprocessing>=1.0.5 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (1.1.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.1.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (2.0.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow-gpu==2.0.1) (0.37.0)\n",
            "Requirement already satisfied: h5py in /usr/local/lib/python3.7/dist-packages (from keras-applications>=1.0.8->tensorflow-gpu==2.0.1) (2.10.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (0.4.6)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (1.0.1)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (1.35.0)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (2.23.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (57.4.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.3.6)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (4.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (4.2.4)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (1.3.0)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (4.8.2)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.10.0.2)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<2,>=1.6.3->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (0.4.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (2021.10.8)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.1.0,>=2.0.0->tensorflow-gpu==2.0.1) (3.1.1)\n",
            "Mounted at /content/gdrive\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "OSError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mOSError\u001b[0m                                   Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-eafc66a63333>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mdrive\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmount\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'/content/gdrive'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mforce_remount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0mnew_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkeras\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'./gdrive/MyDrive/my_model.h5'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0mnew_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/keras/saving/save.py\u001b[0m in \u001b[0;36mload_model\u001b[0;34m(filepath, custom_objects, compile)\u001b[0m\n\u001b[1;32m    147\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstring_types\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m     \u001b[0mloader_impl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparse_saved_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    150\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msaved_model_load\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow_core/python/saved_model/loader_impl.py\u001b[0m in \u001b[0;36mparse_saved_model\u001b[0;34m(export_dir)\u001b[0m\n\u001b[1;32m     81\u001b[0m                   (export_dir,\n\u001b[1;32m     82\u001b[0m                    \u001b[0mconstants\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSAVED_MODEL_FILENAME_PBTXT\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 83\u001b[0;31m                    constants.SAVED_MODEL_FILENAME_PB))\n\u001b[0m\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mOSError\u001b[0m: SavedModel file does not exist at: ./gdrive/MyDrive/my_model.h5/{saved_model.pbtxt|saved_model.pb}"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_LZbvqHB5E3u",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0dc18634-71a0-4a2b-aff2-01fbdf9930f3"
      },
      "source": [
        "!cp /content/my_model.h5 /content/drive/My\\ Drive/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cp: failed to access '/content/drive/My Drive/': Transport endpoint is not connected\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PBxRkkEHVwU1"
      },
      "source": [
        "# FTP 전송"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z1v3JlaOWKAY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "762e78ea-c5c2-4706-ef40-dc8a62b3e09b"
      },
      "source": [
        "from ftplib import FTP\n",
        "\n",
        "ftp = FTP('112.175.184.78')\n",
        "ftp.login('dalasjoe', 'Dalasjoe75!')\n",
        "\n",
        "# ftp.cwd('html') # \"test\"디렉터리로 이동\n",
        "# ftp.retrlines('LIST') # 디렉터리의 내용을 목록화\n",
        "# #ftp.retrbinary('RETR README', open('README', 'wb').write) # README 파일 저장\n",
        "# ftp.quit()\n",
        "\n",
        "\n",
        "ftp.cwd('html')  # 업로드할 FTP 폴더로 이동\n",
        "myfile = open(path.join(lotto_base_dir, \"predict.txt\"),'rb')  # 로컬 파일 열기\n",
        "ftp.storbinary('STOR ' + 'predict.txt', myfile )  # 파일을 FTP로 업로드\n",
        "myfile.close()  # 파일 닫기\n",
        "\n",
        "print (\"File Saved\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "File Saved\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVn3rfIyW7OG"
      },
      "source": [
        ""
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MSlY9i6yW511"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}